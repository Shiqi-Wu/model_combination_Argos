{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style type='text/css'>\n",
       ".CodeMirror{\n",
       "font-size: 22px;\n",
       "</style>\n"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "%%html\n",
    "<style type='text/css'>\n",
    ".CodeMirror{\n",
    "font-size: 22px;\n",
    "</style>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"3\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "from koopman_autoencoder import *\n",
    "from load_dataset import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "if torch.cuda.is_available():\n",
    "    device = torch.device('cuda:3')\n",
    "else:\n",
    "    device = torch.device('cpu')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "File not found: ../data/data_dict_25.npy\n",
      "File not found: ../data/data_dict_44.npy\n"
     ]
    }
   ],
   "source": [
    "x_dataset = []\n",
    "y_dataset = []\n",
    "u_dataset = []\n",
    "\n",
    "for suffix in range(10, 60):\n",
    "    data_file_path = '../data/data_dict_' + str(suffix) + '.npy'\n",
    "        \n",
    "    # Check if the file exists before trying to load it\n",
    "    if os.path.exists(data_file_path):\n",
    "        data_dict = np.load(data_file_path, allow_pickle=True).item()\n",
    "        x_data, y_data, u_data = build_dataset(data_dict)\n",
    "        x_dataset.append(x_data)\n",
    "        y_dataset.append(y_data)\n",
    "        u_dataset.append(u_data)\n",
    "    else:\n",
    "        print(f\"File not found: {data_file_path}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(7221, 6957)"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_data = np.concatenate(x_dataset, axis = 0)\n",
    "y_data = np.concatenate(y_dataset, axis = 0)\n",
    "u_data = np.concatenate(u_dataset, axis = 0)\n",
    "x_data.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.model_selection import train_test_split\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "# Scaling the datasets\n",
    "scaler_x = StandardScaler().fit(x_data)\n",
    "scaler_u = StandardScaler().fit(u_data)\n",
    "\n",
    "x_data_scaled = scaler_x.transform(x_data)\n",
    "y_data_scaled = scaler_x.transform(y_data)\n",
    "u_data_scaled = scaler_u.transform(u_data)\n",
    "\n",
    "shuffled_indices = np.arange(len(x_data))\n",
    "np.random.shuffle(shuffled_indices)\n",
    "\n",
    "x_data_scaled = x_data_scaled[shuffled_indices]\n",
    "y_data_scaled = y_data_scaled[shuffled_indices]\n",
    "u_data_scaled = u_data_scaled[shuffled_indices]\n",
    "\n",
    "x_train, x_test = train_test_split(x_data_scaled, test_size=0.2, random_state=42)\n",
    "y_train, y_test = train_test_split(y_data_scaled, test_size=0.2, random_state=42)\n",
    "u_train, u_test = train_test_split(u_data_scaled, test_size=0.2, random_state=42)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch.optim import Adam\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "from torch.optim.lr_scheduler import StepLR\n",
    "\n",
    "\n",
    "x_train = torch.tensor(x_train, dtype=torch.float32)\n",
    "y_train = torch.tensor(y_train, dtype=torch.float32)\n",
    "u_train = torch.tensor(u_train, dtype=torch.float32)\n",
    "x_test = torch.tensor(x_test, dtype=torch.float32)\n",
    "y_test = torch.tensor(y_test, dtype=torch.float32)\n",
    "u_test = torch.tensor(u_test, dtype=torch.float32)\n",
    "\n",
    "# Create a dataset\n",
    "dataset = TensorDataset(x_train, y_train, u_train)\n",
    "\n",
    "# Create a data loader\n",
    "data_loader = DataLoader(dataset, batch_size=64, shuffle=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_input = x_data.shape[1]\n",
    "layer_sizes_dic = [128, 128, 128, 128]\n",
    "layer_sizes_m = [128, 64, 64, 128]\n",
    "\n",
    "n_psi_train = 256\n",
    "u_dim = 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_psi, model_inv_psi, model_autoencoder, model_correction = BuildNonlinearModel(n_input, layer_sizes_dic, layer_sizes_m, n_psi_train, u_dim)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test for autoencoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the optimizer\n",
    "optimizer_autoencoder = Adam(list(model_autoencoder.parameters()) , lr=0.001)\n",
    "scheduler = StepLR(optimizer_autoencoder, step_size=50, gamma=0.8)\n",
    "\n",
    "mse_loss = nn.MSELoss()\n",
    "\n",
    "# Initialize a list to store the loss values\n",
    "train_losses = []\n",
    "val_losses = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 0, Training Loss: 0.0599786750972271, Validation Loss: 0.05951186642050743, Learning Rate: 0.001\n",
      "Epoch: 1, Training Loss: 0.0328616201877594, Validation Loss: 0.03236745670437813, Learning Rate: 0.001\n",
      "Epoch: 2, Training Loss: 0.021297432482242584, Validation Loss: 0.02162916399538517, Learning Rate: 0.001\n",
      "Epoch: 3, Training Loss: 0.028698712587356567, Validation Loss: 0.02884109690785408, Learning Rate: 0.001\n",
      "Epoch: 4, Training Loss: 0.016017509624361992, Validation Loss: 0.015137340873479843, Learning Rate: 0.001\n",
      "Epoch: 5, Training Loss: 0.019567901268601418, Validation Loss: 0.01947798952460289, Learning Rate: 0.001\n",
      "Epoch: 6, Training Loss: 0.018393149599432945, Validation Loss: 0.018400220200419426, Learning Rate: 0.001\n",
      "Epoch: 7, Training Loss: 0.02183493599295616, Validation Loss: 0.021818295121192932, Learning Rate: 0.001\n",
      "Epoch: 8, Training Loss: 0.02937791310250759, Validation Loss: 0.029951095581054688, Learning Rate: 0.001\n",
      "Epoch: 9, Training Loss: 0.01822722889482975, Validation Loss: 0.016760462895035744, Learning Rate: 0.001\n",
      "Epoch: 10, Training Loss: 0.01412278413772583, Validation Loss: 0.01328029204159975, Learning Rate: 0.001\n",
      "Epoch: 11, Training Loss: 0.01493790466338396, Validation Loss: 0.014195679686963558, Learning Rate: 0.001\n",
      "Epoch: 12, Training Loss: 0.012145038694143295, Validation Loss: 0.011657260358333588, Learning Rate: 0.001\n",
      "Epoch: 13, Training Loss: 0.013724994845688343, Validation Loss: 0.013174776919186115, Learning Rate: 0.001\n",
      "Epoch: 14, Training Loss: 0.010064993984997272, Validation Loss: 0.009571164846420288, Learning Rate: 0.001\n",
      "Epoch: 15, Training Loss: 0.01091835368424654, Validation Loss: 0.010172252543270588, Learning Rate: 0.001\n",
      "Epoch: 16, Training Loss: 0.01106844749301672, Validation Loss: 0.00959587562829256, Learning Rate: 0.001\n",
      "Epoch: 17, Training Loss: 0.012755000032484531, Validation Loss: 0.012288679368793964, Learning Rate: 0.001\n",
      "Epoch: 18, Training Loss: 0.009532425552606583, Validation Loss: 0.008623309433460236, Learning Rate: 0.001\n",
      "Epoch: 19, Training Loss: 0.009003186598420143, Validation Loss: 0.007851202972233295, Learning Rate: 0.001\n",
      "Epoch: 20, Training Loss: 0.009983071126043797, Validation Loss: 0.00910160131752491, Learning Rate: 0.001\n",
      "Epoch: 21, Training Loss: 0.010069239884614944, Validation Loss: 0.009321805089712143, Learning Rate: 0.001\n",
      "Epoch: 22, Training Loss: 0.010841145180165768, Validation Loss: 0.01049806922674179, Learning Rate: 0.001\n",
      "Epoch: 23, Training Loss: 0.01138141006231308, Validation Loss: 0.010965767316520214, Learning Rate: 0.001\n",
      "Epoch: 24, Training Loss: 0.009470123797655106, Validation Loss: 0.008522480726242065, Learning Rate: 0.001\n",
      "Epoch: 25, Training Loss: 0.007685433141887188, Validation Loss: 0.00696631008759141, Learning Rate: 0.001\n",
      "Epoch: 26, Training Loss: 0.007787132635712624, Validation Loss: 0.007109005004167557, Learning Rate: 0.001\n",
      "Epoch: 27, Training Loss: 0.009667230769991875, Validation Loss: 0.009397942572832108, Learning Rate: 0.001\n",
      "Epoch: 28, Training Loss: 0.01043977215886116, Validation Loss: 0.010176709853112698, Learning Rate: 0.001\n",
      "Epoch: 29, Training Loss: 0.00766550051048398, Validation Loss: 0.0069458805955946445, Learning Rate: 0.001\n",
      "Epoch: 30, Training Loss: 0.008325068280100822, Validation Loss: 0.007683060131967068, Learning Rate: 0.001\n",
      "Epoch: 31, Training Loss: 0.009640383534133434, Validation Loss: 0.009300862438976765, Learning Rate: 0.001\n",
      "Epoch: 32, Training Loss: 0.012985694222152233, Validation Loss: 0.012417136691510677, Learning Rate: 0.001\n",
      "Epoch: 33, Training Loss: 0.008639459498226643, Validation Loss: 0.008779811672866344, Learning Rate: 0.001\n",
      "Epoch: 34, Training Loss: 0.01277590449899435, Validation Loss: 0.011706462129950523, Learning Rate: 0.001\n",
      "Epoch: 35, Training Loss: 0.0089041693136096, Validation Loss: 0.008693953976035118, Learning Rate: 0.001\n",
      "Epoch: 36, Training Loss: 0.014409288763999939, Validation Loss: 0.012739217840135098, Learning Rate: 0.001\n",
      "Epoch: 37, Training Loss: 0.010115105658769608, Validation Loss: 0.009119788184762001, Learning Rate: 0.001\n",
      "Epoch: 38, Training Loss: 0.008505004458129406, Validation Loss: 0.007807532325387001, Learning Rate: 0.001\n",
      "Epoch: 39, Training Loss: 0.00970794539898634, Validation Loss: 0.009460985660552979, Learning Rate: 0.001\n",
      "Epoch: 40, Training Loss: 0.005593964364379644, Validation Loss: 0.005733090918511152, Learning Rate: 0.001\n",
      "Epoch: 41, Training Loss: 0.00989348441362381, Validation Loss: 0.009195755235850811, Learning Rate: 0.001\n",
      "Epoch: 42, Training Loss: 0.01037862803786993, Validation Loss: 0.010308112017810345, Learning Rate: 0.001\n",
      "Epoch: 43, Training Loss: 0.011415340937674046, Validation Loss: 0.011756819672882557, Learning Rate: 0.001\n",
      "Epoch: 44, Training Loss: 0.006249603349715471, Validation Loss: 0.0065009742975234985, Learning Rate: 0.001\n",
      "Epoch: 45, Training Loss: 0.010747398249804974, Validation Loss: 0.011364485137164593, Learning Rate: 0.001\n",
      "Epoch: 46, Training Loss: 0.0046220943331718445, Validation Loss: 0.004704922903329134, Learning Rate: 0.001\n",
      "Epoch: 47, Training Loss: 0.009723138995468616, Validation Loss: 0.009748789481818676, Learning Rate: 0.001\n",
      "Epoch: 48, Training Loss: 0.006626802030950785, Validation Loss: 0.007191233336925507, Learning Rate: 0.001\n",
      "Epoch: 49, Training Loss: 0.008330478332936764, Validation Loss: 0.00858792569488287, Learning Rate: 0.001\n",
      "Epoch: 50, Training Loss: 0.005468753166496754, Validation Loss: 0.005879790522158146, Learning Rate: 0.0008\n",
      "Epoch: 51, Training Loss: 0.006632963661104441, Validation Loss: 0.006515274289995432, Learning Rate: 0.0008\n",
      "Epoch: 52, Training Loss: 0.0062433695420622826, Validation Loss: 0.005917731672525406, Learning Rate: 0.0008\n",
      "Epoch: 53, Training Loss: 0.006590326316654682, Validation Loss: 0.006530465558171272, Learning Rate: 0.0008\n",
      "Epoch: 54, Training Loss: 0.004849541466683149, Validation Loss: 0.004749016836285591, Learning Rate: 0.0008\n",
      "Epoch: 55, Training Loss: 0.004956717137247324, Validation Loss: 0.005706142168492079, Learning Rate: 0.0008\n",
      "Epoch: 56, Training Loss: 0.004205717705190182, Validation Loss: 0.004740580916404724, Learning Rate: 0.0008\n",
      "Epoch: 57, Training Loss: 0.004479363095015287, Validation Loss: 0.0052251918241381645, Learning Rate: 0.0008\n",
      "Epoch: 58, Training Loss: 0.005139035638421774, Validation Loss: 0.005885582882910967, Learning Rate: 0.0008\n",
      "Epoch: 59, Training Loss: 0.004395522177219391, Validation Loss: 0.0048975893296301365, Learning Rate: 0.0008\n",
      "Epoch: 60, Training Loss: 0.004226925317198038, Validation Loss: 0.004392056260257959, Learning Rate: 0.0008\n",
      "Epoch: 61, Training Loss: 0.0041663432493805885, Validation Loss: 0.004655556287616491, Learning Rate: 0.0008\n",
      "Epoch: 62, Training Loss: 0.005860448814928532, Validation Loss: 0.006132275331765413, Learning Rate: 0.0008\n",
      "Epoch: 63, Training Loss: 0.006465901155024767, Validation Loss: 0.006320138927549124, Learning Rate: 0.0008\n",
      "Epoch: 64, Training Loss: 0.006207597441971302, Validation Loss: 0.006594505161046982, Learning Rate: 0.0008\n",
      "Epoch: 65, Training Loss: 0.004483547993004322, Validation Loss: 0.004771214444190264, Learning Rate: 0.0008\n",
      "Epoch: 66, Training Loss: 0.0064101628959178925, Validation Loss: 0.006239225622266531, Learning Rate: 0.0008\n",
      "Epoch: 67, Training Loss: 0.0048804376274347305, Validation Loss: 0.005339806899428368, Learning Rate: 0.0008\n",
      "Epoch: 68, Training Loss: 0.004933604039251804, Validation Loss: 0.0052884723991155624, Learning Rate: 0.0008\n",
      "Epoch: 69, Training Loss: 0.005657533649355173, Validation Loss: 0.005803355947136879, Learning Rate: 0.0008\n",
      "Epoch: 70, Training Loss: 0.004082631319761276, Validation Loss: 0.004149767104536295, Learning Rate: 0.0008\n",
      "Epoch: 71, Training Loss: 0.0037070231046527624, Validation Loss: 0.0036189621314406395, Learning Rate: 0.0008\n",
      "Epoch: 72, Training Loss: 0.004767312202602625, Validation Loss: 0.005066840909421444, Learning Rate: 0.0008\n",
      "Epoch: 73, Training Loss: 0.00546261016279459, Validation Loss: 0.006131662521511316, Learning Rate: 0.0008\n",
      "Epoch: 74, Training Loss: 0.005532642360776663, Validation Loss: 0.005792221520096064, Learning Rate: 0.0008\n",
      "Epoch: 75, Training Loss: 0.006040672771632671, Validation Loss: 0.006473203655332327, Learning Rate: 0.0008\n",
      "Epoch: 76, Training Loss: 0.0051310318522155285, Validation Loss: 0.005139780230820179, Learning Rate: 0.0008\n",
      "Epoch: 77, Training Loss: 0.007530465256422758, Validation Loss: 0.007949190214276314, Learning Rate: 0.0008\n",
      "Epoch: 78, Training Loss: 0.006369789130985737, Validation Loss: 0.007280836347490549, Learning Rate: 0.0008\n",
      "Epoch: 79, Training Loss: 0.004634818062186241, Validation Loss: 0.005012158770114183, Learning Rate: 0.0008\n",
      "Epoch: 80, Training Loss: 0.0045403302647173405, Validation Loss: 0.004893627483397722, Learning Rate: 0.0008\n",
      "Epoch: 81, Training Loss: 0.004998433869332075, Validation Loss: 0.005481854546815157, Learning Rate: 0.0008\n",
      "Epoch: 82, Training Loss: 0.004232259467244148, Validation Loss: 0.004549933131784201, Learning Rate: 0.0008\n",
      "Epoch: 83, Training Loss: 0.003974052146077156, Validation Loss: 0.004124446306377649, Learning Rate: 0.0008\n",
      "Epoch: 84, Training Loss: 0.005146554671227932, Validation Loss: 0.005442719906568527, Learning Rate: 0.0008\n",
      "Epoch: 85, Training Loss: 0.0050896089524030685, Validation Loss: 0.0051338206976652145, Learning Rate: 0.0008\n",
      "Epoch: 86, Training Loss: 0.004518469795584679, Validation Loss: 0.0046231625601649284, Learning Rate: 0.0008\n",
      "Epoch: 87, Training Loss: 0.005475288722664118, Validation Loss: 0.006021065171808004, Learning Rate: 0.0008\n",
      "Epoch: 88, Training Loss: 0.004928579553961754, Validation Loss: 0.005209258291870356, Learning Rate: 0.0008\n",
      "Epoch: 89, Training Loss: 0.004665072076022625, Validation Loss: 0.004565775860100985, Learning Rate: 0.0008\n",
      "Epoch: 90, Training Loss: 0.0036058914847671986, Validation Loss: 0.004002875182777643, Learning Rate: 0.0008\n",
      "Epoch: 91, Training Loss: 0.004728748928755522, Validation Loss: 0.005229718517512083, Learning Rate: 0.0008\n",
      "Epoch: 92, Training Loss: 0.004942591767758131, Validation Loss: 0.005584445782005787, Learning Rate: 0.0008\n",
      "Epoch: 93, Training Loss: 0.004692543763667345, Validation Loss: 0.00514377374202013, Learning Rate: 0.0008\n",
      "Epoch: 94, Training Loss: 0.004459860268980265, Validation Loss: 0.0049292477779090405, Learning Rate: 0.0008\n",
      "Epoch: 95, Training Loss: 0.004671445116400719, Validation Loss: 0.0047342535108327866, Learning Rate: 0.0008\n",
      "Epoch: 96, Training Loss: 0.005617366638034582, Validation Loss: 0.005880324635654688, Learning Rate: 0.0008\n",
      "Epoch: 97, Training Loss: 0.00462507177144289, Validation Loss: 0.005191761534661055, Learning Rate: 0.0008\n",
      "Epoch: 98, Training Loss: 0.006021796725690365, Validation Loss: 0.006784828845411539, Learning Rate: 0.0008\n",
      "Epoch: 99, Training Loss: 0.0059365397319197655, Validation Loss: 0.006132539827376604, Learning Rate: 0.0008\n",
      "Epoch: 100, Training Loss: 0.004435703158378601, Validation Loss: 0.004761415999382734, Learning Rate: 0.00064\n",
      "Epoch: 101, Training Loss: 0.0053850337862968445, Validation Loss: 0.0055319275707006454, Learning Rate: 0.00064\n",
      "Epoch: 102, Training Loss: 0.004604210611432791, Validation Loss: 0.004904258530586958, Learning Rate: 0.00064\n",
      "Epoch: 103, Training Loss: 0.004592937882989645, Validation Loss: 0.004817457403987646, Learning Rate: 0.00064\n",
      "Epoch: 104, Training Loss: 0.004764880985021591, Validation Loss: 0.004966960288584232, Learning Rate: 0.00064\n",
      "Epoch: 105, Training Loss: 0.00467179249972105, Validation Loss: 0.005027381703257561, Learning Rate: 0.00064\n",
      "Epoch: 106, Training Loss: 0.0035612466745078564, Validation Loss: 0.003868509316816926, Learning Rate: 0.00064\n",
      "Epoch: 107, Training Loss: 0.004189764615148306, Validation Loss: 0.0045946380123496056, Learning Rate: 0.00064\n",
      "Epoch: 108, Training Loss: 0.004385588690638542, Validation Loss: 0.004815152380615473, Learning Rate: 0.00064\n",
      "Epoch: 109, Training Loss: 0.004545137751847506, Validation Loss: 0.004912154749035835, Learning Rate: 0.00064\n",
      "Epoch: 110, Training Loss: 0.006291947327554226, Validation Loss: 0.006399220786988735, Learning Rate: 0.00064\n",
      "Epoch: 111, Training Loss: 0.004753359593451023, Validation Loss: 0.0049993922002613544, Learning Rate: 0.00064\n",
      "Epoch: 112, Training Loss: 0.004870387259870768, Validation Loss: 0.005224139429628849, Learning Rate: 0.00064\n",
      "Epoch: 113, Training Loss: 0.005622216034680605, Validation Loss: 0.0055924938060343266, Learning Rate: 0.00064\n",
      "Epoch: 114, Training Loss: 0.004107628483325243, Validation Loss: 0.004292507655918598, Learning Rate: 0.00064\n",
      "Epoch: 115, Training Loss: 0.005479027982801199, Validation Loss: 0.005837642587721348, Learning Rate: 0.00064\n",
      "Epoch: 116, Training Loss: 0.004419022239744663, Validation Loss: 0.004465881735086441, Learning Rate: 0.00064\n",
      "Epoch: 117, Training Loss: 0.0047153932973742485, Validation Loss: 0.005116092506796122, Learning Rate: 0.00064\n",
      "Epoch: 118, Training Loss: 0.003889762330800295, Validation Loss: 0.004384624771773815, Learning Rate: 0.00064\n",
      "Epoch: 119, Training Loss: 0.003657455090433359, Validation Loss: 0.003612359520047903, Learning Rate: 0.00064\n",
      "Epoch: 120, Training Loss: 0.004008258227258921, Validation Loss: 0.003766513429582119, Learning Rate: 0.00064\n",
      "Epoch: 121, Training Loss: 0.004573558457195759, Validation Loss: 0.004614453297108412, Learning Rate: 0.00064\n",
      "Epoch: 122, Training Loss: 0.004075478762388229, Validation Loss: 0.004352219868451357, Learning Rate: 0.00064\n",
      "Epoch: 123, Training Loss: 0.0040381536819040775, Validation Loss: 0.0043020728044211864, Learning Rate: 0.00064\n",
      "Epoch: 124, Training Loss: 0.004554998129606247, Validation Loss: 0.004499845206737518, Learning Rate: 0.00064\n",
      "Epoch: 125, Training Loss: 0.0036945734173059464, Validation Loss: 0.004201479721814394, Learning Rate: 0.00064\n",
      "Epoch: 126, Training Loss: 0.0029237272683531046, Validation Loss: 0.003248038934543729, Learning Rate: 0.00064\n",
      "Epoch: 127, Training Loss: 0.003081651171669364, Validation Loss: 0.003101809648796916, Learning Rate: 0.00064\n",
      "Epoch: 128, Training Loss: 0.0034601918887346983, Validation Loss: 0.0032633887603878975, Learning Rate: 0.00064\n",
      "Epoch: 129, Training Loss: 0.003610869636759162, Validation Loss: 0.003735750215128064, Learning Rate: 0.00064\n",
      "Epoch: 130, Training Loss: 0.003591870656237006, Validation Loss: 0.0038339351303875446, Learning Rate: 0.00064\n",
      "Epoch: 131, Training Loss: 0.0044470978900790215, Validation Loss: 0.004647125955671072, Learning Rate: 0.00064\n",
      "Epoch: 132, Training Loss: 0.00547809200361371, Validation Loss: 0.005779363214969635, Learning Rate: 0.00064\n",
      "Epoch: 133, Training Loss: 0.004397084005177021, Validation Loss: 0.004933631047606468, Learning Rate: 0.00064\n",
      "Epoch: 134, Training Loss: 0.004765414632856846, Validation Loss: 0.004998635500669479, Learning Rate: 0.00064\n",
      "Epoch: 135, Training Loss: 0.0038804139476269484, Validation Loss: 0.003720549400895834, Learning Rate: 0.00064\n",
      "Epoch: 136, Training Loss: 0.0040558562614023685, Validation Loss: 0.003917711321264505, Learning Rate: 0.00064\n",
      "Epoch: 137, Training Loss: 0.0031143799424171448, Validation Loss: 0.0031138858757913113, Learning Rate: 0.00064\n",
      "Epoch: 138, Training Loss: 0.003346397541463375, Validation Loss: 0.0032591428607702255, Learning Rate: 0.00064\n",
      "Epoch: 139, Training Loss: 0.004713144153356552, Validation Loss: 0.00498850317671895, Learning Rate: 0.00064\n",
      "Epoch: 140, Training Loss: 0.0032497872598469257, Validation Loss: 0.0035109701566398144, Learning Rate: 0.00064\n",
      "Epoch: 141, Training Loss: 0.0036272869911044836, Validation Loss: 0.0036835907958447933, Learning Rate: 0.00064\n",
      "Epoch: 142, Training Loss: 0.003781640902161598, Validation Loss: 0.003992757294327021, Learning Rate: 0.00064\n",
      "Epoch: 143, Training Loss: 0.0029146771412342787, Validation Loss: 0.003108838340267539, Learning Rate: 0.00064\n",
      "Epoch: 144, Training Loss: 0.0031350490171462297, Validation Loss: 0.0030571387615054846, Learning Rate: 0.00064\n",
      "Epoch: 145, Training Loss: 0.0042625837959349155, Validation Loss: 0.004517907276749611, Learning Rate: 0.00064\n",
      "Epoch: 146, Training Loss: 0.0030222733039408922, Validation Loss: 0.0033485572785139084, Learning Rate: 0.00064\n",
      "Epoch: 147, Training Loss: 0.0035603076685220003, Validation Loss: 0.003766936482861638, Learning Rate: 0.00064\n",
      "Epoch: 148, Training Loss: 0.002746721263974905, Validation Loss: 0.0028654998168349266, Learning Rate: 0.00064\n",
      "Epoch: 149, Training Loss: 0.003876760834828019, Validation Loss: 0.004071402363479137, Learning Rate: 0.00064\n",
      "Epoch: 150, Training Loss: 0.0030887352768331766, Validation Loss: 0.0035772521514445543, Learning Rate: 0.0005120000000000001\n",
      "Epoch: 151, Training Loss: 0.003194181015715003, Validation Loss: 0.003381207352504134, Learning Rate: 0.0005120000000000001\n",
      "Epoch: 152, Training Loss: 0.0027704883832484484, Validation Loss: 0.003191397525370121, Learning Rate: 0.0005120000000000001\n",
      "Epoch: 153, Training Loss: 0.003075274173170328, Validation Loss: 0.003303772071376443, Learning Rate: 0.0005120000000000001\n",
      "Epoch: 154, Training Loss: 0.0028949682600796223, Validation Loss: 0.00334423896856606, Learning Rate: 0.0005120000000000001\n",
      "Epoch: 155, Training Loss: 0.0030335583724081516, Validation Loss: 0.0031628620345145464, Learning Rate: 0.0005120000000000001\n",
      "Epoch: 156, Training Loss: 0.003030590945854783, Validation Loss: 0.003072951687499881, Learning Rate: 0.0005120000000000001\n",
      "Epoch: 157, Training Loss: 0.002716245362535119, Validation Loss: 0.0028020264580845833, Learning Rate: 0.0005120000000000001\n",
      "Epoch: 158, Training Loss: 0.00269132386893034, Validation Loss: 0.0029806140810251236, Learning Rate: 0.0005120000000000001\n",
      "Epoch: 159, Training Loss: 0.0033425542060285807, Validation Loss: 0.003066393779590726, Learning Rate: 0.0005120000000000001\n",
      "Epoch: 160, Training Loss: 0.0028679315000772476, Validation Loss: 0.0031663025729358196, Learning Rate: 0.0005120000000000001\n",
      "Epoch: 161, Training Loss: 0.004656389355659485, Validation Loss: 0.003769912524148822, Learning Rate: 0.0005120000000000001\n",
      "Epoch: 162, Training Loss: 0.003563357749953866, Validation Loss: 0.003633158979937434, Learning Rate: 0.0005120000000000001\n",
      "Epoch: 163, Training Loss: 0.0035953351762145758, Validation Loss: 0.003780075116083026, Learning Rate: 0.0005120000000000001\n",
      "Epoch: 164, Training Loss: 0.0036370032466948032, Validation Loss: 0.003549338085576892, Learning Rate: 0.0005120000000000001\n",
      "Epoch: 165, Training Loss: 0.002870832337066531, Validation Loss: 0.002816029591485858, Learning Rate: 0.0005120000000000001\n",
      "Epoch: 166, Training Loss: 0.0029789935797452927, Validation Loss: 0.0032977776136249304, Learning Rate: 0.0005120000000000001\n",
      "Epoch: 167, Training Loss: 0.0035040623042732477, Validation Loss: 0.004163002595305443, Learning Rate: 0.0005120000000000001\n",
      "Epoch: 168, Training Loss: 0.002256781794130802, Validation Loss: 0.0025284711737185717, Learning Rate: 0.0005120000000000001\n",
      "Epoch: 169, Training Loss: 0.0026438068598508835, Validation Loss: 0.0031062860507518053, Learning Rate: 0.0005120000000000001\n",
      "Epoch: 170, Training Loss: 0.0033222532365471125, Validation Loss: 0.003559725359082222, Learning Rate: 0.0005120000000000001\n",
      "Epoch: 171, Training Loss: 0.0029026770498603582, Validation Loss: 0.003121978836134076, Learning Rate: 0.0005120000000000001\n",
      "Epoch: 172, Training Loss: 0.0031259022653102875, Validation Loss: 0.0032297582365572453, Learning Rate: 0.0005120000000000001\n",
      "Epoch: 173, Training Loss: 0.0025431232061237097, Validation Loss: 0.002534689614549279, Learning Rate: 0.0005120000000000001\n",
      "Epoch: 174, Training Loss: 0.0030222947243601084, Validation Loss: 0.0030266069807112217, Learning Rate: 0.0005120000000000001\n",
      "Epoch: 175, Training Loss: 0.0029170792549848557, Validation Loss: 0.0030768641736358404, Learning Rate: 0.0005120000000000001\n",
      "Epoch: 176, Training Loss: 0.0027682373765856028, Validation Loss: 0.0030232130084186792, Learning Rate: 0.0005120000000000001\n",
      "Epoch: 177, Training Loss: 0.0026950621977448463, Validation Loss: 0.0028009554371237755, Learning Rate: 0.0005120000000000001\n",
      "Epoch: 178, Training Loss: 0.0031327418982982635, Validation Loss: 0.0032864450477063656, Learning Rate: 0.0005120000000000001\n",
      "Epoch: 179, Training Loss: 0.002836568048223853, Validation Loss: 0.0030186509247869253, Learning Rate: 0.0005120000000000001\n",
      "Epoch: 180, Training Loss: 0.0035303712356835604, Validation Loss: 0.0037153654266148806, Learning Rate: 0.0005120000000000001\n",
      "Epoch: 181, Training Loss: 0.0036258103791624308, Validation Loss: 0.003919999580830336, Learning Rate: 0.0005120000000000001\n",
      "Epoch: 182, Training Loss: 0.0035123038105666637, Validation Loss: 0.0037007874343544245, Learning Rate: 0.0005120000000000001\n",
      "Epoch: 183, Training Loss: 0.003634719643741846, Validation Loss: 0.003944416530430317, Learning Rate: 0.0005120000000000001\n",
      "Epoch: 184, Training Loss: 0.003211741102859378, Validation Loss: 0.0039889272302389145, Learning Rate: 0.0005120000000000001\n",
      "Epoch: 185, Training Loss: 0.0037582877557724714, Validation Loss: 0.004378258716315031, Learning Rate: 0.0005120000000000001\n",
      "Epoch: 186, Training Loss: 0.003724543610587716, Validation Loss: 0.00445274543017149, Learning Rate: 0.0005120000000000001\n",
      "Epoch: 187, Training Loss: 0.003843337297439575, Validation Loss: 0.003991520032286644, Learning Rate: 0.0005120000000000001\n",
      "Epoch: 188, Training Loss: 0.003768092952668667, Validation Loss: 0.004548926372081041, Learning Rate: 0.0005120000000000001\n",
      "Epoch: 189, Training Loss: 0.004802602808922529, Validation Loss: 0.005566155072301626, Learning Rate: 0.0005120000000000001\n",
      "Epoch: 190, Training Loss: 0.004211697727441788, Validation Loss: 0.00451267184689641, Learning Rate: 0.0005120000000000001\n",
      "Epoch: 191, Training Loss: 0.003413486760109663, Validation Loss: 0.0037926621735095978, Learning Rate: 0.0005120000000000001\n",
      "Epoch: 192, Training Loss: 0.003733340185135603, Validation Loss: 0.003998678643256426, Learning Rate: 0.0005120000000000001\n",
      "Epoch: 193, Training Loss: 0.003743483917787671, Validation Loss: 0.004019531887024641, Learning Rate: 0.0005120000000000001\n",
      "Epoch: 194, Training Loss: 0.003973451443016529, Validation Loss: 0.004344653803855181, Learning Rate: 0.0005120000000000001\n",
      "Epoch: 195, Training Loss: 0.004002568777650595, Validation Loss: 0.004361913073807955, Learning Rate: 0.0005120000000000001\n",
      "Epoch: 196, Training Loss: 0.003779755672439933, Validation Loss: 0.003968541976064444, Learning Rate: 0.0005120000000000001\n",
      "Epoch: 197, Training Loss: 0.0036585384514182806, Validation Loss: 0.003912950400263071, Learning Rate: 0.0005120000000000001\n",
      "Epoch: 198, Training Loss: 0.0034743421711027622, Validation Loss: 0.003857271047309041, Learning Rate: 0.0005120000000000001\n",
      "Epoch: 199, Training Loss: 0.006028478499501944, Validation Loss: 0.006143163423985243, Learning Rate: 0.0005120000000000001\n",
      "Epoch: 200, Training Loss: 0.0036201183684170246, Validation Loss: 0.004094349220395088, Learning Rate: 0.0004096000000000001\n",
      "Epoch: 201, Training Loss: 0.002571726916357875, Validation Loss: 0.0028253805357962847, Learning Rate: 0.0004096000000000001\n",
      "Epoch: 202, Training Loss: 0.0028991354629397392, Validation Loss: 0.003144988790154457, Learning Rate: 0.0004096000000000001\n",
      "Epoch: 203, Training Loss: 0.0029610132332891226, Validation Loss: 0.0033547459170222282, Learning Rate: 0.0004096000000000001\n",
      "Epoch: 204, Training Loss: 0.0026646475307643414, Validation Loss: 0.0029889121651649475, Learning Rate: 0.0004096000000000001\n",
      "Epoch: 205, Training Loss: 0.0029644034802913666, Validation Loss: 0.0033681103959679604, Learning Rate: 0.0004096000000000001\n",
      "Epoch: 206, Training Loss: 0.0028631840832531452, Validation Loss: 0.0032656595576554537, Learning Rate: 0.0004096000000000001\n",
      "Epoch: 207, Training Loss: 0.004225266166031361, Validation Loss: 0.004154782742261887, Learning Rate: 0.0004096000000000001\n",
      "Epoch: 208, Training Loss: 0.003409580560401082, Validation Loss: 0.003572912886738777, Learning Rate: 0.0004096000000000001\n",
      "Epoch: 209, Training Loss: 0.0029097669757902622, Validation Loss: 0.0034974943846464157, Learning Rate: 0.0004096000000000001\n",
      "Epoch: 210, Training Loss: 0.0035456048790365458, Validation Loss: 0.0034552253782749176, Learning Rate: 0.0004096000000000001\n",
      "Epoch: 211, Training Loss: 0.004106858745217323, Validation Loss: 0.004300047643482685, Learning Rate: 0.0004096000000000001\n",
      "Epoch: 212, Training Loss: 0.003521263599395752, Validation Loss: 0.004092040006071329, Learning Rate: 0.0004096000000000001\n",
      "Epoch: 213, Training Loss: 0.003089403035119176, Validation Loss: 0.003343630814924836, Learning Rate: 0.0004096000000000001\n",
      "Epoch: 214, Training Loss: 0.002637650351971388, Validation Loss: 0.002550425473600626, Learning Rate: 0.0004096000000000001\n",
      "Epoch: 215, Training Loss: 0.0031837967690080404, Validation Loss: 0.0032726717181503773, Learning Rate: 0.0004096000000000001\n",
      "Epoch: 216, Training Loss: 0.003788210218772292, Validation Loss: 0.0035178489051759243, Learning Rate: 0.0004096000000000001\n",
      "Epoch: 217, Training Loss: 0.00371421012096107, Validation Loss: 0.0038742697797715664, Learning Rate: 0.0004096000000000001\n",
      "Epoch: 218, Training Loss: 0.003474730998277664, Validation Loss: 0.003614372806623578, Learning Rate: 0.0004096000000000001\n",
      "Epoch: 219, Training Loss: 0.0037046365905553102, Validation Loss: 0.0041040945798158646, Learning Rate: 0.0004096000000000001\n",
      "Epoch: 220, Training Loss: 0.0028006366919726133, Validation Loss: 0.0028214305639266968, Learning Rate: 0.0004096000000000001\n",
      "Epoch: 221, Training Loss: 0.002432825742289424, Validation Loss: 0.0024805108550935984, Learning Rate: 0.0004096000000000001\n",
      "Epoch: 222, Training Loss: 0.002628515474498272, Validation Loss: 0.0027150295209139585, Learning Rate: 0.0004096000000000001\n",
      "Epoch: 223, Training Loss: 0.0027636128943413496, Validation Loss: 0.0028768836054950953, Learning Rate: 0.0004096000000000001\n",
      "Epoch: 224, Training Loss: 0.002689816290512681, Validation Loss: 0.002837521955370903, Learning Rate: 0.0004096000000000001\n",
      "Epoch: 225, Training Loss: 0.0028644853737205267, Validation Loss: 0.0029870420694351196, Learning Rate: 0.0004096000000000001\n",
      "Epoch: 226, Training Loss: 0.0030471968930214643, Validation Loss: 0.0033284614328294992, Learning Rate: 0.0004096000000000001\n",
      "Epoch: 227, Training Loss: 0.0033410664182156324, Validation Loss: 0.0033891508355736732, Learning Rate: 0.0004096000000000001\n",
      "Epoch: 228, Training Loss: 0.0030482665169984102, Validation Loss: 0.003375799860805273, Learning Rate: 0.0004096000000000001\n",
      "Epoch: 229, Training Loss: 0.0026448743883520365, Validation Loss: 0.003116049338132143, Learning Rate: 0.0004096000000000001\n",
      "Epoch: 230, Training Loss: 0.0035642068833112717, Validation Loss: 0.004409827757626772, Learning Rate: 0.0004096000000000001\n",
      "Epoch: 231, Training Loss: 0.0029620397835969925, Validation Loss: 0.003042541677132249, Learning Rate: 0.0004096000000000001\n",
      "Epoch: 232, Training Loss: 0.0026051909662783146, Validation Loss: 0.003060772316530347, Learning Rate: 0.0004096000000000001\n",
      "Epoch: 233, Training Loss: 0.002772677456960082, Validation Loss: 0.0033888202160596848, Learning Rate: 0.0004096000000000001\n",
      "Epoch: 234, Training Loss: 0.0025543542578816414, Validation Loss: 0.0026951003819704056, Learning Rate: 0.0004096000000000001\n",
      "Epoch: 235, Training Loss: 0.002863036934286356, Validation Loss: 0.0026803109794855118, Learning Rate: 0.0004096000000000001\n",
      "Epoch: 236, Training Loss: 0.0029768759850412607, Validation Loss: 0.0029089199379086494, Learning Rate: 0.0004096000000000001\n",
      "Epoch: 237, Training Loss: 0.0029834280721843243, Validation Loss: 0.0028863975312560797, Learning Rate: 0.0004096000000000001\n",
      "Epoch: 238, Training Loss: 0.0023272084072232246, Validation Loss: 0.0023814323358237743, Learning Rate: 0.0004096000000000001\n",
      "Epoch: 239, Training Loss: 0.0023947241716086864, Validation Loss: 0.0025731916539371014, Learning Rate: 0.0004096000000000001\n",
      "Epoch: 240, Training Loss: 0.0023581129498779774, Validation Loss: 0.0025777367409318686, Learning Rate: 0.0004096000000000001\n",
      "Epoch: 241, Training Loss: 0.002559757325798273, Validation Loss: 0.0027673894073814154, Learning Rate: 0.0004096000000000001\n",
      "Epoch: 242, Training Loss: 0.0025966092944145203, Validation Loss: 0.0031265488360077143, Learning Rate: 0.0004096000000000001\n",
      "Epoch: 243, Training Loss: 0.002905768109485507, Validation Loss: 0.0032179190311580896, Learning Rate: 0.0004096000000000001\n",
      "Epoch: 244, Training Loss: 0.0026424298994243145, Validation Loss: 0.002878607949241996, Learning Rate: 0.0004096000000000001\n",
      "Epoch: 245, Training Loss: 0.002657588804140687, Validation Loss: 0.0028602019883692265, Learning Rate: 0.0004096000000000001\n",
      "Epoch: 246, Training Loss: 0.0027111477684229612, Validation Loss: 0.0028872753027826548, Learning Rate: 0.0004096000000000001\n",
      "Epoch: 247, Training Loss: 0.0024440123233944178, Validation Loss: 0.002692427020519972, Learning Rate: 0.0004096000000000001\n",
      "Epoch: 248, Training Loss: 0.003049624152481556, Validation Loss: 0.0032797339372336864, Learning Rate: 0.0004096000000000001\n",
      "Epoch: 249, Training Loss: 0.0028720630798488855, Validation Loss: 0.003102279035374522, Learning Rate: 0.0004096000000000001\n",
      "Epoch: 250, Training Loss: 0.0031558957416564226, Validation Loss: 0.0028897984884679317, Learning Rate: 0.0003276800000000001\n",
      "Epoch: 251, Training Loss: 0.0029043925460428, Validation Loss: 0.002881196327507496, Learning Rate: 0.0003276800000000001\n",
      "Epoch: 252, Training Loss: 0.002527024829760194, Validation Loss: 0.002839756431058049, Learning Rate: 0.0003276800000000001\n",
      "Epoch: 253, Training Loss: 0.003201706800609827, Validation Loss: 0.0036943622399121523, Learning Rate: 0.0003276800000000001\n",
      "Epoch: 254, Training Loss: 0.0023987472523003817, Validation Loss: 0.0025716712698340416, Learning Rate: 0.0003276800000000001\n",
      "Epoch: 255, Training Loss: 0.0025556308683007956, Validation Loss: 0.0025695296935737133, Learning Rate: 0.0003276800000000001\n",
      "Epoch: 256, Training Loss: 0.0032132863998413086, Validation Loss: 0.003328173654153943, Learning Rate: 0.0003276800000000001\n",
      "Epoch: 257, Training Loss: 0.0031486472580581903, Validation Loss: 0.0035989051684737206, Learning Rate: 0.0003276800000000001\n",
      "Epoch: 258, Training Loss: 0.0028596939519047737, Validation Loss: 0.003357255831360817, Learning Rate: 0.0003276800000000001\n",
      "Epoch: 259, Training Loss: 0.0027312610764056444, Validation Loss: 0.003025080543011427, Learning Rate: 0.0003276800000000001\n",
      "Epoch: 260, Training Loss: 0.003621345851570368, Validation Loss: 0.004306898918002844, Learning Rate: 0.0003276800000000001\n",
      "Epoch: 261, Training Loss: 0.002467110985890031, Validation Loss: 0.0030465952586382627, Learning Rate: 0.0003276800000000001\n",
      "Epoch: 262, Training Loss: 0.0025017200969159603, Validation Loss: 0.0028819991275668144, Learning Rate: 0.0003276800000000001\n",
      "Epoch: 263, Training Loss: 0.0028450284153223038, Validation Loss: 0.0033804881386458874, Learning Rate: 0.0003276800000000001\n",
      "Epoch: 264, Training Loss: 0.0023985612206161022, Validation Loss: 0.0029841247014701366, Learning Rate: 0.0003276800000000001\n",
      "Epoch: 265, Training Loss: 0.0023143647704273462, Validation Loss: 0.002824146067723632, Learning Rate: 0.0003276800000000001\n",
      "Epoch: 266, Training Loss: 0.002197363181039691, Validation Loss: 0.0022351937368512154, Learning Rate: 0.0003276800000000001\n",
      "Epoch: 267, Training Loss: 0.002574545331299305, Validation Loss: 0.002700985409319401, Learning Rate: 0.0003276800000000001\n",
      "Epoch: 268, Training Loss: 0.002779334085062146, Validation Loss: 0.00301484321244061, Learning Rate: 0.0003276800000000001\n",
      "Epoch: 269, Training Loss: 0.0033932924270629883, Validation Loss: 0.003887553932145238, Learning Rate: 0.0003276800000000001\n",
      "Epoch: 270, Training Loss: 0.0036235584411770105, Validation Loss: 0.003888652892783284, Learning Rate: 0.0003276800000000001\n",
      "Epoch: 271, Training Loss: 0.0032688528299331665, Validation Loss: 0.0038623239379376173, Learning Rate: 0.0003276800000000001\n",
      "Epoch: 272, Training Loss: 0.003371973056346178, Validation Loss: 0.0036244289949536324, Learning Rate: 0.0003276800000000001\n",
      "Epoch: 273, Training Loss: 0.002903477754443884, Validation Loss: 0.0032305014319717884, Learning Rate: 0.0003276800000000001\n",
      "Epoch: 274, Training Loss: 0.002509894547984004, Validation Loss: 0.002678006887435913, Learning Rate: 0.0003276800000000001\n",
      "Epoch: 275, Training Loss: 0.0030131738167256117, Validation Loss: 0.003845878643915057, Learning Rate: 0.0003276800000000001\n",
      "Epoch: 276, Training Loss: 0.0024605083744972944, Validation Loss: 0.003000291995704174, Learning Rate: 0.0003276800000000001\n",
      "Epoch: 277, Training Loss: 0.003203765954822302, Validation Loss: 0.0032758261077106, Learning Rate: 0.0003276800000000001\n",
      "Epoch: 278, Training Loss: 0.0024366050492972136, Validation Loss: 0.0027787440922111273, Learning Rate: 0.0003276800000000001\n",
      "Epoch: 279, Training Loss: 0.003303455887362361, Validation Loss: 0.0036630171816796064, Learning Rate: 0.0003276800000000001\n",
      "Epoch: 280, Training Loss: 0.0021118607837706804, Validation Loss: 0.0023703938350081444, Learning Rate: 0.0003276800000000001\n",
      "Epoch: 281, Training Loss: 0.0022931487765163183, Validation Loss: 0.0024314443580806255, Learning Rate: 0.0003276800000000001\n",
      "Epoch: 282, Training Loss: 0.0024860554840415716, Validation Loss: 0.0025609578005969524, Learning Rate: 0.0003276800000000001\n",
      "Epoch: 283, Training Loss: 0.002453123452141881, Validation Loss: 0.002733345376327634, Learning Rate: 0.0003276800000000001\n",
      "Epoch: 284, Training Loss: 0.00360062881372869, Validation Loss: 0.003956309985369444, Learning Rate: 0.0003276800000000001\n",
      "Epoch: 285, Training Loss: 0.002655928721651435, Validation Loss: 0.0027458039112389088, Learning Rate: 0.0003276800000000001\n",
      "Epoch: 286, Training Loss: 0.002274086931720376, Validation Loss: 0.002469294238835573, Learning Rate: 0.0003276800000000001\n",
      "Epoch: 287, Training Loss: 0.0018814107170328498, Validation Loss: 0.0019476040033623576, Learning Rate: 0.0003276800000000001\n",
      "Epoch: 288, Training Loss: 0.002474459819495678, Validation Loss: 0.0025547947734594345, Learning Rate: 0.0003276800000000001\n",
      "Epoch: 289, Training Loss: 0.002370250876992941, Validation Loss: 0.002406912622973323, Learning Rate: 0.0003276800000000001\n",
      "Epoch: 290, Training Loss: 0.0020853232126682997, Validation Loss: 0.0021108118817210197, Learning Rate: 0.0003276800000000001\n",
      "Epoch: 291, Training Loss: 0.002117662923410535, Validation Loss: 0.0022327906917780638, Learning Rate: 0.0003276800000000001\n",
      "Epoch: 292, Training Loss: 0.0017097220988944173, Validation Loss: 0.001945112133398652, Learning Rate: 0.0003276800000000001\n",
      "Epoch: 293, Training Loss: 0.0030029769986867905, Validation Loss: 0.002947460860013962, Learning Rate: 0.0003276800000000001\n",
      "Epoch: 294, Training Loss: 0.002169262617826462, Validation Loss: 0.0022570386063307524, Learning Rate: 0.0003276800000000001\n",
      "Epoch: 295, Training Loss: 0.0024364597629755735, Validation Loss: 0.0025063524954020977, Learning Rate: 0.0003276800000000001\n",
      "Epoch: 296, Training Loss: 0.002014400903135538, Validation Loss: 0.0021954397670924664, Learning Rate: 0.0003276800000000001\n",
      "Epoch: 297, Training Loss: 0.0020759121980518103, Validation Loss: 0.002250411780551076, Learning Rate: 0.0003276800000000001\n",
      "Epoch: 298, Training Loss: 0.002204546006396413, Validation Loss: 0.0024227260146290064, Learning Rate: 0.0003276800000000001\n",
      "Epoch: 299, Training Loss: 0.0023909860756248236, Validation Loss: 0.002491601277142763, Learning Rate: 0.0003276800000000001\n",
      "Epoch: 300, Training Loss: 0.0019119087373837829, Validation Loss: 0.0020512400660663843, Learning Rate: 0.0002621440000000001\n",
      "Epoch: 301, Training Loss: 0.001807929016649723, Validation Loss: 0.0019598377402871847, Learning Rate: 0.0002621440000000001\n",
      "Epoch: 302, Training Loss: 0.0017441165400668979, Validation Loss: 0.001879945513792336, Learning Rate: 0.0002621440000000001\n",
      "Epoch: 303, Training Loss: 0.0016210366738960147, Validation Loss: 0.0017629637150093913, Learning Rate: 0.0002621440000000001\n",
      "Epoch: 304, Training Loss: 0.0017196604749187827, Validation Loss: 0.00184972258284688, Learning Rate: 0.0002621440000000001\n",
      "Epoch: 305, Training Loss: 0.00186271988786757, Validation Loss: 0.002220957074314356, Learning Rate: 0.0002621440000000001\n",
      "Epoch: 306, Training Loss: 0.002013161778450012, Validation Loss: 0.0023912587203085423, Learning Rate: 0.0002621440000000001\n",
      "Epoch: 307, Training Loss: 0.0018850191263481975, Validation Loss: 0.002372175455093384, Learning Rate: 0.0002621440000000001\n",
      "Epoch: 308, Training Loss: 0.0019755521789193153, Validation Loss: 0.0022247314918786287, Learning Rate: 0.0002621440000000001\n",
      "Epoch: 309, Training Loss: 0.001840007957071066, Validation Loss: 0.0020438090432435274, Learning Rate: 0.0002621440000000001\n",
      "Epoch: 310, Training Loss: 0.001967635238543153, Validation Loss: 0.0021140517201274633, Learning Rate: 0.0002621440000000001\n",
      "Epoch: 311, Training Loss: 0.002089770743623376, Validation Loss: 0.0023017949424684048, Learning Rate: 0.0002621440000000001\n",
      "Epoch: 312, Training Loss: 0.0017064828425645828, Validation Loss: 0.0018312872853130102, Learning Rate: 0.0002621440000000001\n",
      "Epoch: 313, Training Loss: 0.0015822749119251966, Validation Loss: 0.001758656813763082, Learning Rate: 0.0002621440000000001\n",
      "Epoch: 314, Training Loss: 0.0017438441282138228, Validation Loss: 0.0017287677619606256, Learning Rate: 0.0002621440000000001\n",
      "Epoch: 315, Training Loss: 0.0015479606809094548, Validation Loss: 0.0016395592829212546, Learning Rate: 0.0002621440000000001\n",
      "Epoch: 316, Training Loss: 0.0015807794407010078, Validation Loss: 0.001693514990620315, Learning Rate: 0.0002621440000000001\n",
      "Epoch: 317, Training Loss: 0.0018057528650388122, Validation Loss: 0.00184917903970927, Learning Rate: 0.0002621440000000001\n",
      "Epoch: 318, Training Loss: 0.0017094549257308245, Validation Loss: 0.0017915481003001332, Learning Rate: 0.0002621440000000001\n",
      "Epoch: 319, Training Loss: 0.0015827628085389733, Validation Loss: 0.0016333862440660596, Learning Rate: 0.0002621440000000001\n",
      "Epoch: 320, Training Loss: 0.001657099463045597, Validation Loss: 0.0017367603722959757, Learning Rate: 0.0002621440000000001\n",
      "Epoch: 321, Training Loss: 0.0015034343814477324, Validation Loss: 0.0016399932792410254, Learning Rate: 0.0002621440000000001\n",
      "Epoch: 322, Training Loss: 0.0016834365669637918, Validation Loss: 0.0019187129801139235, Learning Rate: 0.0002621440000000001\n",
      "Epoch: 323, Training Loss: 0.0017448693979531527, Validation Loss: 0.001931710634380579, Learning Rate: 0.0002621440000000001\n",
      "Epoch: 324, Training Loss: 0.0017549018375575542, Validation Loss: 0.001984757836908102, Learning Rate: 0.0002621440000000001\n",
      "Epoch: 325, Training Loss: 0.0018202378414571285, Validation Loss: 0.0019053927389904857, Learning Rate: 0.0002621440000000001\n",
      "Epoch: 326, Training Loss: 0.001718273852020502, Validation Loss: 0.0018550094682723284, Learning Rate: 0.0002621440000000001\n",
      "Epoch: 327, Training Loss: 0.002150533488020301, Validation Loss: 0.002703857608139515, Learning Rate: 0.0002621440000000001\n",
      "Epoch: 328, Training Loss: 0.0017784970114007592, Validation Loss: 0.0019137179479002953, Learning Rate: 0.0002621440000000001\n",
      "Epoch: 329, Training Loss: 0.0015372857451438904, Validation Loss: 0.0017982381395995617, Learning Rate: 0.0002621440000000001\n",
      "Epoch: 330, Training Loss: 0.0017284128116443753, Validation Loss: 0.001871383050456643, Learning Rate: 0.0002621440000000001\n",
      "Epoch: 331, Training Loss: 0.0016278825933113694, Validation Loss: 0.0016431179828941822, Learning Rate: 0.0002621440000000001\n",
      "Epoch: 332, Training Loss: 0.0018334859050810337, Validation Loss: 0.0019770939834415913, Learning Rate: 0.0002621440000000001\n",
      "Epoch: 333, Training Loss: 0.0018327183788642287, Validation Loss: 0.001968718133866787, Learning Rate: 0.0002621440000000001\n",
      "Epoch: 334, Training Loss: 0.0015900376019999385, Validation Loss: 0.0017564470181241632, Learning Rate: 0.0002621440000000001\n",
      "Epoch: 335, Training Loss: 0.0019753288943320513, Validation Loss: 0.0021337231155484915, Learning Rate: 0.0002621440000000001\n",
      "Epoch: 336, Training Loss: 0.0017843849491328, Validation Loss: 0.0018540003802627325, Learning Rate: 0.0002621440000000001\n",
      "Epoch: 337, Training Loss: 0.0021114025730639696, Validation Loss: 0.0023138043470680714, Learning Rate: 0.0002621440000000001\n",
      "Epoch: 338, Training Loss: 0.002023895736783743, Validation Loss: 0.0021779444068670273, Learning Rate: 0.0002621440000000001\n",
      "Epoch: 339, Training Loss: 0.0019534763414412737, Validation Loss: 0.0023917199578136206, Learning Rate: 0.0002621440000000001\n",
      "Epoch: 340, Training Loss: 0.0018559546442702413, Validation Loss: 0.0021972772665321827, Learning Rate: 0.0002621440000000001\n",
      "Epoch: 341, Training Loss: 0.0017606469336897135, Validation Loss: 0.002311891410499811, Learning Rate: 0.0002621440000000001\n",
      "Epoch: 342, Training Loss: 0.0016882651252672076, Validation Loss: 0.0019039555918425322, Learning Rate: 0.0002621440000000001\n",
      "Epoch: 343, Training Loss: 0.0018092343816533685, Validation Loss: 0.0019553303718566895, Learning Rate: 0.0002621440000000001\n",
      "Epoch: 344, Training Loss: 0.0017957863165065646, Validation Loss: 0.00229624449275434, Learning Rate: 0.0002621440000000001\n",
      "Epoch: 345, Training Loss: 0.002094770548865199, Validation Loss: 0.0028556936886161566, Learning Rate: 0.0002621440000000001\n",
      "Epoch: 346, Training Loss: 0.0019319935236126184, Validation Loss: 0.0027057381812483072, Learning Rate: 0.0002621440000000001\n",
      "Epoch: 347, Training Loss: 0.0016169227892532945, Validation Loss: 0.002258865861222148, Learning Rate: 0.0002621440000000001\n",
      "Epoch: 348, Training Loss: 0.0018997155129909515, Validation Loss: 0.0021065135952085257, Learning Rate: 0.0002621440000000001\n",
      "Epoch: 349, Training Loss: 0.0019169747829437256, Validation Loss: 0.0020227297209203243, Learning Rate: 0.0002621440000000001\n",
      "Epoch: 350, Training Loss: 0.0015854103257879615, Validation Loss: 0.0018471560906618834, Learning Rate: 0.00020971520000000012\n",
      "Epoch: 351, Training Loss: 0.0015756204957142472, Validation Loss: 0.002085349755361676, Learning Rate: 0.00020971520000000012\n",
      "Epoch: 352, Training Loss: 0.0014846321428194642, Validation Loss: 0.001995930913835764, Learning Rate: 0.00020971520000000012\n",
      "Epoch: 353, Training Loss: 0.001500805956311524, Validation Loss: 0.0017861973028630018, Learning Rate: 0.00020971520000000012\n",
      "Epoch: 354, Training Loss: 0.001304730074480176, Validation Loss: 0.0016090234275907278, Learning Rate: 0.00020971520000000012\n",
      "Epoch: 355, Training Loss: 0.0011835657060146332, Validation Loss: 0.0014468717854470015, Learning Rate: 0.00020971520000000012\n",
      "Epoch: 356, Training Loss: 0.0013216090155765414, Validation Loss: 0.0015895459800958633, Learning Rate: 0.00020971520000000012\n",
      "Epoch: 357, Training Loss: 0.0014964350266382098, Validation Loss: 0.0017009091097861528, Learning Rate: 0.00020971520000000012\n",
      "Epoch: 358, Training Loss: 0.00156506709754467, Validation Loss: 0.0020160083658993244, Learning Rate: 0.00020971520000000012\n",
      "Epoch: 359, Training Loss: 0.0015545919304713607, Validation Loss: 0.001955538522452116, Learning Rate: 0.00020971520000000012\n",
      "Epoch: 360, Training Loss: 0.0015773772029206157, Validation Loss: 0.002104580868035555, Learning Rate: 0.00020971520000000012\n",
      "Epoch: 361, Training Loss: 0.001452369848266244, Validation Loss: 0.001715435879305005, Learning Rate: 0.00020971520000000012\n",
      "Epoch: 362, Training Loss: 0.001397794345393777, Validation Loss: 0.001655826112255454, Learning Rate: 0.00020971520000000012\n",
      "Epoch: 363, Training Loss: 0.0012101641623303294, Validation Loss: 0.001511945971287787, Learning Rate: 0.00020971520000000012\n",
      "Epoch: 364, Training Loss: 0.001318139024078846, Validation Loss: 0.0016414292622357607, Learning Rate: 0.00020971520000000012\n",
      "Epoch: 365, Training Loss: 0.0014852849999442697, Validation Loss: 0.0017825305694714189, Learning Rate: 0.00020971520000000012\n",
      "Epoch: 366, Training Loss: 0.0014712985139340162, Validation Loss: 0.0017512321937829256, Learning Rate: 0.00020971520000000012\n",
      "Epoch: 367, Training Loss: 0.0014295142609626055, Validation Loss: 0.001721945940516889, Learning Rate: 0.00020971520000000012\n",
      "Epoch: 368, Training Loss: 0.0013283134903758764, Validation Loss: 0.001616255845874548, Learning Rate: 0.00020971520000000012\n",
      "Epoch: 369, Training Loss: 0.001362696522846818, Validation Loss: 0.0016887332312762737, Learning Rate: 0.00020971520000000012\n",
      "Epoch: 370, Training Loss: 0.0017025412525981665, Validation Loss: 0.002086817752569914, Learning Rate: 0.00020971520000000012\n",
      "Epoch: 371, Training Loss: 0.0012781145051121712, Validation Loss: 0.0015204615192487836, Learning Rate: 0.00020971520000000012\n",
      "Epoch: 372, Training Loss: 0.0012802447890862823, Validation Loss: 0.0016774303512647748, Learning Rate: 0.00020971520000000012\n",
      "Epoch: 373, Training Loss: 0.0014265805948525667, Validation Loss: 0.0016003743512555957, Learning Rate: 0.00020971520000000012\n",
      "Epoch: 374, Training Loss: 0.0012962217442691326, Validation Loss: 0.0014619529247283936, Learning Rate: 0.00020971520000000012\n",
      "Epoch: 375, Training Loss: 0.0013171825557947159, Validation Loss: 0.0016256400849670172, Learning Rate: 0.00020971520000000012\n",
      "Epoch: 376, Training Loss: 0.002357752760872245, Validation Loss: 0.002617503982037306, Learning Rate: 0.00020971520000000012\n",
      "Epoch: 377, Training Loss: 0.0015824617585167289, Validation Loss: 0.0016956169856712222, Learning Rate: 0.00020971520000000012\n",
      "Epoch: 378, Training Loss: 0.0016859433380886912, Validation Loss: 0.001991510624065995, Learning Rate: 0.00020971520000000012\n",
      "Epoch: 379, Training Loss: 0.0012623907532542944, Validation Loss: 0.0015651917783543468, Learning Rate: 0.00020971520000000012\n",
      "Epoch: 380, Training Loss: 0.0014014207990840077, Validation Loss: 0.0015468134079128504, Learning Rate: 0.00020971520000000012\n",
      "Epoch: 381, Training Loss: 0.0016366378404200077, Validation Loss: 0.0017381402431055903, Learning Rate: 0.00020971520000000012\n",
      "Epoch: 382, Training Loss: 0.0014175261603668332, Validation Loss: 0.0015062772436067462, Learning Rate: 0.00020971520000000012\n",
      "Epoch: 383, Training Loss: 0.0013848964590579271, Validation Loss: 0.0013797834981232882, Learning Rate: 0.00020971520000000012\n",
      "Epoch: 384, Training Loss: 0.0012122432235628366, Validation Loss: 0.0014210996450856328, Learning Rate: 0.00020971520000000012\n",
      "Epoch: 385, Training Loss: 0.0011834572069346905, Validation Loss: 0.0014072275953367352, Learning Rate: 0.00020971520000000012\n",
      "Epoch: 386, Training Loss: 0.0012364243157207966, Validation Loss: 0.0015146767254918814, Learning Rate: 0.00020971520000000012\n",
      "Epoch: 387, Training Loss: 0.0012324453564360738, Validation Loss: 0.001557510462589562, Learning Rate: 0.00020971520000000012\n",
      "Epoch: 388, Training Loss: 0.001323162461631, Validation Loss: 0.0015606418019160628, Learning Rate: 0.00020971520000000012\n",
      "Epoch: 389, Training Loss: 0.0013549438444897532, Validation Loss: 0.0015058565186336637, Learning Rate: 0.00020971520000000012\n",
      "Epoch: 390, Training Loss: 0.0012264648685231805, Validation Loss: 0.001413368503563106, Learning Rate: 0.00020971520000000012\n",
      "Epoch: 391, Training Loss: 0.001243056496605277, Validation Loss: 0.0014037661021575332, Learning Rate: 0.00020971520000000012\n",
      "Epoch: 392, Training Loss: 0.0011983198346570134, Validation Loss: 0.0014010316226631403, Learning Rate: 0.00020971520000000012\n",
      "Epoch: 393, Training Loss: 0.0013903139624744654, Validation Loss: 0.0013659187825396657, Learning Rate: 0.00020971520000000012\n",
      "Epoch: 394, Training Loss: 0.0015319110825657845, Validation Loss: 0.0016212515765801072, Learning Rate: 0.00020971520000000012\n",
      "Epoch: 395, Training Loss: 0.0013919308548793197, Validation Loss: 0.0015005983877927065, Learning Rate: 0.00020971520000000012\n",
      "Epoch: 396, Training Loss: 0.00141752022318542, Validation Loss: 0.001542640384286642, Learning Rate: 0.00020971520000000012\n",
      "Epoch: 397, Training Loss: 0.0012312534963712096, Validation Loss: 0.0013816829305142164, Learning Rate: 0.00020971520000000012\n",
      "Epoch: 398, Training Loss: 0.0012188663240522146, Validation Loss: 0.0014168528141453862, Learning Rate: 0.00020971520000000012\n",
      "Epoch: 399, Training Loss: 0.00150699308142066, Validation Loss: 0.0015865456080064178, Learning Rate: 0.00020971520000000012\n",
      "Epoch: 400, Training Loss: 0.001216777483932674, Validation Loss: 0.0013401620090007782, Learning Rate: 0.0001677721600000001\n",
      "Epoch: 401, Training Loss: 0.001272176974453032, Validation Loss: 0.0013725622557103634, Learning Rate: 0.0001677721600000001\n",
      "Epoch: 402, Training Loss: 0.0011253186967223883, Validation Loss: 0.0012117853621020913, Learning Rate: 0.0001677721600000001\n",
      "Epoch: 403, Training Loss: 0.0013175487983971834, Validation Loss: 0.0014469452435150743, Learning Rate: 0.0001677721600000001\n",
      "Epoch: 404, Training Loss: 0.0011773102451115847, Validation Loss: 0.0013542569940909743, Learning Rate: 0.0001677721600000001\n",
      "Epoch: 405, Training Loss: 0.0012297419598326087, Validation Loss: 0.0015673501184210181, Learning Rate: 0.0001677721600000001\n",
      "Epoch: 406, Training Loss: 0.0012254214379936457, Validation Loss: 0.0013564724940806627, Learning Rate: 0.0001677721600000001\n",
      "Epoch: 407, Training Loss: 0.0015673890011385083, Validation Loss: 0.0017752357525750995, Learning Rate: 0.0001677721600000001\n",
      "Epoch: 408, Training Loss: 0.0012308887671679258, Validation Loss: 0.001419469597749412, Learning Rate: 0.0001677721600000001\n",
      "Epoch: 409, Training Loss: 0.0011755120940506458, Validation Loss: 0.001285291975364089, Learning Rate: 0.0001677721600000001\n",
      "Epoch: 410, Training Loss: 0.0015143401687964797, Validation Loss: 0.0014868880389258265, Learning Rate: 0.0001677721600000001\n",
      "Epoch: 411, Training Loss: 0.0012551476247608662, Validation Loss: 0.0012542034965008497, Learning Rate: 0.0001677721600000001\n",
      "Epoch: 412, Training Loss: 0.0015846546739339828, Validation Loss: 0.0015739357331767678, Learning Rate: 0.0001677721600000001\n",
      "Epoch: 413, Training Loss: 0.0012458802666515112, Validation Loss: 0.0014593838714063168, Learning Rate: 0.0001677721600000001\n",
      "Epoch: 414, Training Loss: 0.001203608699142933, Validation Loss: 0.0014605834148824215, Learning Rate: 0.0001677721600000001\n",
      "Epoch: 415, Training Loss: 0.001213960349559784, Validation Loss: 0.0015033234376460314, Learning Rate: 0.0001677721600000001\n",
      "Epoch: 416, Training Loss: 0.0013463214272633195, Validation Loss: 0.00153661472722888, Learning Rate: 0.0001677721600000001\n",
      "Epoch: 417, Training Loss: 0.0012594113359227777, Validation Loss: 0.00146299519110471, Learning Rate: 0.0001677721600000001\n",
      "Epoch: 418, Training Loss: 0.001206112909130752, Validation Loss: 0.0014217511052265763, Learning Rate: 0.0001677721600000001\n",
      "Epoch: 419, Training Loss: 0.001152346027083695, Validation Loss: 0.0014135140227153897, Learning Rate: 0.0001677721600000001\n",
      "Epoch: 420, Training Loss: 0.0012310302117839456, Validation Loss: 0.0012720219092443585, Learning Rate: 0.0001677721600000001\n",
      "Epoch: 421, Training Loss: 0.0012069898657500744, Validation Loss: 0.00139333913102746, Learning Rate: 0.0001677721600000001\n",
      "Epoch: 422, Training Loss: 0.0012190812267363071, Validation Loss: 0.0016434959834441543, Learning Rate: 0.0001677721600000001\n",
      "Epoch: 423, Training Loss: 0.0012576489243656397, Validation Loss: 0.0013547997223213315, Learning Rate: 0.0001677721600000001\n",
      "Epoch: 424, Training Loss: 0.0012642948422580957, Validation Loss: 0.0013499150518327951, Learning Rate: 0.0001677721600000001\n",
      "Epoch: 425, Training Loss: 0.0012297519715502858, Validation Loss: 0.0016751647926867008, Learning Rate: 0.0001677721600000001\n",
      "Epoch: 426, Training Loss: 0.0011892664479091763, Validation Loss: 0.0013082441873848438, Learning Rate: 0.0001677721600000001\n",
      "Epoch: 427, Training Loss: 0.001260446384549141, Validation Loss: 0.0014609376667067409, Learning Rate: 0.0001677721600000001\n",
      "Epoch: 428, Training Loss: 0.00118313601706177, Validation Loss: 0.0014159628190100193, Learning Rate: 0.0001677721600000001\n",
      "Epoch: 429, Training Loss: 0.0012917497660964727, Validation Loss: 0.0015673728194087744, Learning Rate: 0.0001677721600000001\n",
      "Epoch: 430, Training Loss: 0.0014941415283828974, Validation Loss: 0.0016352786915376782, Learning Rate: 0.0001677721600000001\n",
      "Epoch: 431, Training Loss: 0.001134482561610639, Validation Loss: 0.001439161947928369, Learning Rate: 0.0001677721600000001\n",
      "Epoch: 432, Training Loss: 0.001474256394430995, Validation Loss: 0.0014921568799763918, Learning Rate: 0.0001677721600000001\n",
      "Epoch: 433, Training Loss: 0.0019618154037743807, Validation Loss: 0.002059816848486662, Learning Rate: 0.0001677721600000001\n",
      "Epoch: 434, Training Loss: 0.0015306043205782771, Validation Loss: 0.0019501801580190659, Learning Rate: 0.0001677721600000001\n",
      "Epoch: 435, Training Loss: 0.0012676420155912638, Validation Loss: 0.001529914210550487, Learning Rate: 0.0001677721600000001\n",
      "Epoch: 436, Training Loss: 0.0012243610108271241, Validation Loss: 0.0015107708750292659, Learning Rate: 0.0001677721600000001\n",
      "Epoch: 437, Training Loss: 0.0012601560447365046, Validation Loss: 0.001413662568666041, Learning Rate: 0.0001677721600000001\n",
      "Epoch: 438, Training Loss: 0.0012247459962964058, Validation Loss: 0.0013008948881179094, Learning Rate: 0.0001677721600000001\n",
      "Epoch: 439, Training Loss: 0.0011076170485466719, Validation Loss: 0.0015137272421270609, Learning Rate: 0.0001677721600000001\n",
      "Epoch: 440, Training Loss: 0.001195587101392448, Validation Loss: 0.0014400279615074396, Learning Rate: 0.0001677721600000001\n",
      "Epoch: 441, Training Loss: 0.0012304994743317366, Validation Loss: 0.0014715077122673392, Learning Rate: 0.0001677721600000001\n",
      "Epoch: 442, Training Loss: 0.001260662218555808, Validation Loss: 0.0014648113865405321, Learning Rate: 0.0001677721600000001\n",
      "Epoch: 443, Training Loss: 0.0012562526389956474, Validation Loss: 0.0013979541836306453, Learning Rate: 0.0001677721600000001\n",
      "Epoch: 444, Training Loss: 0.001300870906561613, Validation Loss: 0.0014835475012660027, Learning Rate: 0.0001677721600000001\n",
      "Epoch: 445, Training Loss: 0.0013925004750490189, Validation Loss: 0.0015292118769139051, Learning Rate: 0.0001677721600000001\n",
      "Epoch: 446, Training Loss: 0.0011877500219270587, Validation Loss: 0.0013223292771726847, Learning Rate: 0.0001677721600000001\n",
      "Epoch: 447, Training Loss: 0.0010490097338333726, Validation Loss: 0.001230283989571035, Learning Rate: 0.0001677721600000001\n",
      "Epoch: 448, Training Loss: 0.0012406130554154515, Validation Loss: 0.0012732252944260836, Learning Rate: 0.0001677721600000001\n",
      "Epoch: 449, Training Loss: 0.0011385618709027767, Validation Loss: 0.001407619216479361, Learning Rate: 0.0001677721600000001\n",
      "Epoch: 450, Training Loss: 0.0010927151888608932, Validation Loss: 0.0015485319308936596, Learning Rate: 0.00013421772800000008\n",
      "Epoch: 451, Training Loss: 0.0011675688438117504, Validation Loss: 0.0016486436361446977, Learning Rate: 0.00013421772800000008\n",
      "Epoch: 452, Training Loss: 0.0009364240104332566, Validation Loss: 0.0011757335159927607, Learning Rate: 0.00013421772800000008\n",
      "Epoch: 453, Training Loss: 0.0012652818113565445, Validation Loss: 0.0015227110125124454, Learning Rate: 0.00013421772800000008\n",
      "Epoch: 454, Training Loss: 0.0009996878216043115, Validation Loss: 0.001179186045192182, Learning Rate: 0.00013421772800000008\n",
      "Epoch: 455, Training Loss: 0.0009016638505272567, Validation Loss: 0.001091657904908061, Learning Rate: 0.00013421772800000008\n",
      "Epoch: 456, Training Loss: 0.0009417766123078763, Validation Loss: 0.0010832187253981829, Learning Rate: 0.00013421772800000008\n",
      "Epoch: 457, Training Loss: 0.0010295675601810217, Validation Loss: 0.0011377271730452776, Learning Rate: 0.00013421772800000008\n",
      "Epoch: 458, Training Loss: 0.0011034656781703234, Validation Loss: 0.0013582395622506738, Learning Rate: 0.00013421772800000008\n",
      "Epoch: 459, Training Loss: 0.0008868582081049681, Validation Loss: 0.0011241226457059383, Learning Rate: 0.00013421772800000008\n",
      "Epoch: 460, Training Loss: 0.0009153947466984391, Validation Loss: 0.0012645806418731809, Learning Rate: 0.00013421772800000008\n",
      "Epoch: 461, Training Loss: 0.0009751779143698514, Validation Loss: 0.0013144916156306863, Learning Rate: 0.00013421772800000008\n",
      "Epoch: 462, Training Loss: 0.0011487513547763228, Validation Loss: 0.0015294522745534778, Learning Rate: 0.00013421772800000008\n",
      "Epoch: 463, Training Loss: 0.0010683281579986215, Validation Loss: 0.0013070887653157115, Learning Rate: 0.00013421772800000008\n",
      "Epoch: 464, Training Loss: 0.0010803447803482413, Validation Loss: 0.0012447084300220013, Learning Rate: 0.00013421772800000008\n",
      "Epoch: 465, Training Loss: 0.0010265136370435357, Validation Loss: 0.0013540687505155802, Learning Rate: 0.00013421772800000008\n",
      "Epoch: 466, Training Loss: 0.0009372016065753996, Validation Loss: 0.0010997784556820989, Learning Rate: 0.00013421772800000008\n",
      "Epoch: 467, Training Loss: 0.0008738117758184671, Validation Loss: 0.0011278858873993158, Learning Rate: 0.00013421772800000008\n",
      "Epoch: 468, Training Loss: 0.0009459872380830348, Validation Loss: 0.001148440525867045, Learning Rate: 0.00013421772800000008\n",
      "Epoch: 469, Training Loss: 0.0009170518023893237, Validation Loss: 0.0011653410037979484, Learning Rate: 0.00013421772800000008\n",
      "Epoch: 470, Training Loss: 0.0010233439970761538, Validation Loss: 0.0013253329088911414, Learning Rate: 0.00013421772800000008\n",
      "Epoch: 471, Training Loss: 0.0012887859484180808, Validation Loss: 0.0013475770829245448, Learning Rate: 0.00013421772800000008\n",
      "Epoch: 472, Training Loss: 0.0011931307381018996, Validation Loss: 0.001099995686672628, Learning Rate: 0.00013421772800000008\n",
      "Epoch: 473, Training Loss: 0.0011824010871350765, Validation Loss: 0.0012193442089483142, Learning Rate: 0.00013421772800000008\n",
      "Epoch: 474, Training Loss: 0.0011519354302436113, Validation Loss: 0.001159484265372157, Learning Rate: 0.00013421772800000008\n",
      "Epoch: 475, Training Loss: 0.0010369429364800453, Validation Loss: 0.001058085821568966, Learning Rate: 0.00013421772800000008\n",
      "Epoch: 476, Training Loss: 0.0012927671195939183, Validation Loss: 0.0015568881062790751, Learning Rate: 0.00013421772800000008\n",
      "Epoch: 477, Training Loss: 0.0009819651022553444, Validation Loss: 0.0011778032639995217, Learning Rate: 0.00013421772800000008\n",
      "Epoch: 478, Training Loss: 0.001076288754120469, Validation Loss: 0.0012465324252843857, Learning Rate: 0.00013421772800000008\n",
      "Epoch: 479, Training Loss: 0.0010014396393671632, Validation Loss: 0.0013266465393826365, Learning Rate: 0.00013421772800000008\n",
      "Epoch: 480, Training Loss: 0.0008928111637942493, Validation Loss: 0.0011964377481490374, Learning Rate: 0.00013421772800000008\n",
      "Epoch: 481, Training Loss: 0.0009671417064964771, Validation Loss: 0.0012491167290136218, Learning Rate: 0.00013421772800000008\n",
      "Epoch: 482, Training Loss: 0.0009525743662379682, Validation Loss: 0.0012582598719745874, Learning Rate: 0.00013421772800000008\n",
      "Epoch: 483, Training Loss: 0.0009668704005889595, Validation Loss: 0.0011873679468408227, Learning Rate: 0.00013421772800000008\n",
      "Epoch: 484, Training Loss: 0.0010908753611147404, Validation Loss: 0.0012672018492594361, Learning Rate: 0.00013421772800000008\n",
      "Epoch: 485, Training Loss: 0.001057626330293715, Validation Loss: 0.0012101432075724006, Learning Rate: 0.00013421772800000008\n",
      "Epoch: 486, Training Loss: 0.0009897443233057857, Validation Loss: 0.0013938798801973462, Learning Rate: 0.00013421772800000008\n",
      "Epoch: 487, Training Loss: 0.000976233568508178, Validation Loss: 0.0011337712639942765, Learning Rate: 0.00013421772800000008\n",
      "Epoch: 488, Training Loss: 0.001019502291455865, Validation Loss: 0.0013544348767027259, Learning Rate: 0.00013421772800000008\n",
      "Epoch: 489, Training Loss: 0.0011834482429549098, Validation Loss: 0.0018836291274055839, Learning Rate: 0.00013421772800000008\n",
      "Epoch: 490, Training Loss: 0.0009929919615387917, Validation Loss: 0.0015002699801698327, Learning Rate: 0.00013421772800000008\n",
      "Epoch: 491, Training Loss: 0.0011470357421785593, Validation Loss: 0.0018240991048514843, Learning Rate: 0.00013421772800000008\n",
      "Epoch: 492, Training Loss: 0.0013469201512634754, Validation Loss: 0.0016091576544567943, Learning Rate: 0.00013421772800000008\n",
      "Epoch: 493, Training Loss: 0.0015622965293005109, Validation Loss: 0.0018842407735064626, Learning Rate: 0.00013421772800000008\n",
      "Epoch: 494, Training Loss: 0.0012544136261567473, Validation Loss: 0.001720162807032466, Learning Rate: 0.00013421772800000008\n",
      "Epoch: 495, Training Loss: 0.0011994654778391123, Validation Loss: 0.001964246155694127, Learning Rate: 0.00013421772800000008\n",
      "Epoch: 496, Training Loss: 0.0010958375642076135, Validation Loss: 0.0015810044715180993, Learning Rate: 0.00013421772800000008\n",
      "Epoch: 497, Training Loss: 0.0011321817291900516, Validation Loss: 0.0013674588408321142, Learning Rate: 0.00013421772800000008\n",
      "Epoch: 498, Training Loss: 0.0010612830519676208, Validation Loss: 0.0014082660200074315, Learning Rate: 0.00013421772800000008\n",
      "Epoch: 499, Training Loss: 0.0010538662318140268, Validation Loss: 0.0014762494247406721, Learning Rate: 0.00013421772800000008\n",
      "Epoch: 500, Training Loss: 0.0009519198792986572, Validation Loss: 0.0011428473517298698, Learning Rate: 0.00010737418240000007\n",
      "Epoch: 501, Training Loss: 0.0009426133474335074, Validation Loss: 0.0012030787765979767, Learning Rate: 0.00010737418240000007\n",
      "Epoch: 502, Training Loss: 0.001002868521027267, Validation Loss: 0.0012917692074552178, Learning Rate: 0.00010737418240000007\n",
      "Epoch: 503, Training Loss: 0.000981215969659388, Validation Loss: 0.0014848336577415466, Learning Rate: 0.00010737418240000007\n",
      "Epoch: 504, Training Loss: 0.0008730259723961353, Validation Loss: 0.0012682429514825344, Learning Rate: 0.00010737418240000007\n",
      "Epoch: 505, Training Loss: 0.0011980696581304073, Validation Loss: 0.0014219069853425026, Learning Rate: 0.00010737418240000007\n",
      "Epoch: 506, Training Loss: 0.0010234008077532053, Validation Loss: 0.0011708018137142062, Learning Rate: 0.00010737418240000007\n",
      "Epoch: 507, Training Loss: 0.0009122369810938835, Validation Loss: 0.001269828644581139, Learning Rate: 0.00010737418240000007\n",
      "Epoch: 508, Training Loss: 0.000798933207988739, Validation Loss: 0.001232589827850461, Learning Rate: 0.00010737418240000007\n",
      "Epoch: 509, Training Loss: 0.0008424986735917628, Validation Loss: 0.0013043832732364535, Learning Rate: 0.00010737418240000007\n",
      "Epoch: 510, Training Loss: 0.0007723442395217717, Validation Loss: 0.0011572242947295308, Learning Rate: 0.00010737418240000007\n",
      "Epoch: 511, Training Loss: 0.0008510979823768139, Validation Loss: 0.001264791819266975, Learning Rate: 0.00010737418240000007\n",
      "Epoch: 512, Training Loss: 0.0009053367539308965, Validation Loss: 0.001070050522685051, Learning Rate: 0.00010737418240000007\n",
      "Epoch: 513, Training Loss: 0.0008671611431054771, Validation Loss: 0.0010017524473369122, Learning Rate: 0.00010737418240000007\n",
      "Epoch: 514, Training Loss: 0.00089220330119133, Validation Loss: 0.0011004948755726218, Learning Rate: 0.00010737418240000007\n",
      "Epoch: 515, Training Loss: 0.0008849622099660337, Validation Loss: 0.0011334960581734776, Learning Rate: 0.00010737418240000007\n",
      "Epoch: 516, Training Loss: 0.0008175837574526668, Validation Loss: 0.0011579457204788923, Learning Rate: 0.00010737418240000007\n",
      "Epoch: 517, Training Loss: 0.0009358046809211373, Validation Loss: 0.0012531770626083016, Learning Rate: 0.00010737418240000007\n",
      "Epoch: 518, Training Loss: 0.0009460777509957552, Validation Loss: 0.0012970779789611697, Learning Rate: 0.00010737418240000007\n",
      "Epoch: 519, Training Loss: 0.0009418558329343796, Validation Loss: 0.0011660276213660836, Learning Rate: 0.00010737418240000007\n",
      "Epoch: 520, Training Loss: 0.0008400895167142153, Validation Loss: 0.0012160848127678037, Learning Rate: 0.00010737418240000007\n",
      "Epoch: 521, Training Loss: 0.0008570814970880747, Validation Loss: 0.0009743815753608942, Learning Rate: 0.00010737418240000007\n",
      "Epoch: 522, Training Loss: 0.0008507431484758854, Validation Loss: 0.0012632159050554037, Learning Rate: 0.00010737418240000007\n",
      "Epoch: 523, Training Loss: 0.0009824493899941444, Validation Loss: 0.0011426381533965468, Learning Rate: 0.00010737418240000007\n",
      "Epoch: 524, Training Loss: 0.0009024367900565267, Validation Loss: 0.001062088180333376, Learning Rate: 0.00010737418240000007\n",
      "Epoch: 525, Training Loss: 0.0013959966599941254, Validation Loss: 0.0016106796683743596, Learning Rate: 0.00010737418240000007\n",
      "Epoch: 526, Training Loss: 0.001061282935552299, Validation Loss: 0.0013791746459901333, Learning Rate: 0.00010737418240000007\n",
      "Epoch: 527, Training Loss: 0.0009913010289892554, Validation Loss: 0.00134557718411088, Learning Rate: 0.00010737418240000007\n",
      "Epoch: 528, Training Loss: 0.0010817217407748103, Validation Loss: 0.0012992332922294736, Learning Rate: 0.00010737418240000007\n",
      "Epoch: 529, Training Loss: 0.0010011240374296904, Validation Loss: 0.0012819435214623809, Learning Rate: 0.00010737418240000007\n",
      "Epoch: 530, Training Loss: 0.0009731003665365279, Validation Loss: 0.0012229481944814324, Learning Rate: 0.00010737418240000007\n",
      "Epoch: 531, Training Loss: 0.0008935976657085121, Validation Loss: 0.0012212272267788649, Learning Rate: 0.00010737418240000007\n",
      "Epoch: 532, Training Loss: 0.0009597378084436059, Validation Loss: 0.0013000558828935027, Learning Rate: 0.00010737418240000007\n",
      "Epoch: 533, Training Loss: 0.0008611039957031608, Validation Loss: 0.0011619128054007888, Learning Rate: 0.00010737418240000007\n",
      "Epoch: 534, Training Loss: 0.0008307588286697865, Validation Loss: 0.0011435155756771564, Learning Rate: 0.00010737418240000007\n",
      "Epoch: 535, Training Loss: 0.0008573351660743356, Validation Loss: 0.0010549216531217098, Learning Rate: 0.00010737418240000007\n",
      "Epoch: 536, Training Loss: 0.0008231941028498113, Validation Loss: 0.001023123855702579, Learning Rate: 0.00010737418240000007\n",
      "Epoch: 537, Training Loss: 0.0009004367748275399, Validation Loss: 0.0010337081039324403, Learning Rate: 0.00010737418240000007\n",
      "Epoch: 538, Training Loss: 0.0010063761146739125, Validation Loss: 0.0011132617946714163, Learning Rate: 0.00010737418240000007\n",
      "Epoch: 539, Training Loss: 0.0008165807230398059, Validation Loss: 0.0010423391358926892, Learning Rate: 0.00010737418240000007\n",
      "Epoch: 540, Training Loss: 0.0008314981823787093, Validation Loss: 0.001038152608089149, Learning Rate: 0.00010737418240000007\n",
      "Epoch: 541, Training Loss: 0.0008054012432694435, Validation Loss: 0.0009931226959452033, Learning Rate: 0.00010737418240000007\n",
      "Epoch: 542, Training Loss: 0.0008252972620539367, Validation Loss: 0.0011441002134233713, Learning Rate: 0.00010737418240000007\n",
      "Epoch: 543, Training Loss: 0.0008384524844586849, Validation Loss: 0.0009051914676092565, Learning Rate: 0.00010737418240000007\n",
      "Epoch: 544, Training Loss: 0.0007930371793918312, Validation Loss: 0.0010237426031380892, Learning Rate: 0.00010737418240000007\n",
      "Epoch: 545, Training Loss: 0.0009951289976015687, Validation Loss: 0.0015525881899520755, Learning Rate: 0.00010737418240000007\n",
      "Epoch: 546, Training Loss: 0.000982477213256061, Validation Loss: 0.0011540694395080209, Learning Rate: 0.00010737418240000007\n",
      "Epoch: 547, Training Loss: 0.000928707595448941, Validation Loss: 0.0011199024738743901, Learning Rate: 0.00010737418240000007\n",
      "Epoch: 548, Training Loss: 0.0009711416787467897, Validation Loss: 0.0009782030247151852, Learning Rate: 0.00010737418240000007\n",
      "Epoch: 549, Training Loss: 0.0008811384905129671, Validation Loss: 0.0010329517535865307, Learning Rate: 0.00010737418240000007\n",
      "Epoch: 550, Training Loss: 0.0009336231159977615, Validation Loss: 0.001092292950488627, Learning Rate: 8.589934592000007e-05\n",
      "Epoch: 551, Training Loss: 0.0008638114668428898, Validation Loss: 0.0012603463837876916, Learning Rate: 8.589934592000007e-05\n",
      "Epoch: 552, Training Loss: 0.0008145722094923258, Validation Loss: 0.001136665465310216, Learning Rate: 8.589934592000007e-05\n",
      "Epoch: 553, Training Loss: 0.0008556412649340928, Validation Loss: 0.0012160030892118812, Learning Rate: 8.589934592000007e-05\n",
      "Epoch: 554, Training Loss: 0.0009183950605802238, Validation Loss: 0.001044172910042107, Learning Rate: 8.589934592000007e-05\n",
      "Epoch: 555, Training Loss: 0.000868413073476404, Validation Loss: 0.0010925709502771497, Learning Rate: 8.589934592000007e-05\n",
      "Epoch: 556, Training Loss: 0.00100740569178015, Validation Loss: 0.001176037942059338, Learning Rate: 8.589934592000007e-05\n",
      "Epoch: 557, Training Loss: 0.0007865493535064161, Validation Loss: 0.000921786529943347, Learning Rate: 8.589934592000007e-05\n",
      "Epoch: 558, Training Loss: 0.0007118603680282831, Validation Loss: 0.0007576969801448286, Learning Rate: 8.589934592000007e-05\n",
      "Epoch: 559, Training Loss: 0.0008075074292719364, Validation Loss: 0.0009093801490962505, Learning Rate: 8.589934592000007e-05\n",
      "Epoch: 560, Training Loss: 0.0007904726662673056, Validation Loss: 0.0008640455198474228, Learning Rate: 8.589934592000007e-05\n",
      "Epoch: 561, Training Loss: 0.0007359589799307287, Validation Loss: 0.000736937508918345, Learning Rate: 8.589934592000007e-05\n",
      "Epoch: 562, Training Loss: 0.0007017399766482413, Validation Loss: 0.0006668262649327517, Learning Rate: 8.589934592000007e-05\n",
      "Epoch: 563, Training Loss: 0.0007371444953605533, Validation Loss: 0.0006721612880937755, Learning Rate: 8.589934592000007e-05\n",
      "Epoch: 564, Training Loss: 0.000650326139293611, Validation Loss: 0.0006384449079632759, Learning Rate: 8.589934592000007e-05\n",
      "Epoch: 565, Training Loss: 0.0007563444087281823, Validation Loss: 0.000738296308554709, Learning Rate: 8.589934592000007e-05\n",
      "Epoch: 566, Training Loss: 0.000708657898940146, Validation Loss: 0.0007742226007394493, Learning Rate: 8.589934592000007e-05\n",
      "Epoch: 567, Training Loss: 0.0007754175458103418, Validation Loss: 0.0006778225651942194, Learning Rate: 8.589934592000007e-05\n",
      "Epoch: 568, Training Loss: 0.0007787337526679039, Validation Loss: 0.0008630263619124889, Learning Rate: 8.589934592000007e-05\n",
      "Epoch: 569, Training Loss: 0.0007179444073699415, Validation Loss: 0.0007813224801793694, Learning Rate: 8.589934592000007e-05\n",
      "Epoch: 570, Training Loss: 0.0007710034842602909, Validation Loss: 0.0008366908878087997, Learning Rate: 8.589934592000007e-05\n",
      "Epoch: 571, Training Loss: 0.0007307357736863196, Validation Loss: 0.0007833198178559542, Learning Rate: 8.589934592000007e-05\n",
      "Epoch: 572, Training Loss: 0.0006780419498682022, Validation Loss: 0.0008009530138224363, Learning Rate: 8.589934592000007e-05\n",
      "Epoch: 573, Training Loss: 0.0007669774349778891, Validation Loss: 0.0008530750055797398, Learning Rate: 8.589934592000007e-05\n",
      "Epoch: 574, Training Loss: 0.0007871190900914371, Validation Loss: 0.0009775509824976325, Learning Rate: 8.589934592000007e-05\n",
      "Epoch: 575, Training Loss: 0.0007887817919254303, Validation Loss: 0.0009583421051502228, Learning Rate: 8.589934592000007e-05\n",
      "Epoch: 576, Training Loss: 0.0011575659736990929, Validation Loss: 0.001319591305218637, Learning Rate: 8.589934592000007e-05\n",
      "Epoch: 577, Training Loss: 0.0007794192060828209, Validation Loss: 0.0009370361804030836, Learning Rate: 8.589934592000007e-05\n",
      "Epoch: 578, Training Loss: 0.0007912032888270915, Validation Loss: 0.000927897694054991, Learning Rate: 8.589934592000007e-05\n",
      "Epoch: 579, Training Loss: 0.0007317921263165772, Validation Loss: 0.0009171226411126554, Learning Rate: 8.589934592000007e-05\n",
      "Epoch: 580, Training Loss: 0.0009415462845936418, Validation Loss: 0.0010804174235090613, Learning Rate: 8.589934592000007e-05\n",
      "Epoch: 581, Training Loss: 0.0009271173039451241, Validation Loss: 0.0010343159083276987, Learning Rate: 8.589934592000007e-05\n",
      "Epoch: 582, Training Loss: 0.0008834636537358165, Validation Loss: 0.0010174561757594347, Learning Rate: 8.589934592000007e-05\n",
      "Epoch: 583, Training Loss: 0.0007313864189200103, Validation Loss: 0.0009082265896722674, Learning Rate: 8.589934592000007e-05\n",
      "Epoch: 584, Training Loss: 0.0007202755659818649, Validation Loss: 0.0009250981383956969, Learning Rate: 8.589934592000007e-05\n",
      "Epoch: 585, Training Loss: 0.0009060256998054683, Validation Loss: 0.001004683435894549, Learning Rate: 8.589934592000007e-05\n",
      "Epoch: 586, Training Loss: 0.0007760724402032793, Validation Loss: 0.0012205435195937753, Learning Rate: 8.589934592000007e-05\n",
      "Epoch: 587, Training Loss: 0.0007725916802883148, Validation Loss: 0.0011254808632656932, Learning Rate: 8.589934592000007e-05\n",
      "Epoch: 588, Training Loss: 0.0009003016748465598, Validation Loss: 0.0011742719216272235, Learning Rate: 8.589934592000007e-05\n",
      "Epoch: 589, Training Loss: 0.0008336460450664163, Validation Loss: 0.0010288809426128864, Learning Rate: 8.589934592000007e-05\n",
      "Epoch: 590, Training Loss: 0.0007343957549892366, Validation Loss: 0.0009161638445220888, Learning Rate: 8.589934592000007e-05\n",
      "Epoch: 591, Training Loss: 0.0008909949683584273, Validation Loss: 0.001249066088348627, Learning Rate: 8.589934592000007e-05\n",
      "Epoch: 592, Training Loss: 0.0007243803702294827, Validation Loss: 0.0009317743824794888, Learning Rate: 8.589934592000007e-05\n",
      "Epoch: 593, Training Loss: 0.0007087811245582998, Validation Loss: 0.0010016554733738303, Learning Rate: 8.589934592000007e-05\n",
      "Epoch: 594, Training Loss: 0.0006979809259064496, Validation Loss: 0.0009725279523991048, Learning Rate: 8.589934592000007e-05\n",
      "Epoch: 595, Training Loss: 0.0008108671172522008, Validation Loss: 0.0009222411899827421, Learning Rate: 8.589934592000007e-05\n",
      "Epoch: 596, Training Loss: 0.0007807270158082247, Validation Loss: 0.0009023900493048131, Learning Rate: 8.589934592000007e-05\n",
      "Epoch: 597, Training Loss: 0.0008329016272909939, Validation Loss: 0.0008753170841373503, Learning Rate: 8.589934592000007e-05\n",
      "Epoch: 598, Training Loss: 0.000791484781075269, Validation Loss: 0.0008342141518369317, Learning Rate: 8.589934592000007e-05\n",
      "Epoch: 599, Training Loss: 0.0008378148195333779, Validation Loss: 0.0009447655756957829, Learning Rate: 8.589934592000007e-05\n",
      "Epoch: 600, Training Loss: 0.0007389335660263896, Validation Loss: 0.0009362819837406278, Learning Rate: 6.871947673600006e-05\n",
      "Epoch: 601, Training Loss: 0.0007318728603422642, Validation Loss: 0.0008255486027337611, Learning Rate: 6.871947673600006e-05\n",
      "Epoch: 602, Training Loss: 0.0007283283630385995, Validation Loss: 0.0008424644474871457, Learning Rate: 6.871947673600006e-05\n",
      "Epoch: 603, Training Loss: 0.0006717125070281327, Validation Loss: 0.0009595698211342096, Learning Rate: 6.871947673600006e-05\n",
      "Epoch: 604, Training Loss: 0.0012161562917754054, Validation Loss: 0.001217010198161006, Learning Rate: 6.871947673600006e-05\n",
      "Epoch: 605, Training Loss: 0.0007836958393454552, Validation Loss: 0.0008828539866954088, Learning Rate: 6.871947673600006e-05\n",
      "Epoch: 606, Training Loss: 0.0008271316182799637, Validation Loss: 0.001138805877417326, Learning Rate: 6.871947673600006e-05\n",
      "Epoch: 607, Training Loss: 0.0007200076943263412, Validation Loss: 0.001152892247773707, Learning Rate: 6.871947673600006e-05\n",
      "Epoch: 608, Training Loss: 0.0007062139338813722, Validation Loss: 0.0012565694050863385, Learning Rate: 6.871947673600006e-05\n",
      "Epoch: 609, Training Loss: 0.0006875595427118242, Validation Loss: 0.0010290185455232859, Learning Rate: 6.871947673600006e-05\n",
      "Epoch: 610, Training Loss: 0.0006725863204337656, Validation Loss: 0.001133963349275291, Learning Rate: 6.871947673600006e-05\n",
      "Epoch: 611, Training Loss: 0.0007815647404640913, Validation Loss: 0.0012648514239117503, Learning Rate: 6.871947673600006e-05\n",
      "Epoch: 612, Training Loss: 0.000717350107152015, Validation Loss: 0.0011055415961891413, Learning Rate: 6.871947673600006e-05\n",
      "Epoch: 613, Training Loss: 0.0007022743811830878, Validation Loss: 0.0010803885525092483, Learning Rate: 6.871947673600006e-05\n",
      "Epoch: 614, Training Loss: 0.0006788093014620245, Validation Loss: 0.0009908740175887942, Learning Rate: 6.871947673600006e-05\n",
      "Epoch: 615, Training Loss: 0.0006796747911721468, Validation Loss: 0.001031379448249936, Learning Rate: 6.871947673600006e-05\n",
      "Epoch: 616, Training Loss: 0.0006287919823080301, Validation Loss: 0.000947077467571944, Learning Rate: 6.871947673600006e-05\n",
      "Epoch: 617, Training Loss: 0.0010122189996764064, Validation Loss: 0.0015159986214712262, Learning Rate: 6.871947673600006e-05\n",
      "Epoch: 618, Training Loss: 0.0007209557807072997, Validation Loss: 0.0011036621872335672, Learning Rate: 6.871947673600006e-05\n",
      "Epoch: 619, Training Loss: 0.0006588653195649385, Validation Loss: 0.0010150013258680701, Learning Rate: 6.871947673600006e-05\n",
      "Epoch: 620, Training Loss: 0.000674813985824585, Validation Loss: 0.001005464349873364, Learning Rate: 6.871947673600006e-05\n",
      "Epoch: 621, Training Loss: 0.0006247871206142008, Validation Loss: 0.0008798447088338435, Learning Rate: 6.871947673600006e-05\n",
      "Epoch: 622, Training Loss: 0.0006302787805907428, Validation Loss: 0.00083542539505288, Learning Rate: 6.871947673600006e-05\n",
      "Epoch: 623, Training Loss: 0.0005980805144645274, Validation Loss: 0.0006686209235340357, Learning Rate: 6.871947673600006e-05\n",
      "Epoch: 624, Training Loss: 0.0009953381959348917, Validation Loss: 0.0010698351543396711, Learning Rate: 6.871947673600006e-05\n",
      "Epoch: 625, Training Loss: 0.0007485418464057148, Validation Loss: 0.0007992550381459296, Learning Rate: 6.871947673600006e-05\n",
      "Epoch: 626, Training Loss: 0.000672919035423547, Validation Loss: 0.0007728086784482002, Learning Rate: 6.871947673600006e-05\n",
      "Epoch: 627, Training Loss: 0.0006568298558704555, Validation Loss: 0.0007823326159268618, Learning Rate: 6.871947673600006e-05\n",
      "Epoch: 628, Training Loss: 0.0005862049292773008, Validation Loss: 0.0007229635375551879, Learning Rate: 6.871947673600006e-05\n",
      "Epoch: 629, Training Loss: 0.0006836793036200106, Validation Loss: 0.000837211380712688, Learning Rate: 6.871947673600006e-05\n",
      "Epoch: 630, Training Loss: 0.0006677993224002421, Validation Loss: 0.0009493874968029559, Learning Rate: 6.871947673600006e-05\n",
      "Epoch: 631, Training Loss: 0.0005926985759288073, Validation Loss: 0.0007712659426033497, Learning Rate: 6.871947673600006e-05\n",
      "Epoch: 632, Training Loss: 0.0006991111440584064, Validation Loss: 0.0008978506084531546, Learning Rate: 6.871947673600006e-05\n",
      "Epoch: 633, Training Loss: 0.0006489427178166807, Validation Loss: 0.0007323818281292915, Learning Rate: 6.871947673600006e-05\n",
      "Epoch: 634, Training Loss: 0.0006078789010643959, Validation Loss: 0.0007532951422035694, Learning Rate: 6.871947673600006e-05\n",
      "Epoch: 635, Training Loss: 0.0006082287291064858, Validation Loss: 0.0006911835516802967, Learning Rate: 6.871947673600006e-05\n",
      "Epoch: 636, Training Loss: 0.0006603703950531781, Validation Loss: 0.0008122281869873405, Learning Rate: 6.871947673600006e-05\n",
      "Epoch: 637, Training Loss: 0.0005872949841432273, Validation Loss: 0.0006929339724592865, Learning Rate: 6.871947673600006e-05\n",
      "Epoch: 638, Training Loss: 0.0006274378974922001, Validation Loss: 0.000672381604090333, Learning Rate: 6.871947673600006e-05\n",
      "Epoch: 639, Training Loss: 0.0007257572724483907, Validation Loss: 0.0008880152017809451, Learning Rate: 6.871947673600006e-05\n",
      "Epoch: 640, Training Loss: 0.0007099981303326786, Validation Loss: 0.001004119636490941, Learning Rate: 6.871947673600006e-05\n",
      "Epoch: 641, Training Loss: 0.0006566562806256115, Validation Loss: 0.000950177083723247, Learning Rate: 6.871947673600006e-05\n",
      "Epoch: 642, Training Loss: 0.0006642931257374585, Validation Loss: 0.0009641502983868122, Learning Rate: 6.871947673600006e-05\n",
      "Epoch: 643, Training Loss: 0.0006630843272432685, Validation Loss: 0.0008888007723726332, Learning Rate: 6.871947673600006e-05\n",
      "Epoch: 644, Training Loss: 0.0006405642488971353, Validation Loss: 0.0007278735865838826, Learning Rate: 6.871947673600006e-05\n",
      "Epoch: 645, Training Loss: 0.0006509745144285262, Validation Loss: 0.0007448909454979002, Learning Rate: 6.871947673600006e-05\n",
      "Epoch: 646, Training Loss: 0.0011494957143440843, Validation Loss: 0.0009787349263206124, Learning Rate: 6.871947673600006e-05\n",
      "Epoch: 647, Training Loss: 0.0008351312135346234, Validation Loss: 0.0008527341997250915, Learning Rate: 6.871947673600006e-05\n",
      "Epoch: 648, Training Loss: 0.0007830407121218741, Validation Loss: 0.0008179377182386816, Learning Rate: 6.871947673600006e-05\n",
      "Epoch: 649, Training Loss: 0.0007610106258653104, Validation Loss: 0.0008576532127335668, Learning Rate: 6.871947673600006e-05\n",
      "Epoch: 650, Training Loss: 0.0007317487616091967, Validation Loss: 0.0007734569371677935, Learning Rate: 5.497558138880005e-05\n",
      "Epoch: 651, Training Loss: 0.0007016293238848448, Validation Loss: 0.0007074277382344007, Learning Rate: 5.497558138880005e-05\n",
      "Epoch: 652, Training Loss: 0.0006367507157847285, Validation Loss: 0.0006972133414819837, Learning Rate: 5.497558138880005e-05\n",
      "Epoch: 653, Training Loss: 0.0005713578430004418, Validation Loss: 0.0006334040663205087, Learning Rate: 5.497558138880005e-05\n",
      "Epoch: 654, Training Loss: 0.0005843614344485104, Validation Loss: 0.0006761191762052476, Learning Rate: 5.497558138880005e-05\n",
      "Epoch: 655, Training Loss: 0.0005824815016239882, Validation Loss: 0.0006791746709495783, Learning Rate: 5.497558138880005e-05\n",
      "Epoch: 656, Training Loss: 0.0005542354192584753, Validation Loss: 0.000639673788100481, Learning Rate: 5.497558138880005e-05\n",
      "Epoch: 657, Training Loss: 0.0005516035598702729, Validation Loss: 0.0006253343890421093, Learning Rate: 5.497558138880005e-05\n",
      "Epoch: 658, Training Loss: 0.0005636080168187618, Validation Loss: 0.0006557907327078283, Learning Rate: 5.497558138880005e-05\n",
      "Epoch: 659, Training Loss: 0.0006661928491666913, Validation Loss: 0.0007904960075393319, Learning Rate: 5.497558138880005e-05\n",
      "Epoch: 660, Training Loss: 0.0006084738997742534, Validation Loss: 0.0006237002671696246, Learning Rate: 5.497558138880005e-05\n",
      "Epoch: 661, Training Loss: 0.0006153146969154477, Validation Loss: 0.0006598910549655557, Learning Rate: 5.497558138880005e-05\n",
      "Epoch: 662, Training Loss: 0.0006660299841314554, Validation Loss: 0.0007467616233043373, Learning Rate: 5.497558138880005e-05\n",
      "Epoch: 663, Training Loss: 0.000895725388545543, Validation Loss: 0.0015417450340464711, Learning Rate: 5.497558138880005e-05\n",
      "Epoch: 664, Training Loss: 0.0007714289240539074, Validation Loss: 0.0012259328505024314, Learning Rate: 5.497558138880005e-05\n",
      "Epoch: 665, Training Loss: 0.0007434235885739326, Validation Loss: 0.0011281592305749655, Learning Rate: 5.497558138880005e-05\n",
      "Epoch: 666, Training Loss: 0.0007345265476033092, Validation Loss: 0.0011004125699400902, Learning Rate: 5.497558138880005e-05\n",
      "Epoch: 667, Training Loss: 0.0007148921140469611, Validation Loss: 0.0011131528299301863, Learning Rate: 5.497558138880005e-05\n",
      "Epoch: 668, Training Loss: 0.0007476965547539294, Validation Loss: 0.001246602856554091, Learning Rate: 5.497558138880005e-05\n",
      "Epoch: 669, Training Loss: 0.0007136623025871813, Validation Loss: 0.0012844559969380498, Learning Rate: 5.497558138880005e-05\n",
      "Epoch: 670, Training Loss: 0.0007201462867669761, Validation Loss: 0.0012821840355172753, Learning Rate: 5.497558138880005e-05\n",
      "Epoch: 671, Training Loss: 0.0006900440203025937, Validation Loss: 0.001267744810320437, Learning Rate: 5.497558138880005e-05\n",
      "Epoch: 672, Training Loss: 0.0006884504691697657, Validation Loss: 0.001045679091475904, Learning Rate: 5.497558138880005e-05\n",
      "Epoch: 673, Training Loss: 0.0006961151375435293, Validation Loss: 0.0010623884154483676, Learning Rate: 5.497558138880005e-05\n",
      "Epoch: 674, Training Loss: 0.0006819081609137356, Validation Loss: 0.0010960602667182684, Learning Rate: 5.497558138880005e-05\n",
      "Epoch: 675, Training Loss: 0.0006825288292020559, Validation Loss: 0.0011448218720033765, Learning Rate: 5.497558138880005e-05\n",
      "Epoch: 676, Training Loss: 0.0007013371214270592, Validation Loss: 0.0010570768499746919, Learning Rate: 5.497558138880005e-05\n",
      "Epoch: 677, Training Loss: 0.000662486010696739, Validation Loss: 0.0010254698572680354, Learning Rate: 5.497558138880005e-05\n",
      "Epoch: 678, Training Loss: 0.0007244714652188122, Validation Loss: 0.001221817685291171, Learning Rate: 5.497558138880005e-05\n",
      "Epoch: 679, Training Loss: 0.0006430088542401791, Validation Loss: 0.000982231693342328, Learning Rate: 5.497558138880005e-05\n",
      "Epoch: 680, Training Loss: 0.0006965750944800675, Validation Loss: 0.0010512860026210546, Learning Rate: 5.497558138880005e-05\n",
      "Epoch: 681, Training Loss: 0.0006714481860399246, Validation Loss: 0.0011024874402210116, Learning Rate: 5.497558138880005e-05\n",
      "Epoch: 682, Training Loss: 0.0006452748202718794, Validation Loss: 0.0009461593581363559, Learning Rate: 5.497558138880005e-05\n",
      "Epoch: 683, Training Loss: 0.0006405294989235699, Validation Loss: 0.0010086280526593328, Learning Rate: 5.497558138880005e-05\n",
      "Epoch: 684, Training Loss: 0.0006233419990167022, Validation Loss: 0.0009530174429528415, Learning Rate: 5.497558138880005e-05\n",
      "Epoch: 685, Training Loss: 0.0007532195886597037, Validation Loss: 0.0009624914964661002, Learning Rate: 5.497558138880005e-05\n",
      "Epoch: 686, Training Loss: 0.0008054481004364789, Validation Loss: 0.0010750142391771078, Learning Rate: 5.497558138880005e-05\n",
      "Epoch: 687, Training Loss: 0.000737257651053369, Validation Loss: 0.0011476024519652128, Learning Rate: 5.497558138880005e-05\n",
      "Epoch: 688, Training Loss: 0.0007527217967435718, Validation Loss: 0.0010915907332673669, Learning Rate: 5.497558138880005e-05\n",
      "Epoch: 689, Training Loss: 0.0006473390385508537, Validation Loss: 0.0010048476979136467, Learning Rate: 5.497558138880005e-05\n",
      "Epoch: 690, Training Loss: 0.0006975690484978259, Validation Loss: 0.0011216073762625456, Learning Rate: 5.497558138880005e-05\n",
      "Epoch: 691, Training Loss: 0.0005902305128984153, Validation Loss: 0.00095438736025244, Learning Rate: 5.497558138880005e-05\n",
      "Epoch: 692, Training Loss: 0.0006524663767777383, Validation Loss: 0.001000398420728743, Learning Rate: 5.497558138880005e-05\n",
      "Epoch: 693, Training Loss: 0.0005980700370855629, Validation Loss: 0.0008652294054627419, Learning Rate: 5.497558138880005e-05\n",
      "Epoch: 694, Training Loss: 0.0005896398215554655, Validation Loss: 0.0008750645210966468, Learning Rate: 5.497558138880005e-05\n",
      "Epoch: 695, Training Loss: 0.0005765695823356509, Validation Loss: 0.0009228459093719721, Learning Rate: 5.497558138880005e-05\n",
      "Epoch: 696, Training Loss: 0.0005645122728310525, Validation Loss: 0.000858999730553478, Learning Rate: 5.497558138880005e-05\n",
      "Epoch: 697, Training Loss: 0.0005602544406428933, Validation Loss: 0.0008993290248326957, Learning Rate: 5.497558138880005e-05\n",
      "Epoch: 698, Training Loss: 0.0005882497644051909, Validation Loss: 0.0009559866157360375, Learning Rate: 5.497558138880005e-05\n",
      "Epoch: 699, Training Loss: 0.000627836852800101, Validation Loss: 0.0010441746562719345, Learning Rate: 5.497558138880005e-05\n",
      "Epoch: 700, Training Loss: 0.0005620062001980841, Validation Loss: 0.0008863308466970921, Learning Rate: 4.3980465111040044e-05\n",
      "Epoch: 701, Training Loss: 0.0006778897368349135, Validation Loss: 0.0009230567957274616, Learning Rate: 4.3980465111040044e-05\n",
      "Epoch: 702, Training Loss: 0.0005658447626046836, Validation Loss: 0.0008596227853558958, Learning Rate: 4.3980465111040044e-05\n",
      "Epoch: 703, Training Loss: 0.0005498883547261357, Validation Loss: 0.0008694067364558578, Learning Rate: 4.3980465111040044e-05\n",
      "Epoch: 704, Training Loss: 0.000577762140892446, Validation Loss: 0.0008403153624385595, Learning Rate: 4.3980465111040044e-05\n",
      "Epoch: 705, Training Loss: 0.000563086592592299, Validation Loss: 0.0008883537375368178, Learning Rate: 4.3980465111040044e-05\n",
      "Epoch: 706, Training Loss: 0.0006000922876410186, Validation Loss: 0.0008422021637670696, Learning Rate: 4.3980465111040044e-05\n",
      "Epoch: 707, Training Loss: 0.0007351008825935423, Validation Loss: 0.0010246388847008348, Learning Rate: 4.3980465111040044e-05\n",
      "Epoch: 708, Training Loss: 0.0006969741662032902, Validation Loss: 0.0009804038563743234, Learning Rate: 4.3980465111040044e-05\n",
      "Epoch: 709, Training Loss: 0.0006924655754119158, Validation Loss: 0.0009544522617943585, Learning Rate: 4.3980465111040044e-05\n",
      "Epoch: 710, Training Loss: 0.0006768478779122233, Validation Loss: 0.001060930429957807, Learning Rate: 4.3980465111040044e-05\n",
      "Epoch: 711, Training Loss: 0.0006527982768602669, Validation Loss: 0.0010004080832004547, Learning Rate: 4.3980465111040044e-05\n",
      "Epoch: 712, Training Loss: 0.0006650216528214514, Validation Loss: 0.0009524005581624806, Learning Rate: 4.3980465111040044e-05\n",
      "Epoch: 713, Training Loss: 0.0006381137063726783, Validation Loss: 0.0009606681996956468, Learning Rate: 4.3980465111040044e-05\n",
      "Epoch: 714, Training Loss: 0.0006763413548469543, Validation Loss: 0.001125676673837006, Learning Rate: 4.3980465111040044e-05\n",
      "Epoch: 715, Training Loss: 0.0006187523249536753, Validation Loss: 0.0008849422447383404, Learning Rate: 4.3980465111040044e-05\n",
      "Epoch: 716, Training Loss: 0.0006540590547956526, Validation Loss: 0.0007871320121921599, Learning Rate: 4.3980465111040044e-05\n",
      "Epoch: 717, Training Loss: 0.0006185664678923786, Validation Loss: 0.0007634899229742587, Learning Rate: 4.3980465111040044e-05\n",
      "Epoch: 718, Training Loss: 0.0006056553684175014, Validation Loss: 0.0007863017963245511, Learning Rate: 4.3980465111040044e-05\n",
      "Epoch: 719, Training Loss: 0.0005913298227824271, Validation Loss: 0.0007681699353270233, Learning Rate: 4.3980465111040044e-05\n",
      "Epoch: 720, Training Loss: 0.0005519012920558453, Validation Loss: 0.0007920994539745152, Learning Rate: 4.3980465111040044e-05\n",
      "Epoch: 721, Training Loss: 0.0005796366604045033, Validation Loss: 0.0008604975882917643, Learning Rate: 4.3980465111040044e-05\n",
      "Epoch: 722, Training Loss: 0.0005627460195682943, Validation Loss: 0.0008317296160385013, Learning Rate: 4.3980465111040044e-05\n",
      "Epoch: 723, Training Loss: 0.0005911189946345985, Validation Loss: 0.0007873511058278382, Learning Rate: 4.3980465111040044e-05\n",
      "Epoch: 724, Training Loss: 0.0005631099338643253, Validation Loss: 0.0008534276857972145, Learning Rate: 4.3980465111040044e-05\n",
      "Epoch: 725, Training Loss: 0.000566374568734318, Validation Loss: 0.0008140499121509492, Learning Rate: 4.3980465111040044e-05\n",
      "Epoch: 726, Training Loss: 0.0006405533640645444, Validation Loss: 0.000805599382147193, Learning Rate: 4.3980465111040044e-05\n",
      "Epoch: 727, Training Loss: 0.0005637221038341522, Validation Loss: 0.0007445511873811483, Learning Rate: 4.3980465111040044e-05\n",
      "Epoch: 728, Training Loss: 0.0005469496827572584, Validation Loss: 0.0006921469466760755, Learning Rate: 4.3980465111040044e-05\n",
      "Epoch: 729, Training Loss: 0.0005550298956222832, Validation Loss: 0.000750293314922601, Learning Rate: 4.3980465111040044e-05\n",
      "Epoch: 730, Training Loss: 0.0007148875738494098, Validation Loss: 0.0010696356184780598, Learning Rate: 4.3980465111040044e-05\n",
      "Epoch: 731, Training Loss: 0.0006116973236203194, Validation Loss: 0.0010732904775068164, Learning Rate: 4.3980465111040044e-05\n",
      "Epoch: 732, Training Loss: 0.0007055799942463636, Validation Loss: 0.0010608651209622622, Learning Rate: 4.3980465111040044e-05\n",
      "Epoch: 733, Training Loss: 0.0006680264486931264, Validation Loss: 0.000992214772850275, Learning Rate: 4.3980465111040044e-05\n",
      "Epoch: 734, Training Loss: 0.0006231546867638826, Validation Loss: 0.0009498407016508281, Learning Rate: 4.3980465111040044e-05\n",
      "Epoch: 735, Training Loss: 0.0006689928704872727, Validation Loss: 0.0008639553561806679, Learning Rate: 4.3980465111040044e-05\n",
      "Epoch: 736, Training Loss: 0.0007238145335577428, Validation Loss: 0.0009733920451253653, Learning Rate: 4.3980465111040044e-05\n",
      "Epoch: 737, Training Loss: 0.0006740824901498854, Validation Loss: 0.0009165517403744161, Learning Rate: 4.3980465111040044e-05\n",
      "Epoch: 738, Training Loss: 0.0006292631733231246, Validation Loss: 0.001003848621621728, Learning Rate: 4.3980465111040044e-05\n",
      "Epoch: 739, Training Loss: 0.000578675651922822, Validation Loss: 0.0008814297616481781, Learning Rate: 4.3980465111040044e-05\n",
      "Epoch: 740, Training Loss: 0.0006475758273154497, Validation Loss: 0.0009686864796094596, Learning Rate: 4.3980465111040044e-05\n",
      "Epoch: 741, Training Loss: 0.0006203827215358615, Validation Loss: 0.000940337311476469, Learning Rate: 4.3980465111040044e-05\n",
      "Epoch: 742, Training Loss: 0.0005959374248050153, Validation Loss: 0.0009582398342899978, Learning Rate: 4.3980465111040044e-05\n",
      "Epoch: 743, Training Loss: 0.0005701447953470051, Validation Loss: 0.0009438490378670394, Learning Rate: 4.3980465111040044e-05\n",
      "Epoch: 744, Training Loss: 0.0005798283964395523, Validation Loss: 0.0008572812657803297, Learning Rate: 4.3980465111040044e-05\n",
      "Epoch: 745, Training Loss: 0.0005720865447074175, Validation Loss: 0.0008913036435842514, Learning Rate: 4.3980465111040044e-05\n",
      "Epoch: 746, Training Loss: 0.000564464193303138, Validation Loss: 0.0009011767688207328, Learning Rate: 4.3980465111040044e-05\n",
      "Epoch: 747, Training Loss: 0.0005544791929423809, Validation Loss: 0.0009173729922622442, Learning Rate: 4.3980465111040044e-05\n",
      "Epoch: 748, Training Loss: 0.0005455758655443788, Validation Loss: 0.0007946567493490875, Learning Rate: 4.3980465111040044e-05\n",
      "Epoch: 749, Training Loss: 0.0005856292555108666, Validation Loss: 0.0009341269033029675, Learning Rate: 4.3980465111040044e-05\n",
      "Epoch: 750, Training Loss: 0.0005730917328037322, Validation Loss: 0.000851105316542089, Learning Rate: 3.5184372088832036e-05\n",
      "Epoch: 751, Training Loss: 0.0005452986806631088, Validation Loss: 0.0008645753259770572, Learning Rate: 3.5184372088832036e-05\n",
      "Epoch: 752, Training Loss: 0.0005477151717059314, Validation Loss: 0.0008966304012574255, Learning Rate: 3.5184372088832036e-05\n",
      "Epoch: 753, Training Loss: 0.000543872534763068, Validation Loss: 0.0008798785856924951, Learning Rate: 3.5184372088832036e-05\n",
      "Epoch: 754, Training Loss: 0.0005501100677065551, Validation Loss: 0.0009071413078345358, Learning Rate: 3.5184372088832036e-05\n",
      "Epoch: 755, Training Loss: 0.0005267876549623907, Validation Loss: 0.0008476717048324645, Learning Rate: 3.5184372088832036e-05\n",
      "Epoch: 756, Training Loss: 0.0005286217201501131, Validation Loss: 0.0008587791817262769, Learning Rate: 3.5184372088832036e-05\n",
      "Epoch: 757, Training Loss: 0.0005099474801681936, Validation Loss: 0.0008222508477047086, Learning Rate: 3.5184372088832036e-05\n",
      "Epoch: 758, Training Loss: 0.0005115098902024329, Validation Loss: 0.0008043565321713686, Learning Rate: 3.5184372088832036e-05\n",
      "Epoch: 759, Training Loss: 0.0004995186463929713, Validation Loss: 0.000786487536970526, Learning Rate: 3.5184372088832036e-05\n",
      "Epoch: 760, Training Loss: 0.0005045870784670115, Validation Loss: 0.0008112259092740715, Learning Rate: 3.5184372088832036e-05\n",
      "Epoch: 761, Training Loss: 0.0005028790328651667, Validation Loss: 0.0008023037225939333, Learning Rate: 3.5184372088832036e-05\n",
      "Epoch: 762, Training Loss: 0.0005240587051957846, Validation Loss: 0.0008081821142695844, Learning Rate: 3.5184372088832036e-05\n",
      "Epoch: 763, Training Loss: 0.0005171435768716037, Validation Loss: 0.0007685882155783474, Learning Rate: 3.5184372088832036e-05\n",
      "Epoch: 764, Training Loss: 0.000504101742990315, Validation Loss: 0.0007751099765300751, Learning Rate: 3.5184372088832036e-05\n",
      "Epoch: 765, Training Loss: 0.0004962937091477215, Validation Loss: 0.0007921499200165272, Learning Rate: 3.5184372088832036e-05\n",
      "Epoch: 766, Training Loss: 0.00048511248314753175, Validation Loss: 0.0007952357409521937, Learning Rate: 3.5184372088832036e-05\n",
      "Epoch: 767, Training Loss: 0.00051633280236274, Validation Loss: 0.0007028969121165574, Learning Rate: 3.5184372088832036e-05\n",
      "Epoch: 768, Training Loss: 0.0005246672662906349, Validation Loss: 0.0007079935166984797, Learning Rate: 3.5184372088832036e-05\n",
      "Epoch: 769, Training Loss: 0.0005084652220830321, Validation Loss: 0.0007491317810490727, Learning Rate: 3.5184372088832036e-05\n",
      "Epoch: 770, Training Loss: 0.0005301627679727972, Validation Loss: 0.0007275394164025784, Learning Rate: 3.5184372088832036e-05\n",
      "Epoch: 771, Training Loss: 0.0005120460991747677, Validation Loss: 0.0007317629060707986, Learning Rate: 3.5184372088832036e-05\n",
      "Epoch: 772, Training Loss: 0.0004913858720101416, Validation Loss: 0.000683969643432647, Learning Rate: 3.5184372088832036e-05\n",
      "Epoch: 773, Training Loss: 0.0004985775449313223, Validation Loss: 0.0006850159843452275, Learning Rate: 3.5184372088832036e-05\n",
      "Epoch: 774, Training Loss: 0.0004983484977856278, Validation Loss: 0.0006973575218580663, Learning Rate: 3.5184372088832036e-05\n",
      "Epoch: 775, Training Loss: 0.0005219124141149223, Validation Loss: 0.0007557417266070843, Learning Rate: 3.5184372088832036e-05\n",
      "Epoch: 776, Training Loss: 0.0005213782424107194, Validation Loss: 0.0007234345539472997, Learning Rate: 3.5184372088832036e-05\n",
      "Epoch: 777, Training Loss: 0.0005141625297255814, Validation Loss: 0.0006871012737974524, Learning Rate: 3.5184372088832036e-05\n",
      "Epoch: 778, Training Loss: 0.0005372374434955418, Validation Loss: 0.0006878410931676626, Learning Rate: 3.5184372088832036e-05\n",
      "Epoch: 779, Training Loss: 0.0005979743436910212, Validation Loss: 0.0007243602303788066, Learning Rate: 3.5184372088832036e-05\n",
      "Epoch: 780, Training Loss: 0.0005043590790592134, Validation Loss: 0.0006661582738161087, Learning Rate: 3.5184372088832036e-05\n",
      "Epoch: 781, Training Loss: 0.0005205682246014476, Validation Loss: 0.0006769567262381315, Learning Rate: 3.5184372088832036e-05\n",
      "Epoch: 782, Training Loss: 0.0005159463617019355, Validation Loss: 0.0006634563906118274, Learning Rate: 3.5184372088832036e-05\n",
      "Epoch: 783, Training Loss: 0.0005075377412140369, Validation Loss: 0.0006478246068581939, Learning Rate: 3.5184372088832036e-05\n",
      "Epoch: 784, Training Loss: 0.0004969622823409736, Validation Loss: 0.0006543969502672553, Learning Rate: 3.5184372088832036e-05\n",
      "Epoch: 785, Training Loss: 0.0004921755753457546, Validation Loss: 0.0006377996760420501, Learning Rate: 3.5184372088832036e-05\n",
      "Epoch: 786, Training Loss: 0.0005014037014916539, Validation Loss: 0.0006683103274554014, Learning Rate: 3.5184372088832036e-05\n",
      "Epoch: 787, Training Loss: 0.000500931462738663, Validation Loss: 0.0006770016043446958, Learning Rate: 3.5184372088832036e-05\n",
      "Epoch: 788, Training Loss: 0.0005302216159179807, Validation Loss: 0.0006709150038659573, Learning Rate: 3.5184372088832036e-05\n",
      "Epoch: 789, Training Loss: 0.0005243346677161753, Validation Loss: 0.0006256541819311678, Learning Rate: 3.5184372088832036e-05\n",
      "Epoch: 790, Training Loss: 0.0005171065567992628, Validation Loss: 0.000626293127425015, Learning Rate: 3.5184372088832036e-05\n",
      "Epoch: 791, Training Loss: 0.0004929355927743018, Validation Loss: 0.0006207344122231007, Learning Rate: 3.5184372088832036e-05\n",
      "Epoch: 792, Training Loss: 0.0005828388384543359, Validation Loss: 0.0006291468744166195, Learning Rate: 3.5184372088832036e-05\n",
      "Epoch: 793, Training Loss: 0.0005632473039440811, Validation Loss: 0.0005612493841908872, Learning Rate: 3.5184372088832036e-05\n",
      "Epoch: 794, Training Loss: 0.0005366590921767056, Validation Loss: 0.0005676239379681647, Learning Rate: 3.5184372088832036e-05\n",
      "Epoch: 795, Training Loss: 0.0005694152205251157, Validation Loss: 0.0006735046626999974, Learning Rate: 3.5184372088832036e-05\n",
      "Epoch: 796, Training Loss: 0.0005426715943031013, Validation Loss: 0.0006158869946375489, Learning Rate: 3.5184372088832036e-05\n",
      "Epoch: 797, Training Loss: 0.000513830513227731, Validation Loss: 0.0005453528137877584, Learning Rate: 3.5184372088832036e-05\n",
      "Epoch: 798, Training Loss: 0.0006569247925654054, Validation Loss: 0.0005925008445046842, Learning Rate: 3.5184372088832036e-05\n",
      "Epoch: 799, Training Loss: 0.0005657266592606902, Validation Loss: 0.0006214776658453047, Learning Rate: 3.5184372088832036e-05\n",
      "Epoch: 800, Training Loss: 0.0005069737671874464, Validation Loss: 0.0005375458858907223, Learning Rate: 2.814749767106563e-05\n",
      "Epoch: 801, Training Loss: 0.0005063831340521574, Validation Loss: 0.0005399410147219896, Learning Rate: 2.814749767106563e-05\n",
      "Epoch: 802, Training Loss: 0.000513816368766129, Validation Loss: 0.0005896060029044747, Learning Rate: 2.814749767106563e-05\n",
      "Epoch: 803, Training Loss: 0.0005042058764956892, Validation Loss: 0.0005362262018024921, Learning Rate: 2.814749767106563e-05\n",
      "Epoch: 804, Training Loss: 0.0004933025920763612, Validation Loss: 0.000585377449169755, Learning Rate: 2.814749767106563e-05\n",
      "Epoch: 805, Training Loss: 0.0007301069563254714, Validation Loss: 0.0008096686215139925, Learning Rate: 2.814749767106563e-05\n",
      "Epoch: 806, Training Loss: 0.0005342718795873225, Validation Loss: 0.000757188827265054, Learning Rate: 2.814749767106563e-05\n",
      "Epoch: 807, Training Loss: 0.0005709539400413632, Validation Loss: 0.0007362088654190302, Learning Rate: 2.814749767106563e-05\n",
      "Epoch: 808, Training Loss: 0.0005649883532896638, Validation Loss: 0.0008403744432143867, Learning Rate: 2.814749767106563e-05\n",
      "Epoch: 809, Training Loss: 0.0005207901704125106, Validation Loss: 0.0007774336263537407, Learning Rate: 2.814749767106563e-05\n",
      "Epoch: 810, Training Loss: 0.000541143526788801, Validation Loss: 0.0007954702596180141, Learning Rate: 2.814749767106563e-05\n",
      "Epoch: 811, Training Loss: 0.0005276008159853518, Validation Loss: 0.0008026121649891138, Learning Rate: 2.814749767106563e-05\n",
      "Epoch: 812, Training Loss: 0.0005229453672654927, Validation Loss: 0.0008257807930931449, Learning Rate: 2.814749767106563e-05\n",
      "Epoch: 813, Training Loss: 0.0005271778209134936, Validation Loss: 0.0007492514559999108, Learning Rate: 2.814749767106563e-05\n",
      "Epoch: 814, Training Loss: 0.0005125004099681973, Validation Loss: 0.000674000009894371, Learning Rate: 2.814749767106563e-05\n",
      "Epoch: 815, Training Loss: 0.0004896782920695841, Validation Loss: 0.000644237850792706, Learning Rate: 2.814749767106563e-05\n",
      "Epoch: 816, Training Loss: 0.00048105863970704377, Validation Loss: 0.0006577955209650099, Learning Rate: 2.814749767106563e-05\n",
      "Epoch: 817, Training Loss: 0.0004756417765747756, Validation Loss: 0.0006359729450196028, Learning Rate: 2.814749767106563e-05\n",
      "Epoch: 818, Training Loss: 0.0004708859196398407, Validation Loss: 0.0006433246890082955, Learning Rate: 2.814749767106563e-05\n",
      "Epoch: 819, Training Loss: 0.0004801084578502923, Validation Loss: 0.0006520064198412001, Learning Rate: 2.814749767106563e-05\n",
      "Epoch: 820, Training Loss: 0.00047219425323419273, Validation Loss: 0.0006479922449216247, Learning Rate: 2.814749767106563e-05\n",
      "Epoch: 821, Training Loss: 0.0004868849355261773, Validation Loss: 0.0006402970757335424, Learning Rate: 2.814749767106563e-05\n",
      "Epoch: 822, Training Loss: 0.00047304207691922784, Validation Loss: 0.0006266337586566806, Learning Rate: 2.814749767106563e-05\n",
      "Epoch: 823, Training Loss: 0.00047583034029230475, Validation Loss: 0.0006472414825111628, Learning Rate: 2.814749767106563e-05\n",
      "Epoch: 824, Training Loss: 0.0004776606219820678, Validation Loss: 0.0006510314415208995, Learning Rate: 2.814749767106563e-05\n",
      "Epoch: 825, Training Loss: 0.0004685597668867558, Validation Loss: 0.0006346377776935697, Learning Rate: 2.814749767106563e-05\n",
      "Epoch: 826, Training Loss: 0.0004702243022620678, Validation Loss: 0.0006271384772844613, Learning Rate: 2.814749767106563e-05\n",
      "Epoch: 827, Training Loss: 0.00047370270476676524, Validation Loss: 0.0006262966780923307, Learning Rate: 2.814749767106563e-05\n",
      "Epoch: 828, Training Loss: 0.00046330655459314585, Validation Loss: 0.0006360000697895885, Learning Rate: 2.814749767106563e-05\n",
      "Epoch: 829, Training Loss: 0.00048210128443315625, Validation Loss: 0.0006327624432742596, Learning Rate: 2.814749767106563e-05\n",
      "Epoch: 830, Training Loss: 0.0004765555786434561, Validation Loss: 0.000636559387203306, Learning Rate: 2.814749767106563e-05\n",
      "Epoch: 831, Training Loss: 0.0004735496186185628, Validation Loss: 0.0006517854635603726, Learning Rate: 2.814749767106563e-05\n",
      "Epoch: 832, Training Loss: 0.0004679055418819189, Validation Loss: 0.0006394482916221023, Learning Rate: 2.814749767106563e-05\n",
      "Epoch: 833, Training Loss: 0.0004711546062026173, Validation Loss: 0.0006479501025751233, Learning Rate: 2.814749767106563e-05\n",
      "Epoch: 834, Training Loss: 0.0004776867281179875, Validation Loss: 0.0006202722433954477, Learning Rate: 2.814749767106563e-05\n",
      "Epoch: 835, Training Loss: 0.0004687257169280201, Validation Loss: 0.0006158642354421318, Learning Rate: 2.814749767106563e-05\n",
      "Epoch: 836, Training Loss: 0.0004739246505778283, Validation Loss: 0.0006465270998887718, Learning Rate: 2.814749767106563e-05\n",
      "Epoch: 837, Training Loss: 0.000475698005175218, Validation Loss: 0.0006617277394980192, Learning Rate: 2.814749767106563e-05\n",
      "Epoch: 838, Training Loss: 0.00046287779696285725, Validation Loss: 0.0006528748199343681, Learning Rate: 2.814749767106563e-05\n",
      "Epoch: 839, Training Loss: 0.00045790764852426946, Validation Loss: 0.0006336967926472425, Learning Rate: 2.814749767106563e-05\n",
      "Epoch: 840, Training Loss: 0.0005403662798926234, Validation Loss: 0.0006872818921692669, Learning Rate: 2.814749767106563e-05\n",
      "Epoch: 841, Training Loss: 0.0005167183117009699, Validation Loss: 0.0006324393907561898, Learning Rate: 2.814749767106563e-05\n",
      "Epoch: 842, Training Loss: 0.0005536675453186035, Validation Loss: 0.0006477904971688986, Learning Rate: 2.814749767106563e-05\n",
      "Epoch: 843, Training Loss: 0.0005208868533372879, Validation Loss: 0.0006344098364934325, Learning Rate: 2.814749767106563e-05\n",
      "Epoch: 844, Training Loss: 0.0004906047834083438, Validation Loss: 0.0006259013316594064, Learning Rate: 2.814749767106563e-05\n",
      "Epoch: 845, Training Loss: 0.0005178224528208375, Validation Loss: 0.0006356145604513586, Learning Rate: 2.814749767106563e-05\n",
      "Epoch: 846, Training Loss: 0.0005111788050271571, Validation Loss: 0.0007480157655663788, Learning Rate: 2.814749767106563e-05\n",
      "Epoch: 847, Training Loss: 0.0004977281205356121, Validation Loss: 0.0005863473634235561, Learning Rate: 2.814749767106563e-05\n",
      "Epoch: 848, Training Loss: 0.0004942345549352467, Validation Loss: 0.000618492194917053, Learning Rate: 2.814749767106563e-05\n",
      "Epoch: 849, Training Loss: 0.0005164151079952717, Validation Loss: 0.0005866701831109822, Learning Rate: 2.814749767106563e-05\n",
      "Epoch: 850, Training Loss: 0.0004963312530890107, Validation Loss: 0.0006418165867216885, Learning Rate: 2.2517998136852506e-05\n",
      "Epoch: 851, Training Loss: 0.0004900938365608454, Validation Loss: 0.000575760961510241, Learning Rate: 2.2517998136852506e-05\n",
      "Epoch: 852, Training Loss: 0.00047393018030561507, Validation Loss: 0.0005961827700957656, Learning Rate: 2.2517998136852506e-05\n",
      "Epoch: 853, Training Loss: 0.00048642317415215075, Validation Loss: 0.0006254747859202325, Learning Rate: 2.2517998136852506e-05\n",
      "Epoch: 854, Training Loss: 0.0004840826441068202, Validation Loss: 0.000574762059841305, Learning Rate: 2.2517998136852506e-05\n",
      "Epoch: 855, Training Loss: 0.00047647178871557117, Validation Loss: 0.0005524606094695628, Learning Rate: 2.2517998136852506e-05\n",
      "Epoch: 856, Training Loss: 0.0004768606449943036, Validation Loss: 0.0005572435911744833, Learning Rate: 2.2517998136852506e-05\n",
      "Epoch: 857, Training Loss: 0.000478359404951334, Validation Loss: 0.000554583384655416, Learning Rate: 2.2517998136852506e-05\n",
      "Epoch: 858, Training Loss: 0.00047813859418965876, Validation Loss: 0.0005528796464204788, Learning Rate: 2.2517998136852506e-05\n",
      "Epoch: 859, Training Loss: 0.0004829837998840958, Validation Loss: 0.0005937676760368049, Learning Rate: 2.2517998136852506e-05\n",
      "Epoch: 860, Training Loss: 0.000468875135993585, Validation Loss: 0.0005748654948547482, Learning Rate: 2.2517998136852506e-05\n",
      "Epoch: 861, Training Loss: 0.00046518465387634933, Validation Loss: 0.0005895386566407979, Learning Rate: 2.2517998136852506e-05\n",
      "Epoch: 862, Training Loss: 0.000465410208562389, Validation Loss: 0.0005761902430094779, Learning Rate: 2.2517998136852506e-05\n",
      "Epoch: 863, Training Loss: 0.0004619334067683667, Validation Loss: 0.0005696117877960205, Learning Rate: 2.2517998136852506e-05\n",
      "Epoch: 864, Training Loss: 0.00046427163761109114, Validation Loss: 0.0005723052308894694, Learning Rate: 2.2517998136852506e-05\n",
      "Epoch: 865, Training Loss: 0.00046242494136095047, Validation Loss: 0.0005723034264519811, Learning Rate: 2.2517998136852506e-05\n",
      "Epoch: 866, Training Loss: 0.00048720260383561254, Validation Loss: 0.0005995762185193598, Learning Rate: 2.2517998136852506e-05\n",
      "Epoch: 867, Training Loss: 0.00046196067705750465, Validation Loss: 0.0005791771691292524, Learning Rate: 2.2517998136852506e-05\n",
      "Epoch: 868, Training Loss: 0.0004609009774867445, Validation Loss: 0.0005719732725992799, Learning Rate: 2.2517998136852506e-05\n",
      "Epoch: 869, Training Loss: 0.00048382068052887917, Validation Loss: 0.000590221316087991, Learning Rate: 2.2517998136852506e-05\n",
      "Epoch: 870, Training Loss: 0.000468357524368912, Validation Loss: 0.0006007597548887134, Learning Rate: 2.2517998136852506e-05\n",
      "Epoch: 871, Training Loss: 0.0004786702338606119, Validation Loss: 0.0005686645745299757, Learning Rate: 2.2517998136852506e-05\n",
      "Epoch: 872, Training Loss: 0.0004845346848014742, Validation Loss: 0.0005771043943241239, Learning Rate: 2.2517998136852506e-05\n",
      "Epoch: 873, Training Loss: 0.0004781323950737715, Validation Loss: 0.0006177785689942539, Learning Rate: 2.2517998136852506e-05\n",
      "Epoch: 874, Training Loss: 0.0004784745106007904, Validation Loss: 0.0005651263054460287, Learning Rate: 2.2517998136852506e-05\n",
      "Epoch: 875, Training Loss: 0.000467058940557763, Validation Loss: 0.0005638021975755692, Learning Rate: 2.2517998136852506e-05\n",
      "Epoch: 876, Training Loss: 0.00047738998546265066, Validation Loss: 0.0005505725275725126, Learning Rate: 2.2517998136852506e-05\n",
      "Epoch: 877, Training Loss: 0.00047157611697912216, Validation Loss: 0.0005743805086240172, Learning Rate: 2.2517998136852506e-05\n",
      "Epoch: 878, Training Loss: 0.00045921833952888846, Validation Loss: 0.0005503409192897379, Learning Rate: 2.2517998136852506e-05\n",
      "Epoch: 879, Training Loss: 0.0004765807243529707, Validation Loss: 0.0005830421578139067, Learning Rate: 2.2517998136852506e-05\n",
      "Epoch: 880, Training Loss: 0.00046982054482214153, Validation Loss: 0.000562179833650589, Learning Rate: 2.2517998136852506e-05\n",
      "Epoch: 881, Training Loss: 0.0004833170387428254, Validation Loss: 0.0005705140647478402, Learning Rate: 2.2517998136852506e-05\n",
      "Epoch: 882, Training Loss: 0.0005000592209398746, Validation Loss: 0.0006409630295820534, Learning Rate: 2.2517998136852506e-05\n",
      "Epoch: 883, Training Loss: 0.0004739096330013126, Validation Loss: 0.0005579239805229008, Learning Rate: 2.2517998136852506e-05\n",
      "Epoch: 884, Training Loss: 0.0004849709221161902, Validation Loss: 0.0005927857710048556, Learning Rate: 2.2517998136852506e-05\n",
      "Epoch: 885, Training Loss: 0.00046209123684093356, Validation Loss: 0.0005678372690454125, Learning Rate: 2.2517998136852506e-05\n",
      "Epoch: 886, Training Loss: 0.0004720482975244522, Validation Loss: 0.0005799482460133731, Learning Rate: 2.2517998136852506e-05\n",
      "Epoch: 887, Training Loss: 0.0004616965597961098, Validation Loss: 0.0005423621623776853, Learning Rate: 2.2517998136852506e-05\n",
      "Epoch: 888, Training Loss: 0.00046095819561742246, Validation Loss: 0.0005294649745337665, Learning Rate: 2.2517998136852506e-05\n",
      "Epoch: 889, Training Loss: 0.0004585220303852111, Validation Loss: 0.0005145778995938599, Learning Rate: 2.2517998136852506e-05\n",
      "Epoch: 890, Training Loss: 0.0004559952358249575, Validation Loss: 0.0005060277180746198, Learning Rate: 2.2517998136852506e-05\n",
      "Epoch: 891, Training Loss: 0.00045494045480154455, Validation Loss: 0.0005259061581455171, Learning Rate: 2.2517998136852506e-05\n",
      "Epoch: 892, Training Loss: 0.0004583275585900992, Validation Loss: 0.0005341568612493575, Learning Rate: 2.2517998136852506e-05\n",
      "Epoch: 893, Training Loss: 0.00045449272147379816, Validation Loss: 0.0005328697734512389, Learning Rate: 2.2517998136852506e-05\n",
      "Epoch: 894, Training Loss: 0.00045351163134910166, Validation Loss: 0.0005341670475900173, Learning Rate: 2.2517998136852506e-05\n",
      "Epoch: 895, Training Loss: 0.00045446850708685815, Validation Loss: 0.0005196333513595164, Learning Rate: 2.2517998136852506e-05\n",
      "Epoch: 896, Training Loss: 0.00045350583968684077, Validation Loss: 0.0005209734663367271, Learning Rate: 2.2517998136852506e-05\n",
      "Epoch: 897, Training Loss: 0.0004505530232563615, Validation Loss: 0.0005340271745808423, Learning Rate: 2.2517998136852506e-05\n",
      "Epoch: 898, Training Loss: 0.0004473030276130885, Validation Loss: 0.0004951621522195637, Learning Rate: 2.2517998136852506e-05\n",
      "Epoch: 899, Training Loss: 0.00045015354407951236, Validation Loss: 0.0005266544176265597, Learning Rate: 2.2517998136852506e-05\n",
      "Epoch: 900, Training Loss: 0.0004472120781429112, Validation Loss: 0.0005437792860902846, Learning Rate: 1.8014398509482006e-05\n",
      "Epoch: 901, Training Loss: 0.0004476747417356819, Validation Loss: 0.00052725023124367, Learning Rate: 1.8014398509482006e-05\n",
      "Epoch: 902, Training Loss: 0.0004457310715224594, Validation Loss: 0.0005320858326740563, Learning Rate: 1.8014398509482006e-05\n",
      "Epoch: 903, Training Loss: 0.0004458922194316983, Validation Loss: 0.0005159113788977265, Learning Rate: 1.8014398509482006e-05\n",
      "Epoch: 904, Training Loss: 0.00044442035141400993, Validation Loss: 0.000523150956723839, Learning Rate: 1.8014398509482006e-05\n",
      "Epoch: 905, Training Loss: 0.0004515758191701025, Validation Loss: 0.0005697622545994818, Learning Rate: 1.8014398509482006e-05\n",
      "Epoch: 906, Training Loss: 0.00048099798732437193, Validation Loss: 0.000551309552974999, Learning Rate: 1.8014398509482006e-05\n",
      "Epoch: 907, Training Loss: 0.00045039827818982303, Validation Loss: 0.0005439562955871224, Learning Rate: 1.8014398509482006e-05\n",
      "Epoch: 908, Training Loss: 0.0004539212095551193, Validation Loss: 0.0005267598899081349, Learning Rate: 1.8014398509482006e-05\n",
      "Epoch: 909, Training Loss: 0.00044089026050642133, Validation Loss: 0.0005323105142451823, Learning Rate: 1.8014398509482006e-05\n",
      "Epoch: 910, Training Loss: 0.00043837481644004583, Validation Loss: 0.0004997156793251634, Learning Rate: 1.8014398509482006e-05\n",
      "Epoch: 911, Training Loss: 0.00043827545596286654, Validation Loss: 0.000510154408402741, Learning Rate: 1.8014398509482006e-05\n",
      "Epoch: 912, Training Loss: 0.00043673012987710536, Validation Loss: 0.0005192770040594041, Learning Rate: 1.8014398509482006e-05\n",
      "Epoch: 913, Training Loss: 0.0004349725495558232, Validation Loss: 0.0005188893410377204, Learning Rate: 1.8014398509482006e-05\n",
      "Epoch: 914, Training Loss: 0.0004478121118154377, Validation Loss: 0.0005309942062012851, Learning Rate: 1.8014398509482006e-05\n",
      "Epoch: 915, Training Loss: 0.0004712868540082127, Validation Loss: 0.0005427065188996494, Learning Rate: 1.8014398509482006e-05\n",
      "Epoch: 916, Training Loss: 0.0004469353298190981, Validation Loss: 0.0005415418418124318, Learning Rate: 1.8014398509482006e-05\n",
      "Epoch: 917, Training Loss: 0.00044668783084489405, Validation Loss: 0.0005472032353281975, Learning Rate: 1.8014398509482006e-05\n",
      "Epoch: 918, Training Loss: 0.0004789837112184614, Validation Loss: 0.0006145357037894428, Learning Rate: 1.8014398509482006e-05\n",
      "Epoch: 919, Training Loss: 0.00044758274452760816, Validation Loss: 0.0005483666900545359, Learning Rate: 1.8014398509482006e-05\n",
      "Epoch: 920, Training Loss: 0.0004414683789946139, Validation Loss: 0.0005351927829906344, Learning Rate: 1.8014398509482006e-05\n",
      "Epoch: 921, Training Loss: 0.00044007471296936274, Validation Loss: 0.0005317742470651865, Learning Rate: 1.8014398509482006e-05\n",
      "Epoch: 922, Training Loss: 0.00044250168139114976, Validation Loss: 0.0005445548449642956, Learning Rate: 1.8014398509482006e-05\n",
      "Epoch: 923, Training Loss: 0.0004438193573150784, Validation Loss: 0.0005551878130063415, Learning Rate: 1.8014398509482006e-05\n",
      "Epoch: 924, Training Loss: 0.00043789157643914223, Validation Loss: 0.0005279053002595901, Learning Rate: 1.8014398509482006e-05\n",
      "Epoch: 925, Training Loss: 0.0004434024740476161, Validation Loss: 0.0005342133226804435, Learning Rate: 1.8014398509482006e-05\n",
      "Epoch: 926, Training Loss: 0.00044035614700987935, Validation Loss: 0.0005498526734299958, Learning Rate: 1.8014398509482006e-05\n",
      "Epoch: 927, Training Loss: 0.00044156238436698914, Validation Loss: 0.0005352969747036695, Learning Rate: 1.8014398509482006e-05\n",
      "Epoch: 928, Training Loss: 0.00044892277219332755, Validation Loss: 0.0005385724944062531, Learning Rate: 1.8014398509482006e-05\n",
      "Epoch: 929, Training Loss: 0.00044370503746904433, Validation Loss: 0.0005403751274570823, Learning Rate: 1.8014398509482006e-05\n",
      "Epoch: 930, Training Loss: 0.00044482352677732706, Validation Loss: 0.0005353571032173932, Learning Rate: 1.8014398509482006e-05\n",
      "Epoch: 931, Training Loss: 0.0004423954524099827, Validation Loss: 0.0005567488260567188, Learning Rate: 1.8014398509482006e-05\n",
      "Epoch: 932, Training Loss: 0.0004361204046290368, Validation Loss: 0.0005461229593493044, Learning Rate: 1.8014398509482006e-05\n",
      "Epoch: 933, Training Loss: 0.0004336023994255811, Validation Loss: 0.0005414637853391469, Learning Rate: 1.8014398509482006e-05\n",
      "Epoch: 934, Training Loss: 0.0004367507353890687, Validation Loss: 0.0005571866640821099, Learning Rate: 1.8014398509482006e-05\n",
      "Epoch: 935, Training Loss: 0.00044598712702281773, Validation Loss: 0.0005639513838104904, Learning Rate: 1.8014398509482006e-05\n",
      "Epoch: 936, Training Loss: 0.00044183962745592, Validation Loss: 0.0005292285932227969, Learning Rate: 1.8014398509482006e-05\n",
      "Epoch: 937, Training Loss: 0.00045085736201144755, Validation Loss: 0.0005659117014147341, Learning Rate: 1.8014398509482006e-05\n",
      "Epoch: 938, Training Loss: 0.0004576823557727039, Validation Loss: 0.0005377691122703254, Learning Rate: 1.8014398509482006e-05\n",
      "Epoch: 939, Training Loss: 0.0004523718962445855, Validation Loss: 0.0005399329820647836, Learning Rate: 1.8014398509482006e-05\n",
      "Epoch: 940, Training Loss: 0.00044030288700014353, Validation Loss: 0.0005552456714212894, Learning Rate: 1.8014398509482006e-05\n",
      "Epoch: 941, Training Loss: 0.00043754963553510606, Validation Loss: 0.0005462979315780103, Learning Rate: 1.8014398509482006e-05\n",
      "Epoch: 942, Training Loss: 0.0004372272524051368, Validation Loss: 0.0005321460776031017, Learning Rate: 1.8014398509482006e-05\n",
      "Epoch: 943, Training Loss: 0.00044423856888897717, Validation Loss: 0.0005373683525249362, Learning Rate: 1.8014398509482006e-05\n",
      "Epoch: 944, Training Loss: 0.0004403102211654186, Validation Loss: 0.0005438547232188284, Learning Rate: 1.8014398509482006e-05\n",
      "Epoch: 945, Training Loss: 0.00044359819730743766, Validation Loss: 0.0005404196563176811, Learning Rate: 1.8014398509482006e-05\n",
      "Epoch: 946, Training Loss: 0.00045160416630096734, Validation Loss: 0.0005503293359652162, Learning Rate: 1.8014398509482006e-05\n",
      "Epoch: 947, Training Loss: 0.00044337689178064466, Validation Loss: 0.0005438979715108871, Learning Rate: 1.8014398509482006e-05\n",
      "Epoch: 948, Training Loss: 0.00044143409468233585, Validation Loss: 0.0005425375420600176, Learning Rate: 1.8014398509482006e-05\n",
      "Epoch: 949, Training Loss: 0.0004363131301943213, Validation Loss: 0.000522137968800962, Learning Rate: 1.8014398509482006e-05\n",
      "Epoch: 950, Training Loss: 0.0004240856214892119, Validation Loss: 0.0005306733655743301, Learning Rate: 1.4411518807585605e-05\n",
      "Epoch: 951, Training Loss: 0.0004316444683354348, Validation Loss: 0.0005426349816843867, Learning Rate: 1.4411518807585605e-05\n",
      "Epoch: 952, Training Loss: 0.0004360803577583283, Validation Loss: 0.0005247798399068415, Learning Rate: 1.4411518807585605e-05\n",
      "Epoch: 953, Training Loss: 0.0004349985683802515, Validation Loss: 0.000521642214152962, Learning Rate: 1.4411518807585605e-05\n",
      "Epoch: 954, Training Loss: 0.0004355419659987092, Validation Loss: 0.000537747866474092, Learning Rate: 1.4411518807585605e-05\n",
      "Epoch: 955, Training Loss: 0.00043346319580450654, Validation Loss: 0.0005358245689421892, Learning Rate: 1.4411518807585605e-05\n",
      "Epoch: 956, Training Loss: 0.0004295010876376182, Validation Loss: 0.0005258827586658299, Learning Rate: 1.4411518807585605e-05\n",
      "Epoch: 957, Training Loss: 0.0004296307743061334, Validation Loss: 0.0005220213788561523, Learning Rate: 1.4411518807585605e-05\n",
      "Epoch: 958, Training Loss: 0.0004319252329878509, Validation Loss: 0.000528277363628149, Learning Rate: 1.4411518807585605e-05\n",
      "Epoch: 959, Training Loss: 0.0004318490973673761, Validation Loss: 0.0005288495449349284, Learning Rate: 1.4411518807585605e-05\n",
      "Epoch: 960, Training Loss: 0.0004302490269765258, Validation Loss: 0.0005321747157722712, Learning Rate: 1.4411518807585605e-05\n",
      "Epoch: 961, Training Loss: 0.00042793102329596877, Validation Loss: 0.0005319137126207352, Learning Rate: 1.4411518807585605e-05\n",
      "Epoch: 962, Training Loss: 0.00042659646715037525, Validation Loss: 0.0005433352780528367, Learning Rate: 1.4411518807585605e-05\n",
      "Epoch: 963, Training Loss: 0.00042554305400699377, Validation Loss: 0.0005590971559286118, Learning Rate: 1.4411518807585605e-05\n",
      "Epoch: 964, Training Loss: 0.0004227511817589402, Validation Loss: 0.0005574429524131119, Learning Rate: 1.4411518807585605e-05\n",
      "Epoch: 965, Training Loss: 0.000419869291363284, Validation Loss: 0.0005550329806283116, Learning Rate: 1.4411518807585605e-05\n",
      "Epoch: 966, Training Loss: 0.00043065351201221347, Validation Loss: 0.0005244799540378153, Learning Rate: 1.4411518807585605e-05\n",
      "Epoch: 967, Training Loss: 0.00043582814396359026, Validation Loss: 0.0005307552055455744, Learning Rate: 1.4411518807585605e-05\n",
      "Epoch: 968, Training Loss: 0.00042913679499179125, Validation Loss: 0.0005292467540130019, Learning Rate: 1.4411518807585605e-05\n",
      "Epoch: 969, Training Loss: 0.0004266486212145537, Validation Loss: 0.0005307376850396395, Learning Rate: 1.4411518807585605e-05\n",
      "Epoch: 970, Training Loss: 0.0004248524201102555, Validation Loss: 0.0005422834074124694, Learning Rate: 1.4411518807585605e-05\n",
      "Epoch: 971, Training Loss: 0.0004308690840844065, Validation Loss: 0.000515729479957372, Learning Rate: 1.4411518807585605e-05\n",
      "Epoch: 972, Training Loss: 0.0004195695510134101, Validation Loss: 0.0005450514727272093, Learning Rate: 1.4411518807585605e-05\n",
      "Epoch: 973, Training Loss: 0.000427847116952762, Validation Loss: 0.000544638663996011, Learning Rate: 1.4411518807585605e-05\n",
      "Epoch: 974, Training Loss: 0.0004263320006430149, Validation Loss: 0.0005579412099905312, Learning Rate: 1.4411518807585605e-05\n",
      "Epoch: 975, Training Loss: 0.0004150636086706072, Validation Loss: 0.0005497431266121566, Learning Rate: 1.4411518807585605e-05\n",
      "Epoch: 976, Training Loss: 0.00041887929546646774, Validation Loss: 0.0005487418966367841, Learning Rate: 1.4411518807585605e-05\n",
      "Epoch: 977, Training Loss: 0.0004216118832118809, Validation Loss: 0.0005374359898269176, Learning Rate: 1.4411518807585605e-05\n",
      "Epoch: 978, Training Loss: 0.00043158410699106753, Validation Loss: 0.0005618080031126738, Learning Rate: 1.4411518807585605e-05\n",
      "Epoch: 979, Training Loss: 0.0004489464045036584, Validation Loss: 0.000566712231375277, Learning Rate: 1.4411518807585605e-05\n",
      "Epoch: 980, Training Loss: 0.0004539669898804277, Validation Loss: 0.000589742383453995, Learning Rate: 1.4411518807585605e-05\n",
      "Epoch: 981, Training Loss: 0.00044768810039386153, Validation Loss: 0.0005629576044157147, Learning Rate: 1.4411518807585605e-05\n",
      "Epoch: 982, Training Loss: 0.0004523849638644606, Validation Loss: 0.0005711149424314499, Learning Rate: 1.4411518807585605e-05\n",
      "Epoch: 983, Training Loss: 0.0004382744082249701, Validation Loss: 0.0005646936479024589, Learning Rate: 1.4411518807585605e-05\n",
      "Epoch: 984, Training Loss: 0.0004254857776686549, Validation Loss: 0.0005814558244310319, Learning Rate: 1.4411518807585605e-05\n",
      "Epoch: 985, Training Loss: 0.00042827764991670847, Validation Loss: 0.0005712072015739977, Learning Rate: 1.4411518807585605e-05\n",
      "Epoch: 986, Training Loss: 0.00041803449857980013, Validation Loss: 0.0005684433854185045, Learning Rate: 1.4411518807585605e-05\n",
      "Epoch: 987, Training Loss: 0.00041336531285196543, Validation Loss: 0.0005534017691388726, Learning Rate: 1.4411518807585605e-05\n",
      "Epoch: 988, Training Loss: 0.00041517463978379965, Validation Loss: 0.0005535235977731645, Learning Rate: 1.4411518807585605e-05\n",
      "Epoch: 989, Training Loss: 0.0004163519770372659, Validation Loss: 0.0005599583382718265, Learning Rate: 1.4411518807585605e-05\n",
      "Epoch: 990, Training Loss: 0.00042374234180897474, Validation Loss: 0.0005671764374710619, Learning Rate: 1.4411518807585605e-05\n",
      "Epoch: 991, Training Loss: 0.0004169256135355681, Validation Loss: 0.0005545596941374242, Learning Rate: 1.4411518807585605e-05\n",
      "Epoch: 992, Training Loss: 0.00041523276013322175, Validation Loss: 0.0005616809939965606, Learning Rate: 1.4411518807585605e-05\n",
      "Epoch: 993, Training Loss: 0.0004173341440036893, Validation Loss: 0.0005622861790470779, Learning Rate: 1.4411518807585605e-05\n",
      "Epoch: 994, Training Loss: 0.0004120060766581446, Validation Loss: 0.0005752520519308746, Learning Rate: 1.4411518807585605e-05\n",
      "Epoch: 995, Training Loss: 0.00042399633093737066, Validation Loss: 0.0005646342760883272, Learning Rate: 1.4411518807585605e-05\n",
      "Epoch: 996, Training Loss: 0.00041131695616059005, Validation Loss: 0.0005628032376989722, Learning Rate: 1.4411518807585605e-05\n",
      "Epoch: 997, Training Loss: 0.0004280869325157255, Validation Loss: 0.0005697443848475814, Learning Rate: 1.4411518807585605e-05\n",
      "Epoch: 998, Training Loss: 0.00042244262294843793, Validation Loss: 0.0005765777314081788, Learning Rate: 1.4411518807585605e-05\n",
      "Epoch: 999, Training Loss: 0.0004314602701924741, Validation Loss: 0.0005586935440078378, Learning Rate: 1.4411518807585605e-05\n",
      "Epoch: 1000, Training Loss: 0.0004242063732817769, Validation Loss: 0.0005697865854017437, Learning Rate: 1.1529215046068485e-05\n",
      "Epoch: 1001, Training Loss: 0.0004209567268844694, Validation Loss: 0.0005733281141147017, Learning Rate: 1.1529215046068485e-05\n",
      "Epoch: 1002, Training Loss: 0.0004190548788756132, Validation Loss: 0.0005707237287424505, Learning Rate: 1.1529215046068485e-05\n",
      "Epoch: 1003, Training Loss: 0.0004187318845652044, Validation Loss: 0.0005674578715115786, Learning Rate: 1.1529215046068485e-05\n",
      "Epoch: 1004, Training Loss: 0.00041862408397719264, Validation Loss: 0.0005641751340590417, Learning Rate: 1.1529215046068485e-05\n",
      "Epoch: 1005, Training Loss: 0.00042126994230784476, Validation Loss: 0.0005548913031816483, Learning Rate: 1.1529215046068485e-05\n",
      "Epoch: 1006, Training Loss: 0.0004169531457591802, Validation Loss: 0.0005776515463367105, Learning Rate: 1.1529215046068485e-05\n",
      "Epoch: 1007, Training Loss: 0.00042105268221348524, Validation Loss: 0.0005669783568009734, Learning Rate: 1.1529215046068485e-05\n",
      "Epoch: 1008, Training Loss: 0.0004188145976513624, Validation Loss: 0.0005584430764429271, Learning Rate: 1.1529215046068485e-05\n",
      "Epoch: 1009, Training Loss: 0.0004143033002037555, Validation Loss: 0.0005732289282605052, Learning Rate: 1.1529215046068485e-05\n",
      "Epoch: 1010, Training Loss: 0.00042465547448955476, Validation Loss: 0.0005585781182162464, Learning Rate: 1.1529215046068485e-05\n",
      "Epoch: 1011, Training Loss: 0.000430596643127501, Validation Loss: 0.0005706022493541241, Learning Rate: 1.1529215046068485e-05\n",
      "Epoch: 1012, Training Loss: 0.0004222958814352751, Validation Loss: 0.000561475520953536, Learning Rate: 1.1529215046068485e-05\n",
      "Epoch: 1013, Training Loss: 0.00042183921323157847, Validation Loss: 0.0005739852786064148, Learning Rate: 1.1529215046068485e-05\n",
      "Epoch: 1014, Training Loss: 0.0004229066544212401, Validation Loss: 0.0005705242510885, Learning Rate: 1.1529215046068485e-05\n",
      "Epoch: 1015, Training Loss: 0.0004227288591209799, Validation Loss: 0.0005561994621530175, Learning Rate: 1.1529215046068485e-05\n",
      "Epoch: 1016, Training Loss: 0.0004209077451378107, Validation Loss: 0.000552609097212553, Learning Rate: 1.1529215046068485e-05\n",
      "Epoch: 1017, Training Loss: 0.0004202627460472286, Validation Loss: 0.0005609214422293007, Learning Rate: 1.1529215046068485e-05\n",
      "Epoch: 1018, Training Loss: 0.00041403615614399314, Validation Loss: 0.0005563253071159124, Learning Rate: 1.1529215046068485e-05\n",
      "Epoch: 1019, Training Loss: 0.0004202231939416379, Validation Loss: 0.0005438412190414965, Learning Rate: 1.1529215046068485e-05\n",
      "Epoch: 1020, Training Loss: 0.00041359831811860204, Validation Loss: 0.0005458811647258699, Learning Rate: 1.1529215046068485e-05\n",
      "Epoch: 1021, Training Loss: 0.00041383871575817466, Validation Loss: 0.0005463010165840387, Learning Rate: 1.1529215046068485e-05\n",
      "Epoch: 1022, Training Loss: 0.00041622764547355473, Validation Loss: 0.0005575863178819418, Learning Rate: 1.1529215046068485e-05\n",
      "Epoch: 1023, Training Loss: 0.0004121146921534091, Validation Loss: 0.0005334200104698539, Learning Rate: 1.1529215046068485e-05\n",
      "Epoch: 1024, Training Loss: 0.00041344683268107474, Validation Loss: 0.000534655584488064, Learning Rate: 1.1529215046068485e-05\n",
      "Epoch: 1025, Training Loss: 0.00041266068001277745, Validation Loss: 0.0005499200196936727, Learning Rate: 1.1529215046068485e-05\n",
      "Epoch: 1026, Training Loss: 0.0004253684019204229, Validation Loss: 0.0005501531413756311, Learning Rate: 1.1529215046068485e-05\n",
      "Epoch: 1027, Training Loss: 0.00041219822014681995, Validation Loss: 0.0005369898863136768, Learning Rate: 1.1529215046068485e-05\n",
      "Epoch: 1028, Training Loss: 0.000415445159887895, Validation Loss: 0.0005477218073792756, Learning Rate: 1.1529215046068485e-05\n",
      "Epoch: 1029, Training Loss: 0.00040592244477011263, Validation Loss: 0.0005311958957463503, Learning Rate: 1.1529215046068485e-05\n",
      "Epoch: 1030, Training Loss: 0.000410147215006873, Validation Loss: 0.0005433419719338417, Learning Rate: 1.1529215046068485e-05\n",
      "Epoch: 1031, Training Loss: 0.000429671403253451, Validation Loss: 0.0005876045906916261, Learning Rate: 1.1529215046068485e-05\n",
      "Epoch: 1032, Training Loss: 0.0004148186126258224, Validation Loss: 0.0005582856829278171, Learning Rate: 1.1529215046068485e-05\n",
      "Epoch: 1033, Training Loss: 0.00041826386586762965, Validation Loss: 0.0005768530536442995, Learning Rate: 1.1529215046068485e-05\n",
      "Epoch: 1034, Training Loss: 0.0004134355403948575, Validation Loss: 0.0005478269304148853, Learning Rate: 1.1529215046068485e-05\n",
      "Epoch: 1035, Training Loss: 0.00040939031168818474, Validation Loss: 0.0005289443652145565, Learning Rate: 1.1529215046068485e-05\n",
      "Epoch: 1036, Training Loss: 0.0004087749111931771, Validation Loss: 0.0005334828165359795, Learning Rate: 1.1529215046068485e-05\n",
      "Epoch: 1037, Training Loss: 0.0004143167461734265, Validation Loss: 0.0005162758170627058, Learning Rate: 1.1529215046068485e-05\n",
      "Epoch: 1038, Training Loss: 0.0004095816984772682, Validation Loss: 0.0005275532021187246, Learning Rate: 1.1529215046068485e-05\n",
      "Epoch: 1039, Training Loss: 0.0004182847333140671, Validation Loss: 0.0005503750871866941, Learning Rate: 1.1529215046068485e-05\n",
      "Epoch: 1040, Training Loss: 0.0004162751429248601, Validation Loss: 0.0005337070906534791, Learning Rate: 1.1529215046068485e-05\n",
      "Epoch: 1041, Training Loss: 0.0004079278733115643, Validation Loss: 0.0005422409740276635, Learning Rate: 1.1529215046068485e-05\n",
      "Epoch: 1042, Training Loss: 0.00040403325692750514, Validation Loss: 0.0005410819430835545, Learning Rate: 1.1529215046068485e-05\n",
      "Epoch: 1043, Training Loss: 0.0004192009218968451, Validation Loss: 0.000565824331715703, Learning Rate: 1.1529215046068485e-05\n",
      "Epoch: 1044, Training Loss: 0.00040939764585345984, Validation Loss: 0.0005382869276218116, Learning Rate: 1.1529215046068485e-05\n",
      "Epoch: 1045, Training Loss: 0.0004148224543314427, Validation Loss: 0.000564688874874264, Learning Rate: 1.1529215046068485e-05\n",
      "Epoch: 1046, Training Loss: 0.00040940652252174914, Validation Loss: 0.000549397780559957, Learning Rate: 1.1529215046068485e-05\n",
      "Epoch: 1047, Training Loss: 0.0004078523488715291, Validation Loss: 0.0005483472486957908, Learning Rate: 1.1529215046068485e-05\n",
      "Epoch: 1048, Training Loss: 0.0004079983336851001, Validation Loss: 0.0005402242531999946, Learning Rate: 1.1529215046068485e-05\n",
      "Epoch: 1049, Training Loss: 0.00041436482570134103, Validation Loss: 0.0005401498638093472, Learning Rate: 1.1529215046068485e-05\n",
      "Epoch: 1050, Training Loss: 0.0004084245301783085, Validation Loss: 0.0005193211836740375, Learning Rate: 9.223372036854789e-06\n",
      "Epoch: 1051, Training Loss: 0.0004065227403771132, Validation Loss: 0.0005169080104678869, Learning Rate: 9.223372036854789e-06\n",
      "Epoch: 1052, Training Loss: 0.0004050688003189862, Validation Loss: 0.0005226916400715709, Learning Rate: 9.223372036854789e-06\n",
      "Epoch: 1053, Training Loss: 0.00040532666025683284, Validation Loss: 0.0005414033657871187, Learning Rate: 9.223372036854789e-06\n",
      "Epoch: 1054, Training Loss: 0.0004067010013386607, Validation Loss: 0.0005407875869423151, Learning Rate: 9.223372036854789e-06\n",
      "Epoch: 1055, Training Loss: 0.00040075115975923836, Validation Loss: 0.0005385889089666307, Learning Rate: 9.223372036854789e-06\n",
      "Epoch: 1056, Training Loss: 0.000394387636333704, Validation Loss: 0.0005337514448910952, Learning Rate: 9.223372036854789e-06\n",
      "Epoch: 1057, Training Loss: 0.00041141806286759675, Validation Loss: 0.0005329190753400326, Learning Rate: 9.223372036854789e-06\n",
      "Epoch: 1058, Training Loss: 0.0004068702401127666, Validation Loss: 0.0005282429046928883, Learning Rate: 9.223372036854789e-06\n",
      "Epoch: 1059, Training Loss: 0.00041907172999344766, Validation Loss: 0.0005336112808436155, Learning Rate: 9.223372036854789e-06\n",
      "Epoch: 1060, Training Loss: 0.00040551411802880466, Validation Loss: 0.000524805742315948, Learning Rate: 9.223372036854789e-06\n",
      "Epoch: 1061, Training Loss: 0.0004116201016586274, Validation Loss: 0.0005463467678055167, Learning Rate: 9.223372036854789e-06\n",
      "Epoch: 1062, Training Loss: 0.000409524654969573, Validation Loss: 0.0005356169422157109, Learning Rate: 9.223372036854789e-06\n",
      "Epoch: 1063, Training Loss: 0.0003909732913598418, Validation Loss: 0.0005338093615137041, Learning Rate: 9.223372036854789e-06\n",
      "Epoch: 1064, Training Loss: 0.0003871787048410624, Validation Loss: 0.0005256467266008258, Learning Rate: 9.223372036854789e-06\n",
      "Epoch: 1065, Training Loss: 0.0004009173426311463, Validation Loss: 0.0005303635261952877, Learning Rate: 9.223372036854789e-06\n",
      "Epoch: 1066, Training Loss: 0.00038643094012513757, Validation Loss: 0.0005340721108950675, Learning Rate: 9.223372036854789e-06\n",
      "Epoch: 1067, Training Loss: 0.0003969191457144916, Validation Loss: 0.0005357239278964698, Learning Rate: 9.223372036854789e-06\n",
      "Epoch: 1068, Training Loss: 0.00040123172220773995, Validation Loss: 0.0005344337550923228, Learning Rate: 9.223372036854789e-06\n",
      "Epoch: 1069, Training Loss: 0.0004025791131425649, Validation Loss: 0.000535897328518331, Learning Rate: 9.223372036854789e-06\n",
      "Epoch: 1070, Training Loss: 0.00040441734017804265, Validation Loss: 0.0005413845647126436, Learning Rate: 9.223372036854789e-06\n",
      "Epoch: 1071, Training Loss: 0.00040908492519520223, Validation Loss: 0.0005249925889074802, Learning Rate: 9.223372036854789e-06\n",
      "Epoch: 1072, Training Loss: 0.0003928921651095152, Validation Loss: 0.0005279735196381807, Learning Rate: 9.223372036854789e-06\n",
      "Epoch: 1073, Training Loss: 0.00039709251723252237, Validation Loss: 0.0005329946288838983, Learning Rate: 9.223372036854789e-06\n",
      "Epoch: 1074, Training Loss: 0.0003874225076287985, Validation Loss: 0.0005419820081442595, Learning Rate: 9.223372036854789e-06\n",
      "Epoch: 1075, Training Loss: 0.0004014324222225696, Validation Loss: 0.0005510899936780334, Learning Rate: 9.223372036854789e-06\n",
      "Epoch: 1076, Training Loss: 0.0004041356442030519, Validation Loss: 0.0005328109255060554, Learning Rate: 9.223372036854789e-06\n",
      "Epoch: 1077, Training Loss: 0.00038826759555377066, Validation Loss: 0.0005308836116455495, Learning Rate: 9.223372036854789e-06\n",
      "Epoch: 1078, Training Loss: 0.0003899267758242786, Validation Loss: 0.0005405145930126309, Learning Rate: 9.223372036854789e-06\n",
      "Epoch: 1079, Training Loss: 0.00038928218418732285, Validation Loss: 0.0005354791646823287, Learning Rate: 9.223372036854789e-06\n",
      "Epoch: 1080, Training Loss: 0.00039626649231649935, Validation Loss: 0.0005361443618312478, Learning Rate: 9.223372036854789e-06\n",
      "Epoch: 1081, Training Loss: 0.0003876256523653865, Validation Loss: 0.0005375639302656054, Learning Rate: 9.223372036854789e-06\n",
      "Epoch: 1082, Training Loss: 0.0003865747421514243, Validation Loss: 0.0005380138754844666, Learning Rate: 9.223372036854789e-06\n",
      "Epoch: 1083, Training Loss: 0.00038254965329542756, Validation Loss: 0.0005373287131078541, Learning Rate: 9.223372036854789e-06\n",
      "Epoch: 1084, Training Loss: 0.0003818482218775898, Validation Loss: 0.0005278445314615965, Learning Rate: 9.223372036854789e-06\n",
      "Epoch: 1085, Training Loss: 0.00038028351264074445, Validation Loss: 0.000524992763530463, Learning Rate: 9.223372036854789e-06\n",
      "Epoch: 1086, Training Loss: 0.0003977506421506405, Validation Loss: 0.0005288423271849751, Learning Rate: 9.223372036854789e-06\n",
      "Epoch: 1087, Training Loss: 0.000405732833314687, Validation Loss: 0.0005423176917247474, Learning Rate: 9.223372036854789e-06\n",
      "Epoch: 1088, Training Loss: 0.00039860894321464, Validation Loss: 0.0005230724927969277, Learning Rate: 9.223372036854789e-06\n",
      "Epoch: 1089, Training Loss: 0.00039667569217272103, Validation Loss: 0.000532327510882169, Learning Rate: 9.223372036854789e-06\n",
      "Epoch: 1090, Training Loss: 0.00039791216840967536, Validation Loss: 0.000535334984306246, Learning Rate: 9.223372036854789e-06\n",
      "Epoch: 1091, Training Loss: 0.00039892501081340015, Validation Loss: 0.0005486169829964638, Learning Rate: 9.223372036854789e-06\n",
      "Epoch: 1092, Training Loss: 0.00039372299215756357, Validation Loss: 0.0005341892829164863, Learning Rate: 9.223372036854789e-06\n",
      "Epoch: 1093, Training Loss: 0.0004011761920992285, Validation Loss: 0.0005409520235843956, Learning Rate: 9.223372036854789e-06\n",
      "Epoch: 1094, Training Loss: 0.00039621294126845896, Validation Loss: 0.0005264313076622784, Learning Rate: 9.223372036854789e-06\n",
      "Epoch: 1095, Training Loss: 0.00039665072108618915, Validation Loss: 0.0005327665712684393, Learning Rate: 9.223372036854789e-06\n",
      "Epoch: 1096, Training Loss: 0.00039580935845151544, Validation Loss: 0.000539810920599848, Learning Rate: 9.223372036854789e-06\n",
      "Epoch: 1097, Training Loss: 0.0003953909908886999, Validation Loss: 0.00054491515038535, Learning Rate: 9.223372036854789e-06\n",
      "Epoch: 1098, Training Loss: 0.00039837247459217906, Validation Loss: 0.0005466507282108068, Learning Rate: 9.223372036854789e-06\n",
      "Epoch: 1099, Training Loss: 0.00039563371683470905, Validation Loss: 0.0005363925592973828, Learning Rate: 9.223372036854789e-06\n",
      "Epoch: 1100, Training Loss: 0.00039216381264850497, Validation Loss: 0.0005297131137922406, Learning Rate: 7.378697629483831e-06\n",
      "Epoch: 1101, Training Loss: 0.00039056784589774907, Validation Loss: 0.0005253673298284411, Learning Rate: 7.378697629483831e-06\n",
      "Epoch: 1102, Training Loss: 0.0004101511149201542, Validation Loss: 0.0005353778251446784, Learning Rate: 7.378697629483831e-06\n",
      "Epoch: 1103, Training Loss: 0.00040235769120045006, Validation Loss: 0.0005245847278274596, Learning Rate: 7.378697629483831e-06\n",
      "Epoch: 1104, Training Loss: 0.00040197873022407293, Validation Loss: 0.0005505849258042872, Learning Rate: 7.378697629483831e-06\n",
      "Epoch: 1105, Training Loss: 0.0004022176144644618, Validation Loss: 0.0005296727176755667, Learning Rate: 7.378697629483831e-06\n",
      "Epoch: 1106, Training Loss: 0.00039133577956818044, Validation Loss: 0.0005498785176314414, Learning Rate: 7.378697629483831e-06\n",
      "Epoch: 1107, Training Loss: 0.0003873487003147602, Validation Loss: 0.000543172238394618, Learning Rate: 7.378697629483831e-06\n",
      "Epoch: 1108, Training Loss: 0.00038776177098043263, Validation Loss: 0.0005275607691146433, Learning Rate: 7.378697629483831e-06\n",
      "Epoch: 1109, Training Loss: 0.0003869090578518808, Validation Loss: 0.0005362462252378464, Learning Rate: 7.378697629483831e-06\n",
      "Epoch: 1110, Training Loss: 0.00038723243051208556, Validation Loss: 0.000525296141859144, Learning Rate: 7.378697629483831e-06\n",
      "Epoch: 1111, Training Loss: 0.00038522304384969175, Validation Loss: 0.0005273325368762016, Learning Rate: 7.378697629483831e-06\n",
      "Epoch: 1112, Training Loss: 0.00038843625225126743, Validation Loss: 0.0005287176463752985, Learning Rate: 7.378697629483831e-06\n",
      "Epoch: 1113, Training Loss: 0.00038716528797522187, Validation Loss: 0.0005197750870138407, Learning Rate: 7.378697629483831e-06\n",
      "Epoch: 1114, Training Loss: 0.0003944779746234417, Validation Loss: 0.0005208459915593266, Learning Rate: 7.378697629483831e-06\n",
      "Epoch: 1115, Training Loss: 0.0003854787501040846, Validation Loss: 0.0005267073283903301, Learning Rate: 7.378697629483831e-06\n",
      "Epoch: 1116, Training Loss: 0.00038675597170367837, Validation Loss: 0.000529735058080405, Learning Rate: 7.378697629483831e-06\n",
      "Epoch: 1117, Training Loss: 0.00038576999213546515, Validation Loss: 0.0005187264760024846, Learning Rate: 7.378697629483831e-06\n",
      "Epoch: 1118, Training Loss: 0.00038570925244130194, Validation Loss: 0.0005221824394538999, Learning Rate: 7.378697629483831e-06\n",
      "Epoch: 1119, Training Loss: 0.00038515840424224734, Validation Loss: 0.0005229013040661812, Learning Rate: 7.378697629483831e-06\n",
      "Epoch: 1120, Training Loss: 0.0003878325514961034, Validation Loss: 0.0005156229599379003, Learning Rate: 7.378697629483831e-06\n",
      "Epoch: 1121, Training Loss: 0.00038828785181976855, Validation Loss: 0.0005266057560220361, Learning Rate: 7.378697629483831e-06\n",
      "Epoch: 1122, Training Loss: 0.0003861252625938505, Validation Loss: 0.0005281212506815791, Learning Rate: 7.378697629483831e-06\n",
      "Epoch: 1123, Training Loss: 0.0003851910005323589, Validation Loss: 0.0005258099990896881, Learning Rate: 7.378697629483831e-06\n",
      "Epoch: 1124, Training Loss: 0.00038629164919257164, Validation Loss: 0.0005273015704005957, Learning Rate: 7.378697629483831e-06\n",
      "Epoch: 1125, Training Loss: 0.000389466411434114, Validation Loss: 0.0005315060843713582, Learning Rate: 7.378697629483831e-06\n",
      "Epoch: 1126, Training Loss: 0.0003861338773276657, Validation Loss: 0.0005228960071690381, Learning Rate: 7.378697629483831e-06\n",
      "Epoch: 1127, Training Loss: 0.0003855136164929718, Validation Loss: 0.0005224598571658134, Learning Rate: 7.378697629483831e-06\n",
      "Epoch: 1128, Training Loss: 0.0003863524761982262, Validation Loss: 0.0005202615866437554, Learning Rate: 7.378697629483831e-06\n",
      "Epoch: 1129, Training Loss: 0.0003860174911096692, Validation Loss: 0.0005138135165907443, Learning Rate: 7.378697629483831e-06\n",
      "Epoch: 1130, Training Loss: 0.00038482158561237156, Validation Loss: 0.0005173716926947236, Learning Rate: 7.378697629483831e-06\n",
      "Epoch: 1131, Training Loss: 0.00038323839544318616, Validation Loss: 0.0005174276302568614, Learning Rate: 7.378697629483831e-06\n",
      "Epoch: 1132, Training Loss: 0.00038526285788975656, Validation Loss: 0.0005157787236385047, Learning Rate: 7.378697629483831e-06\n",
      "Epoch: 1133, Training Loss: 0.0003856593102682382, Validation Loss: 0.0005116125103086233, Learning Rate: 7.378697629483831e-06\n",
      "Epoch: 1134, Training Loss: 0.00038587188464589417, Validation Loss: 0.0005106588359922171, Learning Rate: 7.378697629483831e-06\n",
      "Epoch: 1135, Training Loss: 0.00038486867561005056, Validation Loss: 0.0005149220232851803, Learning Rate: 7.378697629483831e-06\n",
      "Epoch: 1136, Training Loss: 0.000385487568564713, Validation Loss: 0.0005218926235102117, Learning Rate: 7.378697629483831e-06\n",
      "Epoch: 1137, Training Loss: 0.00038387186941690743, Validation Loss: 0.0005265039508230984, Learning Rate: 7.378697629483831e-06\n",
      "Epoch: 1138, Training Loss: 0.0003861574223265052, Validation Loss: 0.000514877203386277, Learning Rate: 7.378697629483831e-06\n",
      "Epoch: 1139, Training Loss: 0.0004317891725804657, Validation Loss: 0.0005453677149489522, Learning Rate: 7.378697629483831e-06\n",
      "Epoch: 1140, Training Loss: 0.00042155315168201923, Validation Loss: 0.000532136473339051, Learning Rate: 7.378697629483831e-06\n",
      "Epoch: 1141, Training Loss: 0.000414794689277187, Validation Loss: 0.0005235812859609723, Learning Rate: 7.378697629483831e-06\n",
      "Epoch: 1142, Training Loss: 0.00041243573650717735, Validation Loss: 0.0005168401403352618, Learning Rate: 7.378697629483831e-06\n",
      "Epoch: 1143, Training Loss: 0.0004105605185031891, Validation Loss: 0.0005180552252568305, Learning Rate: 7.378697629483831e-06\n",
      "Epoch: 1144, Training Loss: 0.00041015009628608823, Validation Loss: 0.0005159764550626278, Learning Rate: 7.378697629483831e-06\n",
      "Epoch: 1145, Training Loss: 0.0004097365599591285, Validation Loss: 0.0005179428262636065, Learning Rate: 7.378697629483831e-06\n",
      "Epoch: 1146, Training Loss: 0.00040881888708099723, Validation Loss: 0.0005230513052083552, Learning Rate: 7.378697629483831e-06\n",
      "Epoch: 1147, Training Loss: 0.0004095237236469984, Validation Loss: 0.0005194669356569648, Learning Rate: 7.378697629483831e-06\n",
      "Epoch: 1148, Training Loss: 0.0004096431366633624, Validation Loss: 0.0005233606207184494, Learning Rate: 7.378697629483831e-06\n",
      "Epoch: 1149, Training Loss: 0.00040521397022530437, Validation Loss: 0.0005289151449687779, Learning Rate: 7.378697629483831e-06\n",
      "Epoch: 1150, Training Loss: 0.00039498857222497463, Validation Loss: 0.0005406761192716658, Learning Rate: 5.902958103587065e-06\n",
      "Epoch: 1151, Training Loss: 0.00040785555029287934, Validation Loss: 0.0005249358946457505, Learning Rate: 5.902958103587065e-06\n",
      "Epoch: 1152, Training Loss: 0.0004070760332979262, Validation Loss: 0.000517556385602802, Learning Rate: 5.902958103587065e-06\n",
      "Epoch: 1153, Training Loss: 0.0004060654900968075, Validation Loss: 0.0005133564700372517, Learning Rate: 5.902958103587065e-06\n",
      "Epoch: 1154, Training Loss: 0.00040676884236745536, Validation Loss: 0.0005210964009165764, Learning Rate: 5.902958103587065e-06\n",
      "Epoch: 1155, Training Loss: 0.0003981475019827485, Validation Loss: 0.0005221317405812442, Learning Rate: 5.902958103587065e-06\n",
      "Epoch: 1156, Training Loss: 0.00039528904017060995, Validation Loss: 0.0005343376542441547, Learning Rate: 5.902958103587065e-06\n",
      "Epoch: 1157, Training Loss: 0.00038402434438467026, Validation Loss: 0.0005337740294635296, Learning Rate: 5.902958103587065e-06\n",
      "Epoch: 1158, Training Loss: 0.0003820166166406125, Validation Loss: 0.000528826261870563, Learning Rate: 5.902958103587065e-06\n",
      "Epoch: 1159, Training Loss: 0.0003825514577329159, Validation Loss: 0.0005189160583540797, Learning Rate: 5.902958103587065e-06\n",
      "Epoch: 1160, Training Loss: 0.0003821342543233186, Validation Loss: 0.0005214004777371883, Learning Rate: 5.902958103587065e-06\n",
      "Epoch: 1161, Training Loss: 0.0003824926388915628, Validation Loss: 0.0005238742451183498, Learning Rate: 5.902958103587065e-06\n",
      "Epoch: 1162, Training Loss: 0.00038099236553534865, Validation Loss: 0.0005237416480667889, Learning Rate: 5.902958103587065e-06\n",
      "Epoch: 1163, Training Loss: 0.00038166914600878954, Validation Loss: 0.000519238761626184, Learning Rate: 5.902958103587065e-06\n",
      "Epoch: 1164, Training Loss: 0.00038228638004511595, Validation Loss: 0.0005263617495074868, Learning Rate: 5.902958103587065e-06\n",
      "Epoch: 1165, Training Loss: 0.0003822774160653353, Validation Loss: 0.0005289815599098802, Learning Rate: 5.902958103587065e-06\n",
      "Epoch: 1166, Training Loss: 0.00038220779970288277, Validation Loss: 0.0005250718677416444, Learning Rate: 5.902958103587065e-06\n",
      "Epoch: 1167, Training Loss: 0.0003812159411609173, Validation Loss: 0.0005148174241185188, Learning Rate: 5.902958103587065e-06\n",
      "Epoch: 1168, Training Loss: 0.00038307797512970865, Validation Loss: 0.0005214245175011456, Learning Rate: 5.902958103587065e-06\n",
      "Epoch: 1169, Training Loss: 0.0003807636094279587, Validation Loss: 0.0005241867038421333, Learning Rate: 5.902958103587065e-06\n",
      "Epoch: 1170, Training Loss: 0.0003806386375799775, Validation Loss: 0.0005174170364625752, Learning Rate: 5.902958103587065e-06\n",
      "Epoch: 1171, Training Loss: 0.0003807975444942713, Validation Loss: 0.0005164013709872961, Learning Rate: 5.902958103587065e-06\n",
      "Epoch: 1172, Training Loss: 0.0003808621841017157, Validation Loss: 0.0005167916533537209, Learning Rate: 5.902958103587065e-06\n",
      "Epoch: 1173, Training Loss: 0.0003809007175732404, Validation Loss: 0.0005171908414922655, Learning Rate: 5.902958103587065e-06\n",
      "Epoch: 1174, Training Loss: 0.00038036404293961823, Validation Loss: 0.0005176850827410817, Learning Rate: 5.902958103587065e-06\n",
      "Epoch: 1175, Training Loss: 0.00037936997250653803, Validation Loss: 0.0005153545644134283, Learning Rate: 5.902958103587065e-06\n",
      "Epoch: 1176, Training Loss: 0.0003800700942520052, Validation Loss: 0.0005128180491738021, Learning Rate: 5.902958103587065e-06\n",
      "Epoch: 1177, Training Loss: 0.0003795025695580989, Validation Loss: 0.000518403947353363, Learning Rate: 5.902958103587065e-06\n",
      "Epoch: 1178, Training Loss: 0.00037918571615591645, Validation Loss: 0.0005146494368091226, Learning Rate: 5.902958103587065e-06\n",
      "Epoch: 1179, Training Loss: 0.00038099600351415575, Validation Loss: 0.0005116995889693499, Learning Rate: 5.902958103587065e-06\n",
      "Epoch: 1180, Training Loss: 0.0003815356467384845, Validation Loss: 0.0005164930480532348, Learning Rate: 5.902958103587065e-06\n",
      "Epoch: 1181, Training Loss: 0.00038272442179732025, Validation Loss: 0.000515396473929286, Learning Rate: 5.902958103587065e-06\n",
      "Epoch: 1182, Training Loss: 0.00038043869426473975, Validation Loss: 0.0005184509791433811, Learning Rate: 5.902958103587065e-06\n",
      "Epoch: 1183, Training Loss: 0.000381438439944759, Validation Loss: 0.0005223779007792473, Learning Rate: 5.902958103587065e-06\n",
      "Epoch: 1184, Training Loss: 0.00038213972584344447, Validation Loss: 0.0005158757558092475, Learning Rate: 5.902958103587065e-06\n",
      "Epoch: 1185, Training Loss: 0.0003808436158578843, Validation Loss: 0.0005108343902975321, Learning Rate: 5.902958103587065e-06\n",
      "Epoch: 1186, Training Loss: 0.00037982434150762856, Validation Loss: 0.0005179419531486928, Learning Rate: 5.902958103587065e-06\n",
      "Epoch: 1187, Training Loss: 0.0003813922521658242, Validation Loss: 0.0005119642592035234, Learning Rate: 5.902958103587065e-06\n",
      "Epoch: 1188, Training Loss: 0.0003800706472247839, Validation Loss: 0.0005162158631719649, Learning Rate: 5.902958103587065e-06\n",
      "Epoch: 1189, Training Loss: 0.0003789799229707569, Validation Loss: 0.0005227562505751848, Learning Rate: 5.902958103587065e-06\n",
      "Epoch: 1190, Training Loss: 0.00038047233829274774, Validation Loss: 0.0005148203927092254, Learning Rate: 5.902958103587065e-06\n",
      "Epoch: 1191, Training Loss: 0.0003796835953835398, Validation Loss: 0.0005207461072131991, Learning Rate: 5.902958103587065e-06\n",
      "Epoch: 1192, Training Loss: 0.0003796831297222525, Validation Loss: 0.000511678634211421, Learning Rate: 5.902958103587065e-06\n",
      "Epoch: 1193, Training Loss: 0.0003804595326073468, Validation Loss: 0.000510968326125294, Learning Rate: 5.902958103587065e-06\n",
      "Epoch: 1194, Training Loss: 0.00037996156606823206, Validation Loss: 0.0005134381353855133, Learning Rate: 5.902958103587065e-06\n",
      "Epoch: 1195, Training Loss: 0.00037936645094305277, Validation Loss: 0.0005152898957021534, Learning Rate: 5.902958103587065e-06\n",
      "Epoch: 1196, Training Loss: 0.0003781808482017368, Validation Loss: 0.0005169459618628025, Learning Rate: 5.902958103587065e-06\n",
      "Epoch: 1197, Training Loss: 0.00038053173921070993, Validation Loss: 0.0005152557860128582, Learning Rate: 5.902958103587065e-06\n",
      "Epoch: 1198, Training Loss: 0.0004030345007777214, Validation Loss: 0.0005117051186971366, Learning Rate: 5.902958103587065e-06\n",
      "Epoch: 1199, Training Loss: 0.00040506431832909584, Validation Loss: 0.0005094553343951702, Learning Rate: 5.902958103587065e-06\n"
     ]
    }
   ],
   "source": [
    "# Training process\n",
    "\n",
    "epoches = 1200\n",
    "\n",
    "for epoch in range(epoches):\n",
    "    for x_batch, y_batch, u_batch in data_loader:\n",
    "        # Forward propagation\n",
    "        optimizer_autoencoder.zero_grad()\n",
    "        loss = mse_loss(x_batch, model_autoencoder(x_batch))\n",
    "        loss.backward()\n",
    "        optimizer_autoencoder.step()\n",
    "        \n",
    "    \n",
    "    train_loss = mse_loss(x_train, model_autoencoder(x_train))\n",
    "    train_losses.append(train_loss.item())\n",
    "        \n",
    "    # Test the model\n",
    "    with torch.no_grad():\n",
    "        val_loss = mse_loss(x_test, model_autoencoder(x_test))\n",
    "        val_losses.append(val_loss.item())\n",
    "\n",
    "    current_lr = optimizer_autoencoder.param_groups[0]['lr']\n",
    "    scheduler.step()\n",
    "    print(f'Epoch: {epoch}, Training Loss: {train_loss.item()}, Validation Loss: {val_loss.item()}, Learning Rate: {current_lr}')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 1000x500 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Plot the training and validation losses\n",
    "plt.figure(figsize=(10,5))\n",
    "plt.plot(train_losses, label='Training Loss')\n",
    "plt.plot(val_losses, label='Validation Loss')\n",
    "plt.legend()\n",
    "plt.yscale('log')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ModelAutoencoder(\n",
       "  (model_psi): ModelPsi(\n",
       "    (dic): DicNN(\n",
       "      (input_layer): Linear(in_features=6957, out_features=128, bias=False)\n",
       "      (hidden_layers): ModuleList(\n",
       "        (0-2): 3 x Linear(in_features=128, out_features=128, bias=True)\n",
       "      )\n",
       "      (output_layer): Linear(in_features=128, out_features=256, bias=True)\n",
       "      (inv_input_layer): Linear(in_features=256, out_features=128, bias=False)\n",
       "      (inv_hidden_layers): ModuleList(\n",
       "        (0-2): 3 x Linear(in_features=128, out_features=128, bias=True)\n",
       "      )\n",
       "      (inv_output_layer): Linear(in_features=128, out_features=6957, bias=True)\n",
       "      (activation): Tanh()\n",
       "    )\n",
       "  )\n",
       "  (model_inv_psi): ModelPsi()\n",
       ")"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_autoencoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(0.0205, grad_fn=<DivBackward0>)\n"
     ]
    }
   ],
   "source": [
    "x_data_scaled_tensor = torch.tensor(x_data_scaled, dtype=torch.float32)\n",
    "err = x_data_scaled_tensor - model_autoencoder(x_data_scaled_tensor)\n",
    "err_norm = torch.norm(err)\n",
    "x_data_scaled_tensor_norm = torch.norm(x_data_scaled_tensor)\n",
    "ratio = err_norm / x_data_scaled_tensor_norm\n",
    "print(ratio)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(model_autoencoder, 'model_autoencoder.pth')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test for middle layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "layer_sizes_m = [128, 128, 128, 128, 128]\n",
    "model_latent = NonlinearMiddleLayer(layer_sizes_m, n_psi_train, u_dim)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train_latent = model_psi(x_train).detach()\n",
    "y_train_latent = model_psi(y_train).detach()\n",
    "x_test_latent = model_psi(x_test).detach()\n",
    "y_test_latent = model_psi(y_test).detach()\n",
    "\n",
    "# Create a dataset\n",
    "dataset_latent = TensorDataset(x_train_latent, y_train_latent, u_train)\n",
    "\n",
    "# Create a data loader\n",
    "data_loader_latent = DataLoader(dataset_latent, batch_size=64, shuffle=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([5776, 256])"
      ]
     },
     "execution_count": 94,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_train_latent.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the optimizer\n",
    "optimizer_latent = Adam(list(model_latent.parameters()) , lr=0.001)\n",
    "scheduler_latent = StepLR(optimizer_latent, step_size=100, gamma=0.8)\n",
    "\n",
    "mse_loss = nn.MSELoss()\n",
    "\n",
    "epoches = 800\n",
    "\n",
    "# Initialize a list to store the loss values\n",
    "train_losses_latent = []\n",
    "val_losses_latent = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 0, Training Loss: 0.46381375193595886, Validation Loss: 0.465346097946167, Learning Rate: 0.001\n",
      "Epoch: 1, Training Loss: 0.31713661551475525, Validation Loss: 0.32081592082977295, Learning Rate: 0.001\n",
      "Epoch: 2, Training Loss: 0.27599117159843445, Validation Loss: 0.27417147159576416, Learning Rate: 0.001\n",
      "Epoch: 3, Training Loss: 0.256303071975708, Validation Loss: 0.2615835666656494, Learning Rate: 0.001\n",
      "Epoch: 4, Training Loss: 0.2413053810596466, Validation Loss: 0.23918654024600983, Learning Rate: 0.001\n",
      "Epoch: 5, Training Loss: 0.23483124375343323, Validation Loss: 0.23849350214004517, Learning Rate: 0.001\n",
      "Epoch: 6, Training Loss: 0.23350465297698975, Validation Loss: 0.233523890376091, Learning Rate: 0.001\n",
      "Epoch: 7, Training Loss: 0.225118026137352, Validation Loss: 0.22232618927955627, Learning Rate: 0.001\n",
      "Epoch: 8, Training Loss: 0.21781417727470398, Validation Loss: 0.21699978411197662, Learning Rate: 0.001\n",
      "Epoch: 9, Training Loss: 0.2160518616437912, Validation Loss: 0.2195218950510025, Learning Rate: 0.001\n",
      "Epoch: 10, Training Loss: 0.20731547474861145, Validation Loss: 0.21546673774719238, Learning Rate: 0.001\n",
      "Epoch: 11, Training Loss: 0.20104463398456573, Validation Loss: 0.21601858735084534, Learning Rate: 0.001\n",
      "Epoch: 12, Training Loss: 0.200829416513443, Validation Loss: 0.2114270031452179, Learning Rate: 0.001\n",
      "Epoch: 13, Training Loss: 0.19579078257083893, Validation Loss: 0.19722048938274384, Learning Rate: 0.001\n",
      "Epoch: 14, Training Loss: 0.1947149932384491, Validation Loss: 0.19585412740707397, Learning Rate: 0.001\n",
      "Epoch: 15, Training Loss: 0.19165581464767456, Validation Loss: 0.19037224352359772, Learning Rate: 0.001\n",
      "Epoch: 16, Training Loss: 0.18566294014453888, Validation Loss: 0.17819739878177643, Learning Rate: 0.001\n",
      "Epoch: 17, Training Loss: 0.18648658692836761, Validation Loss: 0.1927713304758072, Learning Rate: 0.001\n",
      "Epoch: 18, Training Loss: 0.18497730791568756, Validation Loss: 0.18548007309436798, Learning Rate: 0.001\n",
      "Epoch: 19, Training Loss: 0.1813383400440216, Validation Loss: 0.18425343930721283, Learning Rate: 0.001\n",
      "Epoch: 20, Training Loss: 0.17866526544094086, Validation Loss: 0.18699441850185394, Learning Rate: 0.001\n",
      "Epoch: 21, Training Loss: 0.17787426710128784, Validation Loss: 0.1858757734298706, Learning Rate: 0.001\n",
      "Epoch: 22, Training Loss: 0.17876899242401123, Validation Loss: 0.18038135766983032, Learning Rate: 0.001\n",
      "Epoch: 23, Training Loss: 0.17905935645103455, Validation Loss: 0.1817215383052826, Learning Rate: 0.001\n",
      "Epoch: 24, Training Loss: 0.1722504198551178, Validation Loss: 0.17805135250091553, Learning Rate: 0.001\n",
      "Epoch: 25, Training Loss: 0.17153671383857727, Validation Loss: 0.1744634211063385, Learning Rate: 0.001\n",
      "Epoch: 26, Training Loss: 0.17191068828105927, Validation Loss: 0.17477481067180634, Learning Rate: 0.001\n",
      "Epoch: 27, Training Loss: 0.17176038026809692, Validation Loss: 0.17628411948680878, Learning Rate: 0.001\n",
      "Epoch: 28, Training Loss: 0.16997875273227692, Validation Loss: 0.1832912415266037, Learning Rate: 0.001\n",
      "Epoch: 29, Training Loss: 0.16597092151641846, Validation Loss: 0.17547467350959778, Learning Rate: 0.001\n",
      "Epoch: 30, Training Loss: 0.16890433430671692, Validation Loss: 0.1777152419090271, Learning Rate: 0.001\n",
      "Epoch: 31, Training Loss: 0.1664917767047882, Validation Loss: 0.1779920756816864, Learning Rate: 0.001\n",
      "Epoch: 32, Training Loss: 0.16585618257522583, Validation Loss: 0.16689516603946686, Learning Rate: 0.001\n",
      "Epoch: 33, Training Loss: 0.16137148439884186, Validation Loss: 0.16749095916748047, Learning Rate: 0.001\n",
      "Epoch: 34, Training Loss: 0.16232582926750183, Validation Loss: 0.16779595613479614, Learning Rate: 0.001\n",
      "Epoch: 35, Training Loss: 0.1604224294424057, Validation Loss: 0.16834761202335358, Learning Rate: 0.001\n",
      "Epoch: 36, Training Loss: 0.16351909935474396, Validation Loss: 0.17012660205364227, Learning Rate: 0.001\n",
      "Epoch: 37, Training Loss: 0.1578184962272644, Validation Loss: 0.1681288480758667, Learning Rate: 0.001\n",
      "Epoch: 38, Training Loss: 0.1638755202293396, Validation Loss: 0.16988897323608398, Learning Rate: 0.001\n",
      "Epoch: 39, Training Loss: 0.1552194356918335, Validation Loss: 0.15879741311073303, Learning Rate: 0.001\n",
      "Epoch: 40, Training Loss: 0.15645822882652283, Validation Loss: 0.15973104536533356, Learning Rate: 0.001\n",
      "Epoch: 41, Training Loss: 0.1561167687177658, Validation Loss: 0.15930286049842834, Learning Rate: 0.001\n",
      "Epoch: 42, Training Loss: 0.15653420984745026, Validation Loss: 0.1647324115037918, Learning Rate: 0.001\n",
      "Epoch: 43, Training Loss: 0.1515132337808609, Validation Loss: 0.16186346113681793, Learning Rate: 0.001\n",
      "Epoch: 44, Training Loss: 0.15541768074035645, Validation Loss: 0.16078966856002808, Learning Rate: 0.001\n",
      "Epoch: 45, Training Loss: 0.15128430724143982, Validation Loss: 0.1546361893415451, Learning Rate: 0.001\n",
      "Epoch: 46, Training Loss: 0.15235339105129242, Validation Loss: 0.15136434137821198, Learning Rate: 0.001\n",
      "Epoch: 47, Training Loss: 0.14836573600769043, Validation Loss: 0.16156627237796783, Learning Rate: 0.001\n",
      "Epoch: 48, Training Loss: 0.14911723136901855, Validation Loss: 0.15706488490104675, Learning Rate: 0.001\n",
      "Epoch: 49, Training Loss: 0.1519871950149536, Validation Loss: 0.15035994350910187, Learning Rate: 0.001\n",
      "Epoch: 50, Training Loss: 0.1488794982433319, Validation Loss: 0.15666337311267853, Learning Rate: 0.001\n",
      "Epoch: 51, Training Loss: 0.14808282256126404, Validation Loss: 0.15593081712722778, Learning Rate: 0.001\n",
      "Epoch: 52, Training Loss: 0.14908671379089355, Validation Loss: 0.15743431448936462, Learning Rate: 0.001\n",
      "Epoch: 53, Training Loss: 0.1473950296640396, Validation Loss: 0.15528954565525055, Learning Rate: 0.001\n",
      "Epoch: 54, Training Loss: 0.1500183641910553, Validation Loss: 0.15293288230895996, Learning Rate: 0.001\n",
      "Epoch: 55, Training Loss: 0.14689338207244873, Validation Loss: 0.15019668638706207, Learning Rate: 0.001\n",
      "Epoch: 56, Training Loss: 0.14034661650657654, Validation Loss: 0.14935272932052612, Learning Rate: 0.001\n",
      "Epoch: 57, Training Loss: 0.14366279542446136, Validation Loss: 0.14708927273750305, Learning Rate: 0.001\n",
      "Epoch: 58, Training Loss: 0.1434827744960785, Validation Loss: 0.15020257234573364, Learning Rate: 0.001\n",
      "Epoch: 59, Training Loss: 0.14745455980300903, Validation Loss: 0.15245036780834198, Learning Rate: 0.001\n",
      "Epoch: 60, Training Loss: 0.14298196136951447, Validation Loss: 0.14698196947574615, Learning Rate: 0.001\n",
      "Epoch: 61, Training Loss: 0.14219743013381958, Validation Loss: 0.14686307311058044, Learning Rate: 0.001\n",
      "Epoch: 62, Training Loss: 0.1429487019777298, Validation Loss: 0.14804476499557495, Learning Rate: 0.001\n",
      "Epoch: 63, Training Loss: 0.1409515142440796, Validation Loss: 0.14721539616584778, Learning Rate: 0.001\n",
      "Epoch: 64, Training Loss: 0.14239756762981415, Validation Loss: 0.1472172886133194, Learning Rate: 0.001\n",
      "Epoch: 65, Training Loss: 0.13828690350055695, Validation Loss: 0.14437104761600494, Learning Rate: 0.001\n",
      "Epoch: 66, Training Loss: 0.14063768088817596, Validation Loss: 0.14539170265197754, Learning Rate: 0.001\n",
      "Epoch: 67, Training Loss: 0.14042682945728302, Validation Loss: 0.14802561700344086, Learning Rate: 0.001\n",
      "Epoch: 68, Training Loss: 0.13869266211986542, Validation Loss: 0.14441803097724915, Learning Rate: 0.001\n",
      "Epoch: 69, Training Loss: 0.14424309134483337, Validation Loss: 0.14956390857696533, Learning Rate: 0.001\n",
      "Epoch: 70, Training Loss: 0.13687622547149658, Validation Loss: 0.14491203427314758, Learning Rate: 0.001\n",
      "Epoch: 71, Training Loss: 0.13463319838047028, Validation Loss: 0.14758798480033875, Learning Rate: 0.001\n",
      "Epoch: 72, Training Loss: 0.13875333964824677, Validation Loss: 0.14194466173648834, Learning Rate: 0.001\n",
      "Epoch: 73, Training Loss: 0.13545531034469604, Validation Loss: 0.13964171707630157, Learning Rate: 0.001\n",
      "Epoch: 74, Training Loss: 0.13849671185016632, Validation Loss: 0.14652790129184723, Learning Rate: 0.001\n",
      "Epoch: 75, Training Loss: 0.1351865977048874, Validation Loss: 0.1396133154630661, Learning Rate: 0.001\n",
      "Epoch: 76, Training Loss: 0.1317370980978012, Validation Loss: 0.13659325242042542, Learning Rate: 0.001\n",
      "Epoch: 77, Training Loss: 0.13383391499519348, Validation Loss: 0.1361486166715622, Learning Rate: 0.001\n",
      "Epoch: 78, Training Loss: 0.13206611573696136, Validation Loss: 0.1410365104675293, Learning Rate: 0.001\n",
      "Epoch: 79, Training Loss: 0.13175511360168457, Validation Loss: 0.13939496874809265, Learning Rate: 0.001\n",
      "Epoch: 80, Training Loss: 0.13213229179382324, Validation Loss: 0.13660205900669098, Learning Rate: 0.001\n",
      "Epoch: 81, Training Loss: 0.13456343114376068, Validation Loss: 0.1345820277929306, Learning Rate: 0.001\n",
      "Epoch: 82, Training Loss: 0.1334248185157776, Validation Loss: 0.1380285620689392, Learning Rate: 0.001\n",
      "Epoch: 83, Training Loss: 0.13108356297016144, Validation Loss: 0.1394645720720291, Learning Rate: 0.001\n",
      "Epoch: 84, Training Loss: 0.13083311915397644, Validation Loss: 0.1395004242658615, Learning Rate: 0.001\n",
      "Epoch: 85, Training Loss: 0.13406848907470703, Validation Loss: 0.13639135658740997, Learning Rate: 0.001\n",
      "Epoch: 86, Training Loss: 0.132608562707901, Validation Loss: 0.1366536021232605, Learning Rate: 0.001\n",
      "Epoch: 87, Training Loss: 0.12928591668605804, Validation Loss: 0.13744240999221802, Learning Rate: 0.001\n",
      "Epoch: 88, Training Loss: 0.13304144144058228, Validation Loss: 0.13626039028167725, Learning Rate: 0.001\n",
      "Epoch: 89, Training Loss: 0.13121183216571808, Validation Loss: 0.13421288132667542, Learning Rate: 0.001\n",
      "Epoch: 90, Training Loss: 0.13176892697811127, Validation Loss: 0.13400235772132874, Learning Rate: 0.001\n",
      "Epoch: 91, Training Loss: 0.13222387433052063, Validation Loss: 0.13926763832569122, Learning Rate: 0.001\n",
      "Epoch: 92, Training Loss: 0.13041016459465027, Validation Loss: 0.14130431413650513, Learning Rate: 0.001\n",
      "Epoch: 93, Training Loss: 0.13224394619464874, Validation Loss: 0.13504891097545624, Learning Rate: 0.001\n",
      "Epoch: 94, Training Loss: 0.13278989493846893, Validation Loss: 0.13942950963974, Learning Rate: 0.001\n",
      "Epoch: 95, Training Loss: 0.1312536895275116, Validation Loss: 0.13563507795333862, Learning Rate: 0.001\n",
      "Epoch: 96, Training Loss: 0.12961256504058838, Validation Loss: 0.1346888244152069, Learning Rate: 0.001\n",
      "Epoch: 97, Training Loss: 0.12928450107574463, Validation Loss: 0.13115182518959045, Learning Rate: 0.001\n",
      "Epoch: 98, Training Loss: 0.12827663123607635, Validation Loss: 0.1326570212841034, Learning Rate: 0.001\n",
      "Epoch: 99, Training Loss: 0.12910446524620056, Validation Loss: 0.13766025006771088, Learning Rate: 0.001\n",
      "Epoch: 100, Training Loss: 0.12792690098285675, Validation Loss: 0.1332215815782547, Learning Rate: 0.0008\n",
      "Epoch: 101, Training Loss: 0.12653301656246185, Validation Loss: 0.1340743899345398, Learning Rate: 0.0008\n",
      "Epoch: 102, Training Loss: 0.1275581568479538, Validation Loss: 0.1356060653924942, Learning Rate: 0.0008\n",
      "Epoch: 103, Training Loss: 0.12517966330051422, Validation Loss: 0.13074195384979248, Learning Rate: 0.0008\n",
      "Epoch: 104, Training Loss: 0.12426699697971344, Validation Loss: 0.13127641379833221, Learning Rate: 0.0008\n",
      "Epoch: 105, Training Loss: 0.12430909276008606, Validation Loss: 0.13124166429042816, Learning Rate: 0.0008\n",
      "Epoch: 106, Training Loss: 0.12493424862623215, Validation Loss: 0.1287878006696701, Learning Rate: 0.0008\n",
      "Epoch: 107, Training Loss: 0.12483483552932739, Validation Loss: 0.1324073076248169, Learning Rate: 0.0008\n",
      "Epoch: 108, Training Loss: 0.12352431565523148, Validation Loss: 0.12578079104423523, Learning Rate: 0.0008\n",
      "Epoch: 109, Training Loss: 0.12508833408355713, Validation Loss: 0.12794852256774902, Learning Rate: 0.0008\n",
      "Epoch: 110, Training Loss: 0.12488244473934174, Validation Loss: 0.1314505934715271, Learning Rate: 0.0008\n",
      "Epoch: 111, Training Loss: 0.12543213367462158, Validation Loss: 0.12647871673107147, Learning Rate: 0.0008\n",
      "Epoch: 112, Training Loss: 0.12459922581911087, Validation Loss: 0.129746675491333, Learning Rate: 0.0008\n",
      "Epoch: 113, Training Loss: 0.12563027441501617, Validation Loss: 0.1294339895248413, Learning Rate: 0.0008\n",
      "Epoch: 114, Training Loss: 0.12500223517417908, Validation Loss: 0.13628529012203217, Learning Rate: 0.0008\n",
      "Epoch: 115, Training Loss: 0.12501609325408936, Validation Loss: 0.12840473651885986, Learning Rate: 0.0008\n",
      "Epoch: 116, Training Loss: 0.1235634833574295, Validation Loss: 0.13090364634990692, Learning Rate: 0.0008\n",
      "Epoch: 117, Training Loss: 0.12458197772502899, Validation Loss: 0.12627850472927094, Learning Rate: 0.0008\n",
      "Epoch: 118, Training Loss: 0.12432698905467987, Validation Loss: 0.129852756857872, Learning Rate: 0.0008\n",
      "Epoch: 119, Training Loss: 0.12408798187971115, Validation Loss: 0.13169686496257782, Learning Rate: 0.0008\n",
      "Epoch: 120, Training Loss: 0.12389523535966873, Validation Loss: 0.1321662813425064, Learning Rate: 0.0008\n",
      "Epoch: 121, Training Loss: 0.1252768635749817, Validation Loss: 0.12995676696300507, Learning Rate: 0.0008\n",
      "Epoch: 122, Training Loss: 0.1250257045030594, Validation Loss: 0.12622301280498505, Learning Rate: 0.0008\n",
      "Epoch: 123, Training Loss: 0.12412115931510925, Validation Loss: 0.12929260730743408, Learning Rate: 0.0008\n",
      "Epoch: 124, Training Loss: 0.12321888655424118, Validation Loss: 0.1275268793106079, Learning Rate: 0.0008\n",
      "Epoch: 125, Training Loss: 0.12413449585437775, Validation Loss: 0.12846088409423828, Learning Rate: 0.0008\n",
      "Epoch: 126, Training Loss: 0.12401314824819565, Validation Loss: 0.12827520072460175, Learning Rate: 0.0008\n",
      "Epoch: 127, Training Loss: 0.12507006525993347, Validation Loss: 0.12866850197315216, Learning Rate: 0.0008\n",
      "Epoch: 128, Training Loss: 0.12456043064594269, Validation Loss: 0.13119451701641083, Learning Rate: 0.0008\n",
      "Epoch: 129, Training Loss: 0.1235877200961113, Validation Loss: 0.13013838231563568, Learning Rate: 0.0008\n",
      "Epoch: 130, Training Loss: 0.12266860902309418, Validation Loss: 0.131257101893425, Learning Rate: 0.0008\n",
      "Epoch: 131, Training Loss: 0.12193714827299118, Validation Loss: 0.12682974338531494, Learning Rate: 0.0008\n",
      "Epoch: 132, Training Loss: 0.1249437928199768, Validation Loss: 0.12807363271713257, Learning Rate: 0.0008\n",
      "Epoch: 133, Training Loss: 0.12275054305791855, Validation Loss: 0.12843918800354004, Learning Rate: 0.0008\n",
      "Epoch: 134, Training Loss: 0.12521065771579742, Validation Loss: 0.13039898872375488, Learning Rate: 0.0008\n",
      "Epoch: 135, Training Loss: 0.12355223298072815, Validation Loss: 0.12797343730926514, Learning Rate: 0.0008\n",
      "Epoch: 136, Training Loss: 0.12312165647745132, Validation Loss: 0.12450511008501053, Learning Rate: 0.0008\n",
      "Epoch: 137, Training Loss: 0.12147399038076401, Validation Loss: 0.13004331290721893, Learning Rate: 0.0008\n",
      "Epoch: 138, Training Loss: 0.1252625584602356, Validation Loss: 0.12758243083953857, Learning Rate: 0.0008\n",
      "Epoch: 139, Training Loss: 0.12076662480831146, Validation Loss: 0.12612248957157135, Learning Rate: 0.0008\n",
      "Epoch: 140, Training Loss: 0.12273789197206497, Validation Loss: 0.1277572363615036, Learning Rate: 0.0008\n",
      "Epoch: 141, Training Loss: 0.12148840725421906, Validation Loss: 0.12533998489379883, Learning Rate: 0.0008\n",
      "Epoch: 142, Training Loss: 0.12183600664138794, Validation Loss: 0.13151668012142181, Learning Rate: 0.0008\n",
      "Epoch: 143, Training Loss: 0.12190545350313187, Validation Loss: 0.12566448748111725, Learning Rate: 0.0008\n",
      "Epoch: 144, Training Loss: 0.12455728650093079, Validation Loss: 0.1272272765636444, Learning Rate: 0.0008\n",
      "Epoch: 145, Training Loss: 0.12485349923372269, Validation Loss: 0.12739719450473785, Learning Rate: 0.0008\n",
      "Epoch: 146, Training Loss: 0.12205807864665985, Validation Loss: 0.1260756254196167, Learning Rate: 0.0008\n",
      "Epoch: 147, Training Loss: 0.12170823663473129, Validation Loss: 0.12739647924900055, Learning Rate: 0.0008\n",
      "Epoch: 148, Training Loss: 0.12068063020706177, Validation Loss: 0.12590335309505463, Learning Rate: 0.0008\n",
      "Epoch: 149, Training Loss: 0.12142827361822128, Validation Loss: 0.12941041588783264, Learning Rate: 0.0008\n",
      "Epoch: 150, Training Loss: 0.12257911264896393, Validation Loss: 0.13034939765930176, Learning Rate: 0.0008\n",
      "Epoch: 151, Training Loss: 0.12123583257198334, Validation Loss: 0.12589380145072937, Learning Rate: 0.0008\n",
      "Epoch: 152, Training Loss: 0.12054140865802765, Validation Loss: 0.12629230320453644, Learning Rate: 0.0008\n",
      "Epoch: 153, Training Loss: 0.12227984517812729, Validation Loss: 0.12545883655548096, Learning Rate: 0.0008\n",
      "Epoch: 154, Training Loss: 0.12090002745389938, Validation Loss: 0.12820185720920563, Learning Rate: 0.0008\n",
      "Epoch: 155, Training Loss: 0.12002994865179062, Validation Loss: 0.12397830188274384, Learning Rate: 0.0008\n",
      "Epoch: 156, Training Loss: 0.12064814567565918, Validation Loss: 0.12792202830314636, Learning Rate: 0.0008\n",
      "Epoch: 157, Training Loss: 0.12145793437957764, Validation Loss: 0.13053719699382782, Learning Rate: 0.0008\n",
      "Epoch: 158, Training Loss: 0.12028432637453079, Validation Loss: 0.12345258891582489, Learning Rate: 0.0008\n",
      "Epoch: 159, Training Loss: 0.1219852864742279, Validation Loss: 0.12436507642269135, Learning Rate: 0.0008\n",
      "Epoch: 160, Training Loss: 0.12371063977479935, Validation Loss: 0.12710003554821014, Learning Rate: 0.0008\n",
      "Epoch: 161, Training Loss: 0.119899220764637, Validation Loss: 0.12465894967317581, Learning Rate: 0.0008\n",
      "Epoch: 162, Training Loss: 0.12193985283374786, Validation Loss: 0.123678058385849, Learning Rate: 0.0008\n",
      "Epoch: 163, Training Loss: 0.11999471485614777, Validation Loss: 0.12541435658931732, Learning Rate: 0.0008\n",
      "Epoch: 164, Training Loss: 0.1193927749991417, Validation Loss: 0.12429240345954895, Learning Rate: 0.0008\n",
      "Epoch: 165, Training Loss: 0.11995873600244522, Validation Loss: 0.1258154809474945, Learning Rate: 0.0008\n",
      "Epoch: 166, Training Loss: 0.11970300227403641, Validation Loss: 0.12503796815872192, Learning Rate: 0.0008\n",
      "Epoch: 167, Training Loss: 0.12102603912353516, Validation Loss: 0.12924569845199585, Learning Rate: 0.0008\n",
      "Epoch: 168, Training Loss: 0.11965886503458023, Validation Loss: 0.12377319484949112, Learning Rate: 0.0008\n",
      "Epoch: 169, Training Loss: 0.11957800388336182, Validation Loss: 0.1304103136062622, Learning Rate: 0.0008\n",
      "Epoch: 170, Training Loss: 0.12054413557052612, Validation Loss: 0.12597329914569855, Learning Rate: 0.0008\n",
      "Epoch: 171, Training Loss: 0.12086687237024307, Validation Loss: 0.12142670154571533, Learning Rate: 0.0008\n",
      "Epoch: 172, Training Loss: 0.11797044426202774, Validation Loss: 0.12244725227355957, Learning Rate: 0.0008\n",
      "Epoch: 173, Training Loss: 0.11993923783302307, Validation Loss: 0.12347377091646194, Learning Rate: 0.0008\n",
      "Epoch: 174, Training Loss: 0.11889906227588654, Validation Loss: 0.12317442893981934, Learning Rate: 0.0008\n",
      "Epoch: 175, Training Loss: 0.11826848238706589, Validation Loss: 0.11917392164468765, Learning Rate: 0.0008\n",
      "Epoch: 176, Training Loss: 0.12022560834884644, Validation Loss: 0.12650449573993683, Learning Rate: 0.0008\n",
      "Epoch: 177, Training Loss: 0.11861880123615265, Validation Loss: 0.12357724457979202, Learning Rate: 0.0008\n",
      "Epoch: 178, Training Loss: 0.11789774894714355, Validation Loss: 0.12683643400669098, Learning Rate: 0.0008\n",
      "Epoch: 179, Training Loss: 0.11761593073606491, Validation Loss: 0.12669938802719116, Learning Rate: 0.0008\n",
      "Epoch: 180, Training Loss: 0.11844711005687714, Validation Loss: 0.11886437982320786, Learning Rate: 0.0008\n",
      "Epoch: 181, Training Loss: 0.11899058520793915, Validation Loss: 0.12203244864940643, Learning Rate: 0.0008\n",
      "Epoch: 182, Training Loss: 0.11808476597070694, Validation Loss: 0.12396728992462158, Learning Rate: 0.0008\n",
      "Epoch: 183, Training Loss: 0.11753185093402863, Validation Loss: 0.12628073990345, Learning Rate: 0.0008\n",
      "Epoch: 184, Training Loss: 0.12217095494270325, Validation Loss: 0.126613587141037, Learning Rate: 0.0008\n",
      "Epoch: 185, Training Loss: 0.1223430186510086, Validation Loss: 0.12585346400737762, Learning Rate: 0.0008\n",
      "Epoch: 186, Training Loss: 0.11926908791065216, Validation Loss: 0.1270403116941452, Learning Rate: 0.0008\n",
      "Epoch: 187, Training Loss: 0.11928075551986694, Validation Loss: 0.12523220479488373, Learning Rate: 0.0008\n",
      "Epoch: 188, Training Loss: 0.11769187450408936, Validation Loss: 0.12165632843971252, Learning Rate: 0.0008\n",
      "Epoch: 189, Training Loss: 0.11847638338804245, Validation Loss: 0.12162328511476517, Learning Rate: 0.0008\n",
      "Epoch: 190, Training Loss: 0.1175304427742958, Validation Loss: 0.1263989955186844, Learning Rate: 0.0008\n",
      "Epoch: 191, Training Loss: 0.11899124830961227, Validation Loss: 0.12269891053438187, Learning Rate: 0.0008\n",
      "Epoch: 192, Training Loss: 0.11789024621248245, Validation Loss: 0.1263277530670166, Learning Rate: 0.0008\n",
      "Epoch: 193, Training Loss: 0.12010172009468079, Validation Loss: 0.1255159080028534, Learning Rate: 0.0008\n",
      "Epoch: 194, Training Loss: 0.11810994148254395, Validation Loss: 0.1274680495262146, Learning Rate: 0.0008\n",
      "Epoch: 195, Training Loss: 0.11674976348876953, Validation Loss: 0.12483546882867813, Learning Rate: 0.0008\n",
      "Epoch: 196, Training Loss: 0.11955121904611588, Validation Loss: 0.12504538893699646, Learning Rate: 0.0008\n",
      "Epoch: 197, Training Loss: 0.11864617466926575, Validation Loss: 0.12328749150037766, Learning Rate: 0.0008\n",
      "Epoch: 198, Training Loss: 0.11992820352315903, Validation Loss: 0.12317880988121033, Learning Rate: 0.0008\n",
      "Epoch: 199, Training Loss: 0.11735733598470688, Validation Loss: 0.12382244318723679, Learning Rate: 0.0008\n",
      "Epoch: 200, Training Loss: 0.1184590756893158, Validation Loss: 0.12646247446537018, Learning Rate: 0.00064\n",
      "Epoch: 201, Training Loss: 0.1196741834282875, Validation Loss: 0.1233750730752945, Learning Rate: 0.00064\n",
      "Epoch: 202, Training Loss: 0.11889228969812393, Validation Loss: 0.12195491790771484, Learning Rate: 0.00064\n",
      "Epoch: 203, Training Loss: 0.11788679659366608, Validation Loss: 0.12289271503686905, Learning Rate: 0.00064\n",
      "Epoch: 204, Training Loss: 0.11785151809453964, Validation Loss: 0.12519721686840057, Learning Rate: 0.00064\n",
      "Epoch: 205, Training Loss: 0.11893061548471451, Validation Loss: 0.12735086679458618, Learning Rate: 0.00064\n",
      "Epoch: 206, Training Loss: 0.11766315251588821, Validation Loss: 0.12261006981134415, Learning Rate: 0.00064\n",
      "Epoch: 207, Training Loss: 0.11634518951177597, Validation Loss: 0.12353574484586716, Learning Rate: 0.00064\n",
      "Epoch: 208, Training Loss: 0.11678650230169296, Validation Loss: 0.12639513611793518, Learning Rate: 0.00064\n",
      "Epoch: 209, Training Loss: 0.11755318194627762, Validation Loss: 0.1250370740890503, Learning Rate: 0.00064\n",
      "Epoch: 210, Training Loss: 0.11796926707029343, Validation Loss: 0.1251770704984665, Learning Rate: 0.00064\n",
      "Epoch: 211, Training Loss: 0.11823831498622894, Validation Loss: 0.11812011152505875, Learning Rate: 0.00064\n",
      "Epoch: 212, Training Loss: 0.11690978705883026, Validation Loss: 0.12127301096916199, Learning Rate: 0.00064\n",
      "Epoch: 213, Training Loss: 0.11687435954809189, Validation Loss: 0.12253261357545853, Learning Rate: 0.00064\n",
      "Epoch: 214, Training Loss: 0.11620906740427017, Validation Loss: 0.12013830989599228, Learning Rate: 0.00064\n",
      "Epoch: 215, Training Loss: 0.1180051863193512, Validation Loss: 0.12139356881380081, Learning Rate: 0.00064\n",
      "Epoch: 216, Training Loss: 0.11680620163679123, Validation Loss: 0.12473992258310318, Learning Rate: 0.00064\n",
      "Epoch: 217, Training Loss: 0.11701325327157974, Validation Loss: 0.12309614568948746, Learning Rate: 0.00064\n",
      "Epoch: 218, Training Loss: 0.11517158895730972, Validation Loss: 0.12243225425481796, Learning Rate: 0.00064\n",
      "Epoch: 219, Training Loss: 0.11660410463809967, Validation Loss: 0.12436611205339432, Learning Rate: 0.00064\n",
      "Epoch: 220, Training Loss: 0.11595486849546432, Validation Loss: 0.11779989302158356, Learning Rate: 0.00064\n",
      "Epoch: 221, Training Loss: 0.11600547283887863, Validation Loss: 0.12181860208511353, Learning Rate: 0.00064\n",
      "Epoch: 222, Training Loss: 0.11563952267169952, Validation Loss: 0.12000563740730286, Learning Rate: 0.00064\n",
      "Epoch: 223, Training Loss: 0.11727149039506912, Validation Loss: 0.11863414943218231, Learning Rate: 0.00064\n",
      "Epoch: 224, Training Loss: 0.11620255559682846, Validation Loss: 0.12236379832029343, Learning Rate: 0.00064\n",
      "Epoch: 225, Training Loss: 0.11591459810733795, Validation Loss: 0.12165101617574692, Learning Rate: 0.00064\n",
      "Epoch: 226, Training Loss: 0.11757796257734299, Validation Loss: 0.12430047243833542, Learning Rate: 0.00064\n",
      "Epoch: 227, Training Loss: 0.1162005215883255, Validation Loss: 0.12361800670623779, Learning Rate: 0.00064\n",
      "Epoch: 228, Training Loss: 0.11652792990207672, Validation Loss: 0.11832240223884583, Learning Rate: 0.00064\n",
      "Epoch: 229, Training Loss: 0.11554422229528427, Validation Loss: 0.12118060141801834, Learning Rate: 0.00064\n",
      "Epoch: 230, Training Loss: 0.11638671159744263, Validation Loss: 0.12168720364570618, Learning Rate: 0.00064\n",
      "Epoch: 231, Training Loss: 0.11688406020402908, Validation Loss: 0.12413667142391205, Learning Rate: 0.00064\n",
      "Epoch: 232, Training Loss: 0.11694420129060745, Validation Loss: 0.12141744792461395, Learning Rate: 0.00064\n",
      "Epoch: 233, Training Loss: 0.11556877940893173, Validation Loss: 0.12063644081354141, Learning Rate: 0.00064\n",
      "Epoch: 234, Training Loss: 0.1158532202243805, Validation Loss: 0.11876770853996277, Learning Rate: 0.00064\n",
      "Epoch: 235, Training Loss: 0.11520703881978989, Validation Loss: 0.11910723149776459, Learning Rate: 0.00064\n",
      "Epoch: 236, Training Loss: 0.11671829968690872, Validation Loss: 0.12674492597579956, Learning Rate: 0.00064\n",
      "Epoch: 237, Training Loss: 0.11854361742734909, Validation Loss: 0.12215164303779602, Learning Rate: 0.00064\n",
      "Epoch: 238, Training Loss: 0.11544156074523926, Validation Loss: 0.12099192291498184, Learning Rate: 0.00064\n",
      "Epoch: 239, Training Loss: 0.11575651168823242, Validation Loss: 0.12207537889480591, Learning Rate: 0.00064\n",
      "Epoch: 240, Training Loss: 0.11628778278827667, Validation Loss: 0.12465844303369522, Learning Rate: 0.00064\n",
      "Epoch: 241, Training Loss: 0.11564883589744568, Validation Loss: 0.1199110820889473, Learning Rate: 0.00064\n",
      "Epoch: 242, Training Loss: 0.11727740615606308, Validation Loss: 0.12302503734827042, Learning Rate: 0.00064\n",
      "Epoch: 243, Training Loss: 0.1151016354560852, Validation Loss: 0.12325645238161087, Learning Rate: 0.00064\n",
      "Epoch: 244, Training Loss: 0.11523500829935074, Validation Loss: 0.1201612651348114, Learning Rate: 0.00064\n",
      "Epoch: 245, Training Loss: 0.11734506487846375, Validation Loss: 0.12382730841636658, Learning Rate: 0.00064\n",
      "Epoch: 246, Training Loss: 0.11619312316179276, Validation Loss: 0.1180407851934433, Learning Rate: 0.00064\n",
      "Epoch: 247, Training Loss: 0.11624909192323685, Validation Loss: 0.12227526307106018, Learning Rate: 0.00064\n",
      "Epoch: 248, Training Loss: 0.1158512532711029, Validation Loss: 0.12251210957765579, Learning Rate: 0.00064\n",
      "Epoch: 249, Training Loss: 0.11558818817138672, Validation Loss: 0.11853569746017456, Learning Rate: 0.00064\n",
      "Epoch: 250, Training Loss: 0.11405925452709198, Validation Loss: 0.12119365483522415, Learning Rate: 0.00064\n",
      "Epoch: 251, Training Loss: 0.11560212820768356, Validation Loss: 0.11994966119527817, Learning Rate: 0.00064\n",
      "Epoch: 252, Training Loss: 0.11659988760948181, Validation Loss: 0.12306731939315796, Learning Rate: 0.00064\n",
      "Epoch: 253, Training Loss: 0.1161542534828186, Validation Loss: 0.12354300916194916, Learning Rate: 0.00064\n",
      "Epoch: 254, Training Loss: 0.1149631068110466, Validation Loss: 0.11900559067726135, Learning Rate: 0.00064\n",
      "Epoch: 255, Training Loss: 0.11416067183017731, Validation Loss: 0.12073753774166107, Learning Rate: 0.00064\n",
      "Epoch: 256, Training Loss: 0.11539607495069504, Validation Loss: 0.12325724214315414, Learning Rate: 0.00064\n",
      "Epoch: 257, Training Loss: 0.11378994584083557, Validation Loss: 0.11716345697641373, Learning Rate: 0.00064\n",
      "Epoch: 258, Training Loss: 0.11592063307762146, Validation Loss: 0.11976674199104309, Learning Rate: 0.00064\n",
      "Epoch: 259, Training Loss: 0.11399566382169724, Validation Loss: 0.12119555473327637, Learning Rate: 0.00064\n",
      "Epoch: 260, Training Loss: 0.11630883067846298, Validation Loss: 0.12133368849754333, Learning Rate: 0.00064\n",
      "Epoch: 261, Training Loss: 0.11417704820632935, Validation Loss: 0.11545870453119278, Learning Rate: 0.00064\n",
      "Epoch: 262, Training Loss: 0.11492919921875, Validation Loss: 0.12190112471580505, Learning Rate: 0.00064\n",
      "Epoch: 263, Training Loss: 0.1154722198843956, Validation Loss: 0.12150763720273972, Learning Rate: 0.00064\n",
      "Epoch: 264, Training Loss: 0.11397285014390945, Validation Loss: 0.12278519570827484, Learning Rate: 0.00064\n",
      "Epoch: 265, Training Loss: 0.11531554162502289, Validation Loss: 0.11908138543367386, Learning Rate: 0.00064\n",
      "Epoch: 266, Training Loss: 0.11317405849695206, Validation Loss: 0.11838847398757935, Learning Rate: 0.00064\n",
      "Epoch: 267, Training Loss: 0.11426746845245361, Validation Loss: 0.11979087442159653, Learning Rate: 0.00064\n",
      "Epoch: 268, Training Loss: 0.11497631669044495, Validation Loss: 0.12220963835716248, Learning Rate: 0.00064\n",
      "Epoch: 269, Training Loss: 0.11573109030723572, Validation Loss: 0.12478147447109222, Learning Rate: 0.00064\n",
      "Epoch: 270, Training Loss: 0.11290410161018372, Validation Loss: 0.12135077267885208, Learning Rate: 0.00064\n",
      "Epoch: 271, Training Loss: 0.11475526541471481, Validation Loss: 0.12051264196634293, Learning Rate: 0.00064\n",
      "Epoch: 272, Training Loss: 0.11562539637088776, Validation Loss: 0.12122419476509094, Learning Rate: 0.00064\n",
      "Epoch: 273, Training Loss: 0.11457100510597229, Validation Loss: 0.12077268958091736, Learning Rate: 0.00064\n",
      "Epoch: 274, Training Loss: 0.11304999142885208, Validation Loss: 0.12009753286838531, Learning Rate: 0.00064\n",
      "Epoch: 275, Training Loss: 0.11468145251274109, Validation Loss: 0.11758800595998764, Learning Rate: 0.00064\n",
      "Epoch: 276, Training Loss: 0.11345607787370682, Validation Loss: 0.12146013230085373, Learning Rate: 0.00064\n",
      "Epoch: 277, Training Loss: 0.11515907198190689, Validation Loss: 0.11795360594987869, Learning Rate: 0.00064\n",
      "Epoch: 278, Training Loss: 0.11326344311237335, Validation Loss: 0.11987604945898056, Learning Rate: 0.00064\n",
      "Epoch: 279, Training Loss: 0.11516853421926498, Validation Loss: 0.11810492724180222, Learning Rate: 0.00064\n",
      "Epoch: 280, Training Loss: 0.1141350120306015, Validation Loss: 0.11790838092565536, Learning Rate: 0.00064\n",
      "Epoch: 281, Training Loss: 0.11298342794179916, Validation Loss: 0.12205874174833298, Learning Rate: 0.00064\n",
      "Epoch: 282, Training Loss: 0.11618727445602417, Validation Loss: 0.12098825722932816, Learning Rate: 0.00064\n",
      "Epoch: 283, Training Loss: 0.11441199481487274, Validation Loss: 0.11674932390451431, Learning Rate: 0.00064\n",
      "Epoch: 284, Training Loss: 0.11413013935089111, Validation Loss: 0.12117277830839157, Learning Rate: 0.00064\n",
      "Epoch: 285, Training Loss: 0.11460620909929276, Validation Loss: 0.11933238059282303, Learning Rate: 0.00064\n",
      "Epoch: 286, Training Loss: 0.11490898579359055, Validation Loss: 0.12291594594717026, Learning Rate: 0.00064\n",
      "Epoch: 287, Training Loss: 0.11419764161109924, Validation Loss: 0.11735078692436218, Learning Rate: 0.00064\n",
      "Epoch: 288, Training Loss: 0.11584113538265228, Validation Loss: 0.11535938084125519, Learning Rate: 0.00064\n",
      "Epoch: 289, Training Loss: 0.1146504208445549, Validation Loss: 0.11939377337694168, Learning Rate: 0.00064\n",
      "Epoch: 290, Training Loss: 0.11397057771682739, Validation Loss: 0.12343855947256088, Learning Rate: 0.00064\n",
      "Epoch: 291, Training Loss: 0.11531170457601547, Validation Loss: 0.12097315490245819, Learning Rate: 0.00064\n",
      "Epoch: 292, Training Loss: 0.11207030713558197, Validation Loss: 0.12017959356307983, Learning Rate: 0.00064\n",
      "Epoch: 293, Training Loss: 0.114805206656456, Validation Loss: 0.11596143990755081, Learning Rate: 0.00064\n",
      "Epoch: 294, Training Loss: 0.11289998888969421, Validation Loss: 0.12231865525245667, Learning Rate: 0.00064\n",
      "Epoch: 295, Training Loss: 0.11342785507440567, Validation Loss: 0.12104775011539459, Learning Rate: 0.00064\n",
      "Epoch: 296, Training Loss: 0.11323905736207962, Validation Loss: 0.11810608953237534, Learning Rate: 0.00064\n",
      "Epoch: 297, Training Loss: 0.11280693113803864, Validation Loss: 0.12031147629022598, Learning Rate: 0.00064\n",
      "Epoch: 298, Training Loss: 0.11292790621519089, Validation Loss: 0.12066461145877838, Learning Rate: 0.00064\n",
      "Epoch: 299, Training Loss: 0.11508138477802277, Validation Loss: 0.11869101226329803, Learning Rate: 0.00064\n",
      "Epoch: 300, Training Loss: 0.11239243298768997, Validation Loss: 0.12365026026964188, Learning Rate: 0.0005120000000000001\n",
      "Epoch: 301, Training Loss: 0.11610404402017593, Validation Loss: 0.11538735032081604, Learning Rate: 0.0005120000000000001\n",
      "Epoch: 302, Training Loss: 0.1121772825717926, Validation Loss: 0.12383279949426651, Learning Rate: 0.0005120000000000001\n",
      "Epoch: 303, Training Loss: 0.11145918071269989, Validation Loss: 0.12290889024734497, Learning Rate: 0.0005120000000000001\n",
      "Epoch: 304, Training Loss: 0.1147976890206337, Validation Loss: 0.11918231844902039, Learning Rate: 0.0005120000000000001\n",
      "Epoch: 305, Training Loss: 0.11209096014499664, Validation Loss: 0.1181887611746788, Learning Rate: 0.0005120000000000001\n",
      "Epoch: 306, Training Loss: 0.11443662643432617, Validation Loss: 0.11929847300052643, Learning Rate: 0.0005120000000000001\n",
      "Epoch: 307, Training Loss: 0.11471244692802429, Validation Loss: 0.11962904036045074, Learning Rate: 0.0005120000000000001\n",
      "Epoch: 308, Training Loss: 0.11326006054878235, Validation Loss: 0.11760745942592621, Learning Rate: 0.0005120000000000001\n",
      "Epoch: 309, Training Loss: 0.11155381798744202, Validation Loss: 0.1152476891875267, Learning Rate: 0.0005120000000000001\n",
      "Epoch: 310, Training Loss: 0.1123688742518425, Validation Loss: 0.11736372858285904, Learning Rate: 0.0005120000000000001\n",
      "Epoch: 311, Training Loss: 0.11352582275867462, Validation Loss: 0.11534348875284195, Learning Rate: 0.0005120000000000001\n",
      "Epoch: 312, Training Loss: 0.11424777656793594, Validation Loss: 0.11762362718582153, Learning Rate: 0.0005120000000000001\n",
      "Epoch: 313, Training Loss: 0.11355304718017578, Validation Loss: 0.11940494179725647, Learning Rate: 0.0005120000000000001\n",
      "Epoch: 314, Training Loss: 0.11470997333526611, Validation Loss: 0.11892776191234589, Learning Rate: 0.0005120000000000001\n",
      "Epoch: 315, Training Loss: 0.1131289154291153, Validation Loss: 0.11931725591421127, Learning Rate: 0.0005120000000000001\n",
      "Epoch: 316, Training Loss: 0.11283014714717865, Validation Loss: 0.11810636520385742, Learning Rate: 0.0005120000000000001\n",
      "Epoch: 317, Training Loss: 0.11175456643104553, Validation Loss: 0.11740697175264359, Learning Rate: 0.0005120000000000001\n",
      "Epoch: 318, Training Loss: 0.11237506568431854, Validation Loss: 0.11808553338050842, Learning Rate: 0.0005120000000000001\n",
      "Epoch: 319, Training Loss: 0.11154685914516449, Validation Loss: 0.11827347427606583, Learning Rate: 0.0005120000000000001\n",
      "Epoch: 320, Training Loss: 0.1132332980632782, Validation Loss: 0.11946308612823486, Learning Rate: 0.0005120000000000001\n",
      "Epoch: 321, Training Loss: 0.11232544481754303, Validation Loss: 0.11699371039867401, Learning Rate: 0.0005120000000000001\n",
      "Epoch: 322, Training Loss: 0.11293613165616989, Validation Loss: 0.11933666467666626, Learning Rate: 0.0005120000000000001\n",
      "Epoch: 323, Training Loss: 0.1133284643292427, Validation Loss: 0.11749112606048584, Learning Rate: 0.0005120000000000001\n",
      "Epoch: 324, Training Loss: 0.1134251058101654, Validation Loss: 0.11673102527856827, Learning Rate: 0.0005120000000000001\n",
      "Epoch: 325, Training Loss: 0.11289149522781372, Validation Loss: 0.11871210485696793, Learning Rate: 0.0005120000000000001\n",
      "Epoch: 326, Training Loss: 0.11210832744836807, Validation Loss: 0.11969050019979477, Learning Rate: 0.0005120000000000001\n",
      "Epoch: 327, Training Loss: 0.11246512830257416, Validation Loss: 0.11599989980459213, Learning Rate: 0.0005120000000000001\n",
      "Epoch: 328, Training Loss: 0.11424505710601807, Validation Loss: 0.11618370562791824, Learning Rate: 0.0005120000000000001\n",
      "Epoch: 329, Training Loss: 0.11234989017248154, Validation Loss: 0.12016703933477402, Learning Rate: 0.0005120000000000001\n",
      "Epoch: 330, Training Loss: 0.11360226571559906, Validation Loss: 0.11704077571630478, Learning Rate: 0.0005120000000000001\n",
      "Epoch: 331, Training Loss: 0.11430520564317703, Validation Loss: 0.11988941580057144, Learning Rate: 0.0005120000000000001\n",
      "Epoch: 332, Training Loss: 0.11269974708557129, Validation Loss: 0.1165197566151619, Learning Rate: 0.0005120000000000001\n",
      "Epoch: 333, Training Loss: 0.11238211393356323, Validation Loss: 0.11758735775947571, Learning Rate: 0.0005120000000000001\n",
      "Epoch: 334, Training Loss: 0.11209432035684586, Validation Loss: 0.11837589740753174, Learning Rate: 0.0005120000000000001\n",
      "Epoch: 335, Training Loss: 0.11353670060634613, Validation Loss: 0.11761000752449036, Learning Rate: 0.0005120000000000001\n",
      "Epoch: 336, Training Loss: 0.11398914456367493, Validation Loss: 0.11983545124530792, Learning Rate: 0.0005120000000000001\n",
      "Epoch: 337, Training Loss: 0.11149998754262924, Validation Loss: 0.11723138391971588, Learning Rate: 0.0005120000000000001\n",
      "Epoch: 338, Training Loss: 0.11232646554708481, Validation Loss: 0.12064101547002792, Learning Rate: 0.0005120000000000001\n",
      "Epoch: 339, Training Loss: 0.11255612224340439, Validation Loss: 0.11906015872955322, Learning Rate: 0.0005120000000000001\n",
      "Epoch: 340, Training Loss: 0.11335299909114838, Validation Loss: 0.11806955188512802, Learning Rate: 0.0005120000000000001\n",
      "Epoch: 341, Training Loss: 0.11327488720417023, Validation Loss: 0.12214748561382294, Learning Rate: 0.0005120000000000001\n",
      "Epoch: 342, Training Loss: 0.11214697360992432, Validation Loss: 0.11772115528583527, Learning Rate: 0.0005120000000000001\n",
      "Epoch: 343, Training Loss: 0.11433698982000351, Validation Loss: 0.11982158571481705, Learning Rate: 0.0005120000000000001\n",
      "Epoch: 344, Training Loss: 0.11189783364534378, Validation Loss: 0.11692261695861816, Learning Rate: 0.0005120000000000001\n",
      "Epoch: 345, Training Loss: 0.1123289167881012, Validation Loss: 0.11441932618618011, Learning Rate: 0.0005120000000000001\n",
      "Epoch: 346, Training Loss: 0.11268112808465958, Validation Loss: 0.11896142363548279, Learning Rate: 0.0005120000000000001\n",
      "Epoch: 347, Training Loss: 0.11258131265640259, Validation Loss: 0.11737475544214249, Learning Rate: 0.0005120000000000001\n",
      "Epoch: 348, Training Loss: 0.11193788796663284, Validation Loss: 0.11514284461736679, Learning Rate: 0.0005120000000000001\n",
      "Epoch: 349, Training Loss: 0.11211582273244858, Validation Loss: 0.12031358480453491, Learning Rate: 0.0005120000000000001\n",
      "Epoch: 350, Training Loss: 0.11344127357006073, Validation Loss: 0.11385022103786469, Learning Rate: 0.0005120000000000001\n",
      "Epoch: 351, Training Loss: 0.11306226253509521, Validation Loss: 0.11842136830091476, Learning Rate: 0.0005120000000000001\n",
      "Epoch: 352, Training Loss: 0.11351747810840607, Validation Loss: 0.12075792998075485, Learning Rate: 0.0005120000000000001\n",
      "Epoch: 353, Training Loss: 0.11268257349729538, Validation Loss: 0.11928888410329819, Learning Rate: 0.0005120000000000001\n",
      "Epoch: 354, Training Loss: 0.1120634600520134, Validation Loss: 0.11550278961658478, Learning Rate: 0.0005120000000000001\n",
      "Epoch: 355, Training Loss: 0.1111898347735405, Validation Loss: 0.11993002891540527, Learning Rate: 0.0005120000000000001\n",
      "Epoch: 356, Training Loss: 0.1109452173113823, Validation Loss: 0.11850964277982712, Learning Rate: 0.0005120000000000001\n",
      "Epoch: 357, Training Loss: 0.11212782561779022, Validation Loss: 0.11498993635177612, Learning Rate: 0.0005120000000000001\n",
      "Epoch: 358, Training Loss: 0.1124400720000267, Validation Loss: 0.11610700190067291, Learning Rate: 0.0005120000000000001\n",
      "Epoch: 359, Training Loss: 0.1121775433421135, Validation Loss: 0.11934653669595718, Learning Rate: 0.0005120000000000001\n",
      "Epoch: 360, Training Loss: 0.111908920109272, Validation Loss: 0.11716328561306, Learning Rate: 0.0005120000000000001\n",
      "Epoch: 361, Training Loss: 0.11160717904567719, Validation Loss: 0.11958740651607513, Learning Rate: 0.0005120000000000001\n",
      "Epoch: 362, Training Loss: 0.11209286004304886, Validation Loss: 0.11889433115720749, Learning Rate: 0.0005120000000000001\n",
      "Epoch: 363, Training Loss: 0.11233830451965332, Validation Loss: 0.12056951224803925, Learning Rate: 0.0005120000000000001\n",
      "Epoch: 364, Training Loss: 0.11155538260936737, Validation Loss: 0.11791712045669556, Learning Rate: 0.0005120000000000001\n",
      "Epoch: 365, Training Loss: 0.11187788844108582, Validation Loss: 0.11895281821489334, Learning Rate: 0.0005120000000000001\n",
      "Epoch: 366, Training Loss: 0.11447443813085556, Validation Loss: 0.1131274476647377, Learning Rate: 0.0005120000000000001\n",
      "Epoch: 367, Training Loss: 0.11322131752967834, Validation Loss: 0.1227368712425232, Learning Rate: 0.0005120000000000001\n",
      "Epoch: 368, Training Loss: 0.11155616492033005, Validation Loss: 0.11985444277524948, Learning Rate: 0.0005120000000000001\n",
      "Epoch: 369, Training Loss: 0.11196926981210709, Validation Loss: 0.11624652147293091, Learning Rate: 0.0005120000000000001\n",
      "Epoch: 370, Training Loss: 0.11367952823638916, Validation Loss: 0.12065448611974716, Learning Rate: 0.0005120000000000001\n",
      "Epoch: 371, Training Loss: 0.11189638078212738, Validation Loss: 0.11699610203504562, Learning Rate: 0.0005120000000000001\n",
      "Epoch: 372, Training Loss: 0.11082294583320618, Validation Loss: 0.11773661524057388, Learning Rate: 0.0005120000000000001\n",
      "Epoch: 373, Training Loss: 0.11224016547203064, Validation Loss: 0.12024044245481491, Learning Rate: 0.0005120000000000001\n",
      "Epoch: 374, Training Loss: 0.11441664397716522, Validation Loss: 0.11758250743150711, Learning Rate: 0.0005120000000000001\n",
      "Epoch: 375, Training Loss: 0.11065246909856796, Validation Loss: 0.11621151119470596, Learning Rate: 0.0005120000000000001\n",
      "Epoch: 376, Training Loss: 0.11314516514539719, Validation Loss: 0.1159493550658226, Learning Rate: 0.0005120000000000001\n",
      "Epoch: 377, Training Loss: 0.11359501630067825, Validation Loss: 0.11970390379428864, Learning Rate: 0.0005120000000000001\n",
      "Epoch: 378, Training Loss: 0.11206620931625366, Validation Loss: 0.11931180953979492, Learning Rate: 0.0005120000000000001\n",
      "Epoch: 379, Training Loss: 0.11179275810718536, Validation Loss: 0.11736776679754257, Learning Rate: 0.0005120000000000001\n",
      "Epoch: 380, Training Loss: 0.11233353614807129, Validation Loss: 0.11928162723779678, Learning Rate: 0.0005120000000000001\n",
      "Epoch: 381, Training Loss: 0.11140028387308121, Validation Loss: 0.11848762631416321, Learning Rate: 0.0005120000000000001\n",
      "Epoch: 382, Training Loss: 0.1118081733584404, Validation Loss: 0.11959241330623627, Learning Rate: 0.0005120000000000001\n",
      "Epoch: 383, Training Loss: 0.11067380756139755, Validation Loss: 0.11449640989303589, Learning Rate: 0.0005120000000000001\n",
      "Epoch: 384, Training Loss: 0.11297758668661118, Validation Loss: 0.11784464865922928, Learning Rate: 0.0005120000000000001\n",
      "Epoch: 385, Training Loss: 0.11177821457386017, Validation Loss: 0.11615440249443054, Learning Rate: 0.0005120000000000001\n",
      "Epoch: 386, Training Loss: 0.11146552115678787, Validation Loss: 0.1154676303267479, Learning Rate: 0.0005120000000000001\n",
      "Epoch: 387, Training Loss: 0.11117592453956604, Validation Loss: 0.11451847106218338, Learning Rate: 0.0005120000000000001\n",
      "Epoch: 388, Training Loss: 0.11093957722187042, Validation Loss: 0.11631784588098526, Learning Rate: 0.0005120000000000001\n",
      "Epoch: 389, Training Loss: 0.11150304228067398, Validation Loss: 0.11754535883665085, Learning Rate: 0.0005120000000000001\n",
      "Epoch: 390, Training Loss: 0.1115618571639061, Validation Loss: 0.12084250152111053, Learning Rate: 0.0005120000000000001\n",
      "Epoch: 391, Training Loss: 0.11185383796691895, Validation Loss: 0.11519795656204224, Learning Rate: 0.0005120000000000001\n",
      "Epoch: 392, Training Loss: 0.11083079129457474, Validation Loss: 0.11861442029476166, Learning Rate: 0.0005120000000000001\n",
      "Epoch: 393, Training Loss: 0.11124963313341141, Validation Loss: 0.11967343837022781, Learning Rate: 0.0005120000000000001\n",
      "Epoch: 394, Training Loss: 0.11269181966781616, Validation Loss: 0.11695785075426102, Learning Rate: 0.0005120000000000001\n",
      "Epoch: 395, Training Loss: 0.11216790974140167, Validation Loss: 0.1191861480474472, Learning Rate: 0.0005120000000000001\n",
      "Epoch: 396, Training Loss: 0.11202678829431534, Validation Loss: 0.11639130860567093, Learning Rate: 0.0005120000000000001\n",
      "Epoch: 397, Training Loss: 0.11261507123708725, Validation Loss: 0.1168607696890831, Learning Rate: 0.0005120000000000001\n",
      "Epoch: 398, Training Loss: 0.11002713441848755, Validation Loss: 0.11808253079652786, Learning Rate: 0.0005120000000000001\n",
      "Epoch: 399, Training Loss: 0.11103780567646027, Validation Loss: 0.11129193753004074, Learning Rate: 0.0005120000000000001\n",
      "Epoch: 400, Training Loss: 0.110045425593853, Validation Loss: 0.11591547727584839, Learning Rate: 0.0004096000000000001\n",
      "Epoch: 401, Training Loss: 0.10912439972162247, Validation Loss: 0.11613234877586365, Learning Rate: 0.0004096000000000001\n",
      "Epoch: 402, Training Loss: 0.1128130555152893, Validation Loss: 0.1196008175611496, Learning Rate: 0.0004096000000000001\n",
      "Epoch: 403, Training Loss: 0.11045967042446136, Validation Loss: 0.11737606674432755, Learning Rate: 0.0004096000000000001\n",
      "Epoch: 404, Training Loss: 0.11208818852901459, Validation Loss: 0.11703066527843475, Learning Rate: 0.0004096000000000001\n",
      "Epoch: 405, Training Loss: 0.11231578141450882, Validation Loss: 0.11599496006965637, Learning Rate: 0.0004096000000000001\n",
      "Epoch: 406, Training Loss: 0.10854354500770569, Validation Loss: 0.1160968691110611, Learning Rate: 0.0004096000000000001\n",
      "Epoch: 407, Training Loss: 0.11043266206979752, Validation Loss: 0.11592695862054825, Learning Rate: 0.0004096000000000001\n",
      "Epoch: 408, Training Loss: 0.11180390417575836, Validation Loss: 0.11361084133386612, Learning Rate: 0.0004096000000000001\n",
      "Epoch: 409, Training Loss: 0.10972347855567932, Validation Loss: 0.11552584171295166, Learning Rate: 0.0004096000000000001\n",
      "Epoch: 410, Training Loss: 0.11127234995365143, Validation Loss: 0.12006434798240662, Learning Rate: 0.0004096000000000001\n",
      "Epoch: 411, Training Loss: 0.11150853335857391, Validation Loss: 0.11898854374885559, Learning Rate: 0.0004096000000000001\n",
      "Epoch: 412, Training Loss: 0.1110253781080246, Validation Loss: 0.12270649522542953, Learning Rate: 0.0004096000000000001\n",
      "Epoch: 413, Training Loss: 0.11000929772853851, Validation Loss: 0.11645793169736862, Learning Rate: 0.0004096000000000001\n",
      "Epoch: 414, Training Loss: 0.10987530648708344, Validation Loss: 0.1153353676199913, Learning Rate: 0.0004096000000000001\n",
      "Epoch: 415, Training Loss: 0.11151184141635895, Validation Loss: 0.11693539470434189, Learning Rate: 0.0004096000000000001\n",
      "Epoch: 416, Training Loss: 0.11159417033195496, Validation Loss: 0.11628562957048416, Learning Rate: 0.0004096000000000001\n",
      "Epoch: 417, Training Loss: 0.11143706738948822, Validation Loss: 0.11688151210546494, Learning Rate: 0.0004096000000000001\n",
      "Epoch: 418, Training Loss: 0.1102404072880745, Validation Loss: 0.11815158277750015, Learning Rate: 0.0004096000000000001\n",
      "Epoch: 419, Training Loss: 0.11136910319328308, Validation Loss: 0.11499794572591782, Learning Rate: 0.0004096000000000001\n",
      "Epoch: 420, Training Loss: 0.10958180576562881, Validation Loss: 0.11592989414930344, Learning Rate: 0.0004096000000000001\n",
      "Epoch: 421, Training Loss: 0.11046433448791504, Validation Loss: 0.12303439527750015, Learning Rate: 0.0004096000000000001\n",
      "Epoch: 422, Training Loss: 0.11036275327205658, Validation Loss: 0.11271987110376358, Learning Rate: 0.0004096000000000001\n",
      "Epoch: 423, Training Loss: 0.10908611863851547, Validation Loss: 0.11564263701438904, Learning Rate: 0.0004096000000000001\n",
      "Epoch: 424, Training Loss: 0.10946932435035706, Validation Loss: 0.11666644364595413, Learning Rate: 0.0004096000000000001\n",
      "Epoch: 425, Training Loss: 0.11191937327384949, Validation Loss: 0.11582291126251221, Learning Rate: 0.0004096000000000001\n",
      "Epoch: 426, Training Loss: 0.11075222492218018, Validation Loss: 0.11414408683776855, Learning Rate: 0.0004096000000000001\n",
      "Epoch: 427, Training Loss: 0.11054608225822449, Validation Loss: 0.11417195945978165, Learning Rate: 0.0004096000000000001\n",
      "Epoch: 428, Training Loss: 0.1094198152422905, Validation Loss: 0.11570509523153305, Learning Rate: 0.0004096000000000001\n",
      "Epoch: 429, Training Loss: 0.10938242822885513, Validation Loss: 0.11861049383878708, Learning Rate: 0.0004096000000000001\n",
      "Epoch: 430, Training Loss: 0.10969176888465881, Validation Loss: 0.11726325750350952, Learning Rate: 0.0004096000000000001\n",
      "Epoch: 431, Training Loss: 0.10954365134239197, Validation Loss: 0.11903586238622665, Learning Rate: 0.0004096000000000001\n",
      "Epoch: 432, Training Loss: 0.11308171600103378, Validation Loss: 0.11318916827440262, Learning Rate: 0.0004096000000000001\n",
      "Epoch: 433, Training Loss: 0.11122702062129974, Validation Loss: 0.11426353454589844, Learning Rate: 0.0004096000000000001\n",
      "Epoch: 434, Training Loss: 0.1095413789153099, Validation Loss: 0.11431904137134552, Learning Rate: 0.0004096000000000001\n",
      "Epoch: 435, Training Loss: 0.11018747836351395, Validation Loss: 0.11678976565599442, Learning Rate: 0.0004096000000000001\n",
      "Epoch: 436, Training Loss: 0.11035115271806717, Validation Loss: 0.11305161565542221, Learning Rate: 0.0004096000000000001\n",
      "Epoch: 437, Training Loss: 0.11030992865562439, Validation Loss: 0.11757729202508926, Learning Rate: 0.0004096000000000001\n",
      "Epoch: 438, Training Loss: 0.1100497767329216, Validation Loss: 0.11407498270273209, Learning Rate: 0.0004096000000000001\n",
      "Epoch: 439, Training Loss: 0.10980121791362762, Validation Loss: 0.1140201985836029, Learning Rate: 0.0004096000000000001\n",
      "Epoch: 440, Training Loss: 0.11170663684606552, Validation Loss: 0.11591465026140213, Learning Rate: 0.0004096000000000001\n",
      "Epoch: 441, Training Loss: 0.1107342392206192, Validation Loss: 0.1148327887058258, Learning Rate: 0.0004096000000000001\n",
      "Epoch: 442, Training Loss: 0.1093965545296669, Validation Loss: 0.11561562120914459, Learning Rate: 0.0004096000000000001\n",
      "Epoch: 443, Training Loss: 0.10911533981561661, Validation Loss: 0.11798224598169327, Learning Rate: 0.0004096000000000001\n",
      "Epoch: 444, Training Loss: 0.11025059223175049, Validation Loss: 0.11347850412130356, Learning Rate: 0.0004096000000000001\n",
      "Epoch: 445, Training Loss: 0.10975085943937302, Validation Loss: 0.11395016312599182, Learning Rate: 0.0004096000000000001\n",
      "Epoch: 446, Training Loss: 0.10830602049827576, Validation Loss: 0.11674089729785919, Learning Rate: 0.0004096000000000001\n",
      "Epoch: 447, Training Loss: 0.11142487823963165, Validation Loss: 0.11832565069198608, Learning Rate: 0.0004096000000000001\n",
      "Epoch: 448, Training Loss: 0.1104544922709465, Validation Loss: 0.12034053355455399, Learning Rate: 0.0004096000000000001\n",
      "Epoch: 449, Training Loss: 0.10955280065536499, Validation Loss: 0.11443335562944412, Learning Rate: 0.0004096000000000001\n",
      "Epoch: 450, Training Loss: 0.1111297532916069, Validation Loss: 0.1150873526930809, Learning Rate: 0.0004096000000000001\n",
      "Epoch: 451, Training Loss: 0.11062829196453094, Validation Loss: 0.1170300841331482, Learning Rate: 0.0004096000000000001\n",
      "Epoch: 452, Training Loss: 0.11151659488677979, Validation Loss: 0.11644691228866577, Learning Rate: 0.0004096000000000001\n",
      "Epoch: 453, Training Loss: 0.10932409018278122, Validation Loss: 0.11373759061098099, Learning Rate: 0.0004096000000000001\n",
      "Epoch: 454, Training Loss: 0.11052228510379791, Validation Loss: 0.11635158210992813, Learning Rate: 0.0004096000000000001\n",
      "Epoch: 455, Training Loss: 0.10872616618871689, Validation Loss: 0.11645279824733734, Learning Rate: 0.0004096000000000001\n",
      "Epoch: 456, Training Loss: 0.11164707690477371, Validation Loss: 0.11861859261989594, Learning Rate: 0.0004096000000000001\n",
      "Epoch: 457, Training Loss: 0.11015345901250839, Validation Loss: 0.11694847047328949, Learning Rate: 0.0004096000000000001\n",
      "Epoch: 458, Training Loss: 0.10905587673187256, Validation Loss: 0.11385416984558105, Learning Rate: 0.0004096000000000001\n",
      "Epoch: 459, Training Loss: 0.11044706404209137, Validation Loss: 0.11533202230930328, Learning Rate: 0.0004096000000000001\n",
      "Epoch: 460, Training Loss: 0.11041850596666336, Validation Loss: 0.11182386428117752, Learning Rate: 0.0004096000000000001\n",
      "Epoch: 461, Training Loss: 0.11064199358224869, Validation Loss: 0.11281782388687134, Learning Rate: 0.0004096000000000001\n",
      "Epoch: 462, Training Loss: 0.11024525761604309, Validation Loss: 0.11246577650308609, Learning Rate: 0.0004096000000000001\n",
      "Epoch: 463, Training Loss: 0.11108153313398361, Validation Loss: 0.11514750868082047, Learning Rate: 0.0004096000000000001\n",
      "Epoch: 464, Training Loss: 0.11031588912010193, Validation Loss: 0.11409226059913635, Learning Rate: 0.0004096000000000001\n",
      "Epoch: 465, Training Loss: 0.10978405177593231, Validation Loss: 0.11736541241407394, Learning Rate: 0.0004096000000000001\n",
      "Epoch: 466, Training Loss: 0.1115211620926857, Validation Loss: 0.11468765884637833, Learning Rate: 0.0004096000000000001\n",
      "Epoch: 467, Training Loss: 0.1114593967795372, Validation Loss: 0.11637597531080246, Learning Rate: 0.0004096000000000001\n",
      "Epoch: 468, Training Loss: 0.10969328135251999, Validation Loss: 0.11476226150989532, Learning Rate: 0.0004096000000000001\n",
      "Epoch: 469, Training Loss: 0.10943681001663208, Validation Loss: 0.11711728572845459, Learning Rate: 0.0004096000000000001\n",
      "Epoch: 470, Training Loss: 0.10915844142436981, Validation Loss: 0.11651351302862167, Learning Rate: 0.0004096000000000001\n",
      "Epoch: 471, Training Loss: 0.11116748303174973, Validation Loss: 0.11495953798294067, Learning Rate: 0.0004096000000000001\n",
      "Epoch: 472, Training Loss: 0.11039336770772934, Validation Loss: 0.11501214653253555, Learning Rate: 0.0004096000000000001\n",
      "Epoch: 473, Training Loss: 0.11152178794145584, Validation Loss: 0.11268392205238342, Learning Rate: 0.0004096000000000001\n",
      "Epoch: 474, Training Loss: 0.10949345678091049, Validation Loss: 0.11517534404993057, Learning Rate: 0.0004096000000000001\n",
      "Epoch: 475, Training Loss: 0.10963252186775208, Validation Loss: 0.11498686671257019, Learning Rate: 0.0004096000000000001\n",
      "Epoch: 476, Training Loss: 0.11027514934539795, Validation Loss: 0.11476311832666397, Learning Rate: 0.0004096000000000001\n",
      "Epoch: 477, Training Loss: 0.11083433777093887, Validation Loss: 0.11472663283348083, Learning Rate: 0.0004096000000000001\n",
      "Epoch: 478, Training Loss: 0.11073607206344604, Validation Loss: 0.11472082138061523, Learning Rate: 0.0004096000000000001\n",
      "Epoch: 479, Training Loss: 0.10882815718650818, Validation Loss: 0.11869705468416214, Learning Rate: 0.0004096000000000001\n",
      "Epoch: 480, Training Loss: 0.1106736809015274, Validation Loss: 0.11517010629177094, Learning Rate: 0.0004096000000000001\n",
      "Epoch: 481, Training Loss: 0.11021587252616882, Validation Loss: 0.11280515789985657, Learning Rate: 0.0004096000000000001\n",
      "Epoch: 482, Training Loss: 0.10948001593351364, Validation Loss: 0.11626207083463669, Learning Rate: 0.0004096000000000001\n",
      "Epoch: 483, Training Loss: 0.10817790031433105, Validation Loss: 0.11185213923454285, Learning Rate: 0.0004096000000000001\n",
      "Epoch: 484, Training Loss: 0.11069930344820023, Validation Loss: 0.11477430909872055, Learning Rate: 0.0004096000000000001\n",
      "Epoch: 485, Training Loss: 0.11074783653020859, Validation Loss: 0.11480981856584549, Learning Rate: 0.0004096000000000001\n",
      "Epoch: 486, Training Loss: 0.10881511867046356, Validation Loss: 0.11220729351043701, Learning Rate: 0.0004096000000000001\n",
      "Epoch: 487, Training Loss: 0.109340600669384, Validation Loss: 0.11278028786182404, Learning Rate: 0.0004096000000000001\n",
      "Epoch: 488, Training Loss: 0.11051493883132935, Validation Loss: 0.11560875922441483, Learning Rate: 0.0004096000000000001\n",
      "Epoch: 489, Training Loss: 0.1099010780453682, Validation Loss: 0.11769923567771912, Learning Rate: 0.0004096000000000001\n",
      "Epoch: 490, Training Loss: 0.1091233491897583, Validation Loss: 0.11825516819953918, Learning Rate: 0.0004096000000000001\n",
      "Epoch: 491, Training Loss: 0.1097746416926384, Validation Loss: 0.11173960566520691, Learning Rate: 0.0004096000000000001\n",
      "Epoch: 492, Training Loss: 0.11013121157884598, Validation Loss: 0.11843135207891464, Learning Rate: 0.0004096000000000001\n",
      "Epoch: 493, Training Loss: 0.10991627722978592, Validation Loss: 0.11666574329137802, Learning Rate: 0.0004096000000000001\n",
      "Epoch: 494, Training Loss: 0.1117013618350029, Validation Loss: 0.11611761897802353, Learning Rate: 0.0004096000000000001\n",
      "Epoch: 495, Training Loss: 0.11015821993350983, Validation Loss: 0.11662684381008148, Learning Rate: 0.0004096000000000001\n",
      "Epoch: 496, Training Loss: 0.11050326377153397, Validation Loss: 0.11389569938182831, Learning Rate: 0.0004096000000000001\n",
      "Epoch: 497, Training Loss: 0.11044442653656006, Validation Loss: 0.11394395679235458, Learning Rate: 0.0004096000000000001\n",
      "Epoch: 498, Training Loss: 0.1088971197605133, Validation Loss: 0.11198537051677704, Learning Rate: 0.0004096000000000001\n",
      "Epoch: 499, Training Loss: 0.11069930344820023, Validation Loss: 0.11566014587879181, Learning Rate: 0.0004096000000000001\n",
      "Epoch: 500, Training Loss: 0.11105003207921982, Validation Loss: 0.1121830940246582, Learning Rate: 0.0003276800000000001\n",
      "Epoch: 501, Training Loss: 0.10973399132490158, Validation Loss: 0.11668367683887482, Learning Rate: 0.0003276800000000001\n",
      "Epoch: 502, Training Loss: 0.11049878597259521, Validation Loss: 0.11540085822343826, Learning Rate: 0.0003276800000000001\n",
      "Epoch: 503, Training Loss: 0.10860837250947952, Validation Loss: 0.11630664020776749, Learning Rate: 0.0003276800000000001\n",
      "Epoch: 504, Training Loss: 0.10915852338075638, Validation Loss: 0.11320740729570389, Learning Rate: 0.0003276800000000001\n",
      "Epoch: 505, Training Loss: 0.10979471355676651, Validation Loss: 0.11260966211557388, Learning Rate: 0.0003276800000000001\n",
      "Epoch: 506, Training Loss: 0.10929027199745178, Validation Loss: 0.11460433900356293, Learning Rate: 0.0003276800000000001\n",
      "Epoch: 507, Training Loss: 0.10926036536693573, Validation Loss: 0.11726304143667221, Learning Rate: 0.0003276800000000001\n",
      "Epoch: 508, Training Loss: 0.10901325941085815, Validation Loss: 0.11307521909475327, Learning Rate: 0.0003276800000000001\n",
      "Epoch: 509, Training Loss: 0.11086462438106537, Validation Loss: 0.11485280841588974, Learning Rate: 0.0003276800000000001\n",
      "Epoch: 510, Training Loss: 0.10870073735713959, Validation Loss: 0.11537110060453415, Learning Rate: 0.0003276800000000001\n",
      "Epoch: 511, Training Loss: 0.108546681702137, Validation Loss: 0.11741752922534943, Learning Rate: 0.0003276800000000001\n",
      "Epoch: 512, Training Loss: 0.1094648614525795, Validation Loss: 0.11422879248857498, Learning Rate: 0.0003276800000000001\n",
      "Epoch: 513, Training Loss: 0.10981526225805283, Validation Loss: 0.11904343217611313, Learning Rate: 0.0003276800000000001\n",
      "Epoch: 514, Training Loss: 0.109309621155262, Validation Loss: 0.11657560616731644, Learning Rate: 0.0003276800000000001\n",
      "Epoch: 515, Training Loss: 0.10829828679561615, Validation Loss: 0.11225630342960358, Learning Rate: 0.0003276800000000001\n",
      "Epoch: 516, Training Loss: 0.11085527390241623, Validation Loss: 0.11558544635772705, Learning Rate: 0.0003276800000000001\n",
      "Epoch: 517, Training Loss: 0.10943040996789932, Validation Loss: 0.11676043272018433, Learning Rate: 0.0003276800000000001\n",
      "Epoch: 518, Training Loss: 0.10961469262838364, Validation Loss: 0.11630930006504059, Learning Rate: 0.0003276800000000001\n",
      "Epoch: 519, Training Loss: 0.11056359857320786, Validation Loss: 0.11235344409942627, Learning Rate: 0.0003276800000000001\n",
      "Epoch: 520, Training Loss: 0.10737229138612747, Validation Loss: 0.11013782024383545, Learning Rate: 0.0003276800000000001\n",
      "Epoch: 521, Training Loss: 0.11057263612747192, Validation Loss: 0.11692889034748077, Learning Rate: 0.0003276800000000001\n",
      "Epoch: 522, Training Loss: 0.10975939780473709, Validation Loss: 0.11549269407987595, Learning Rate: 0.0003276800000000001\n",
      "Epoch: 523, Training Loss: 0.10763207077980042, Validation Loss: 0.1122523620724678, Learning Rate: 0.0003276800000000001\n",
      "Epoch: 524, Training Loss: 0.10799243301153183, Validation Loss: 0.11424762010574341, Learning Rate: 0.0003276800000000001\n",
      "Epoch: 525, Training Loss: 0.10888037830591202, Validation Loss: 0.11754832416772842, Learning Rate: 0.0003276800000000001\n",
      "Epoch: 526, Training Loss: 0.11080323904752731, Validation Loss: 0.115296371281147, Learning Rate: 0.0003276800000000001\n",
      "Epoch: 527, Training Loss: 0.10970266163349152, Validation Loss: 0.11622259765863419, Learning Rate: 0.0003276800000000001\n",
      "Epoch: 528, Training Loss: 0.10907469689846039, Validation Loss: 0.11231189966201782, Learning Rate: 0.0003276800000000001\n",
      "Epoch: 529, Training Loss: 0.11085911095142365, Validation Loss: 0.1140037328004837, Learning Rate: 0.0003276800000000001\n",
      "Epoch: 530, Training Loss: 0.10917850583791733, Validation Loss: 0.11162662506103516, Learning Rate: 0.0003276800000000001\n",
      "Epoch: 531, Training Loss: 0.10964587330818176, Validation Loss: 0.11400601267814636, Learning Rate: 0.0003276800000000001\n",
      "Epoch: 532, Training Loss: 0.109747014939785, Validation Loss: 0.11331169307231903, Learning Rate: 0.0003276800000000001\n",
      "Epoch: 533, Training Loss: 0.10944630950689316, Validation Loss: 0.1158335730433464, Learning Rate: 0.0003276800000000001\n",
      "Epoch: 534, Training Loss: 0.10795322060585022, Validation Loss: 0.1121758297085762, Learning Rate: 0.0003276800000000001\n",
      "Epoch: 535, Training Loss: 0.11097278445959091, Validation Loss: 0.11395645141601562, Learning Rate: 0.0003276800000000001\n",
      "Epoch: 536, Training Loss: 0.11051660031080246, Validation Loss: 0.11395473778247833, Learning Rate: 0.0003276800000000001\n",
      "Epoch: 537, Training Loss: 0.10776383429765701, Validation Loss: 0.11460117250680923, Learning Rate: 0.0003276800000000001\n",
      "Epoch: 538, Training Loss: 0.11066769808530807, Validation Loss: 0.11231482774019241, Learning Rate: 0.0003276800000000001\n",
      "Epoch: 539, Training Loss: 0.10859531909227371, Validation Loss: 0.11050665378570557, Learning Rate: 0.0003276800000000001\n",
      "Epoch: 540, Training Loss: 0.10970162600278854, Validation Loss: 0.10995893180370331, Learning Rate: 0.0003276800000000001\n",
      "Epoch: 541, Training Loss: 0.10866265743970871, Validation Loss: 0.11361611634492874, Learning Rate: 0.0003276800000000001\n",
      "Epoch: 542, Training Loss: 0.10940466076135635, Validation Loss: 0.11229822784662247, Learning Rate: 0.0003276800000000001\n",
      "Epoch: 543, Training Loss: 0.10845950245857239, Validation Loss: 0.11244945973157883, Learning Rate: 0.0003276800000000001\n",
      "Epoch: 544, Training Loss: 0.10871642827987671, Validation Loss: 0.11203677952289581, Learning Rate: 0.0003276800000000001\n",
      "Epoch: 545, Training Loss: 0.10865016281604767, Validation Loss: 0.1151730939745903, Learning Rate: 0.0003276800000000001\n",
      "Epoch: 546, Training Loss: 0.10952004045248032, Validation Loss: 0.11519584059715271, Learning Rate: 0.0003276800000000001\n",
      "Epoch: 547, Training Loss: 0.10854985564947128, Validation Loss: 0.111094631254673, Learning Rate: 0.0003276800000000001\n",
      "Epoch: 548, Training Loss: 0.10835524648427963, Validation Loss: 0.11434412747621536, Learning Rate: 0.0003276800000000001\n",
      "Epoch: 549, Training Loss: 0.1073826402425766, Validation Loss: 0.11633922904729843, Learning Rate: 0.0003276800000000001\n",
      "Epoch: 550, Training Loss: 0.10834000259637833, Validation Loss: 0.11423013359308243, Learning Rate: 0.0003276800000000001\n",
      "Epoch: 551, Training Loss: 0.10934629291296005, Validation Loss: 0.11537707597017288, Learning Rate: 0.0003276800000000001\n",
      "Epoch: 552, Training Loss: 0.10963531583547592, Validation Loss: 0.1147090271115303, Learning Rate: 0.0003276800000000001\n",
      "Epoch: 553, Training Loss: 0.11121866852045059, Validation Loss: 0.1151328757405281, Learning Rate: 0.0003276800000000001\n",
      "Epoch: 554, Training Loss: 0.10741646587848663, Validation Loss: 0.11671969294548035, Learning Rate: 0.0003276800000000001\n",
      "Epoch: 555, Training Loss: 0.10808815062046051, Validation Loss: 0.11345434188842773, Learning Rate: 0.0003276800000000001\n",
      "Epoch: 556, Training Loss: 0.10695655643939972, Validation Loss: 0.11103732883930206, Learning Rate: 0.0003276800000000001\n",
      "Epoch: 557, Training Loss: 0.10815628618001938, Validation Loss: 0.11201309412717819, Learning Rate: 0.0003276800000000001\n",
      "Epoch: 558, Training Loss: 0.10660363733768463, Validation Loss: 0.11534333974123001, Learning Rate: 0.0003276800000000001\n",
      "Epoch: 559, Training Loss: 0.10708580166101456, Validation Loss: 0.11203121393918991, Learning Rate: 0.0003276800000000001\n",
      "Epoch: 560, Training Loss: 0.1101338192820549, Validation Loss: 0.11130254715681076, Learning Rate: 0.0003276800000000001\n",
      "Epoch: 561, Training Loss: 0.11045784503221512, Validation Loss: 0.10988166928291321, Learning Rate: 0.0003276800000000001\n",
      "Epoch: 562, Training Loss: 0.1082519069314003, Validation Loss: 0.1174762099981308, Learning Rate: 0.0003276800000000001\n",
      "Epoch: 563, Training Loss: 0.10932359099388123, Validation Loss: 0.11392614990472794, Learning Rate: 0.0003276800000000001\n",
      "Epoch: 564, Training Loss: 0.1097780168056488, Validation Loss: 0.11258774995803833, Learning Rate: 0.0003276800000000001\n",
      "Epoch: 565, Training Loss: 0.10858811438083649, Validation Loss: 0.11222183704376221, Learning Rate: 0.0003276800000000001\n",
      "Epoch: 566, Training Loss: 0.10887154191732407, Validation Loss: 0.11460430920124054, Learning Rate: 0.0003276800000000001\n",
      "Epoch: 567, Training Loss: 0.10800867527723312, Validation Loss: 0.11409077048301697, Learning Rate: 0.0003276800000000001\n",
      "Epoch: 568, Training Loss: 0.10979598760604858, Validation Loss: 0.11733075231313705, Learning Rate: 0.0003276800000000001\n",
      "Epoch: 569, Training Loss: 0.10924314707517624, Validation Loss: 0.11365533620119095, Learning Rate: 0.0003276800000000001\n",
      "Epoch: 570, Training Loss: 0.10871244966983795, Validation Loss: 0.11302801966667175, Learning Rate: 0.0003276800000000001\n",
      "Epoch: 571, Training Loss: 0.10767798870801926, Validation Loss: 0.111121267080307, Learning Rate: 0.0003276800000000001\n",
      "Epoch: 572, Training Loss: 0.10933104902505875, Validation Loss: 0.11258486658334732, Learning Rate: 0.0003276800000000001\n",
      "Epoch: 573, Training Loss: 0.10929416120052338, Validation Loss: 0.11453692615032196, Learning Rate: 0.0003276800000000001\n",
      "Epoch: 574, Training Loss: 0.10897167772054672, Validation Loss: 0.11280271410942078, Learning Rate: 0.0003276800000000001\n",
      "Epoch: 575, Training Loss: 0.1088678166270256, Validation Loss: 0.11263491213321686, Learning Rate: 0.0003276800000000001\n",
      "Epoch: 576, Training Loss: 0.11015796661376953, Validation Loss: 0.116207554936409, Learning Rate: 0.0003276800000000001\n",
      "Epoch: 577, Training Loss: 0.10853838920593262, Validation Loss: 0.11454563587903976, Learning Rate: 0.0003276800000000001\n",
      "Epoch: 578, Training Loss: 0.11054348945617676, Validation Loss: 0.11549300700426102, Learning Rate: 0.0003276800000000001\n",
      "Epoch: 579, Training Loss: 0.10735020786523819, Validation Loss: 0.11751800030469894, Learning Rate: 0.0003276800000000001\n",
      "Epoch: 580, Training Loss: 0.10761016607284546, Validation Loss: 0.11163154989480972, Learning Rate: 0.0003276800000000001\n",
      "Epoch: 581, Training Loss: 0.1078140139579773, Validation Loss: 0.11177824437618256, Learning Rate: 0.0003276800000000001\n",
      "Epoch: 582, Training Loss: 0.10908934473991394, Validation Loss: 0.11358693242073059, Learning Rate: 0.0003276800000000001\n",
      "Epoch: 583, Training Loss: 0.1061740443110466, Validation Loss: 0.11140499264001846, Learning Rate: 0.0003276800000000001\n",
      "Epoch: 584, Training Loss: 0.10853852331638336, Validation Loss: 0.11643809825181961, Learning Rate: 0.0003276800000000001\n",
      "Epoch: 585, Training Loss: 0.10906936228275299, Validation Loss: 0.1155373826622963, Learning Rate: 0.0003276800000000001\n",
      "Epoch: 586, Training Loss: 0.10769011080265045, Validation Loss: 0.11430856585502625, Learning Rate: 0.0003276800000000001\n",
      "Epoch: 587, Training Loss: 0.10783159732818604, Validation Loss: 0.11541545391082764, Learning Rate: 0.0003276800000000001\n",
      "Epoch: 588, Training Loss: 0.10914425551891327, Validation Loss: 0.11161629110574722, Learning Rate: 0.0003276800000000001\n",
      "Epoch: 589, Training Loss: 0.10867488384246826, Validation Loss: 0.1147974357008934, Learning Rate: 0.0003276800000000001\n",
      "Epoch: 590, Training Loss: 0.10939101874828339, Validation Loss: 0.11382433772087097, Learning Rate: 0.0003276800000000001\n",
      "Epoch: 591, Training Loss: 0.10803725570440292, Validation Loss: 0.11583007127046585, Learning Rate: 0.0003276800000000001\n",
      "Epoch: 592, Training Loss: 0.10882797837257385, Validation Loss: 0.11386644840240479, Learning Rate: 0.0003276800000000001\n",
      "Epoch: 593, Training Loss: 0.10787317156791687, Validation Loss: 0.11589281260967255, Learning Rate: 0.0003276800000000001\n",
      "Epoch: 594, Training Loss: 0.10891573876142502, Validation Loss: 0.11371998488903046, Learning Rate: 0.0003276800000000001\n",
      "Epoch: 595, Training Loss: 0.10818041861057281, Validation Loss: 0.11356771737337112, Learning Rate: 0.0003276800000000001\n",
      "Epoch: 596, Training Loss: 0.10980866849422455, Validation Loss: 0.11385386437177658, Learning Rate: 0.0003276800000000001\n",
      "Epoch: 597, Training Loss: 0.10978822410106659, Validation Loss: 0.11415176838636398, Learning Rate: 0.0003276800000000001\n",
      "Epoch: 598, Training Loss: 0.10858818888664246, Validation Loss: 0.11160753667354584, Learning Rate: 0.0003276800000000001\n",
      "Epoch: 599, Training Loss: 0.10949733108282089, Validation Loss: 0.11181353032588959, Learning Rate: 0.0003276800000000001\n",
      "Epoch: 600, Training Loss: 0.10751265287399292, Validation Loss: 0.10892101377248764, Learning Rate: 0.0002621440000000001\n",
      "Epoch: 601, Training Loss: 0.10733386129140854, Validation Loss: 0.11118879169225693, Learning Rate: 0.0002621440000000001\n",
      "Epoch: 602, Training Loss: 0.10931649804115295, Validation Loss: 0.11379263550043106, Learning Rate: 0.0002621440000000001\n",
      "Epoch: 603, Training Loss: 0.10753147304058075, Validation Loss: 0.11478947103023529, Learning Rate: 0.0002621440000000001\n",
      "Epoch: 604, Training Loss: 0.1082630604505539, Validation Loss: 0.11024583131074905, Learning Rate: 0.0002621440000000001\n",
      "Epoch: 605, Training Loss: 0.10796709358692169, Validation Loss: 0.11301200091838837, Learning Rate: 0.0002621440000000001\n",
      "Epoch: 606, Training Loss: 0.10780753195285797, Validation Loss: 0.11313655972480774, Learning Rate: 0.0002621440000000001\n",
      "Epoch: 607, Training Loss: 0.10774347931146622, Validation Loss: 0.11560788005590439, Learning Rate: 0.0002621440000000001\n",
      "Epoch: 608, Training Loss: 0.10811764746904373, Validation Loss: 0.11311348527669907, Learning Rate: 0.0002621440000000001\n",
      "Epoch: 609, Training Loss: 0.1105208769440651, Validation Loss: 0.11649933457374573, Learning Rate: 0.0002621440000000001\n",
      "Epoch: 610, Training Loss: 0.10797327011823654, Validation Loss: 0.11625529080629349, Learning Rate: 0.0002621440000000001\n",
      "Epoch: 611, Training Loss: 0.10737594962120056, Validation Loss: 0.11008912324905396, Learning Rate: 0.0002621440000000001\n",
      "Epoch: 612, Training Loss: 0.10776448249816895, Validation Loss: 0.11528399586677551, Learning Rate: 0.0002621440000000001\n",
      "Epoch: 613, Training Loss: 0.10771020501852036, Validation Loss: 0.11436527967453003, Learning Rate: 0.0002621440000000001\n",
      "Epoch: 614, Training Loss: 0.10769274830818176, Validation Loss: 0.11546069383621216, Learning Rate: 0.0002621440000000001\n",
      "Epoch: 615, Training Loss: 0.1075846403837204, Validation Loss: 0.1146894320845604, Learning Rate: 0.0002621440000000001\n",
      "Epoch: 616, Training Loss: 0.10801438242197037, Validation Loss: 0.1164894625544548, Learning Rate: 0.0002621440000000001\n",
      "Epoch: 617, Training Loss: 0.10997603088617325, Validation Loss: 0.11529407650232315, Learning Rate: 0.0002621440000000001\n",
      "Epoch: 618, Training Loss: 0.10737492889165878, Validation Loss: 0.11254077404737473, Learning Rate: 0.0002621440000000001\n",
      "Epoch: 619, Training Loss: 0.10869179666042328, Validation Loss: 0.11584141105413437, Learning Rate: 0.0002621440000000001\n",
      "Epoch: 620, Training Loss: 0.10740797221660614, Validation Loss: 0.1148904487490654, Learning Rate: 0.0002621440000000001\n",
      "Epoch: 621, Training Loss: 0.10765394568443298, Validation Loss: 0.11215051263570786, Learning Rate: 0.0002621440000000001\n",
      "Epoch: 622, Training Loss: 0.10819655656814575, Validation Loss: 0.11332601308822632, Learning Rate: 0.0002621440000000001\n",
      "Epoch: 623, Training Loss: 0.10920674353837967, Validation Loss: 0.11294043809175491, Learning Rate: 0.0002621440000000001\n",
      "Epoch: 624, Training Loss: 0.10894904285669327, Validation Loss: 0.11337368190288544, Learning Rate: 0.0002621440000000001\n",
      "Epoch: 625, Training Loss: 0.10830878466367722, Validation Loss: 0.11194781213998795, Learning Rate: 0.0002621440000000001\n",
      "Epoch: 626, Training Loss: 0.10687144100666046, Validation Loss: 0.10921039432287216, Learning Rate: 0.0002621440000000001\n",
      "Epoch: 627, Training Loss: 0.10878066718578339, Validation Loss: 0.11392275989055634, Learning Rate: 0.0002621440000000001\n",
      "Epoch: 628, Training Loss: 0.10791712999343872, Validation Loss: 0.11472199857234955, Learning Rate: 0.0002621440000000001\n",
      "Epoch: 629, Training Loss: 0.107726089656353, Validation Loss: 0.11417447775602341, Learning Rate: 0.0002621440000000001\n",
      "Epoch: 630, Training Loss: 0.10876776278018951, Validation Loss: 0.11598663032054901, Learning Rate: 0.0002621440000000001\n",
      "Epoch: 631, Training Loss: 0.10712288320064545, Validation Loss: 0.11244244873523712, Learning Rate: 0.0002621440000000001\n",
      "Epoch: 632, Training Loss: 0.10825165361166, Validation Loss: 0.11091231554746628, Learning Rate: 0.0002621440000000001\n",
      "Epoch: 633, Training Loss: 0.10626337677240372, Validation Loss: 0.11525727808475494, Learning Rate: 0.0002621440000000001\n",
      "Epoch: 634, Training Loss: 0.10604127496480942, Validation Loss: 0.11133208125829697, Learning Rate: 0.0002621440000000001\n",
      "Epoch: 635, Training Loss: 0.1076866015791893, Validation Loss: 0.1144813522696495, Learning Rate: 0.0002621440000000001\n",
      "Epoch: 636, Training Loss: 0.10881277918815613, Validation Loss: 0.11194863170385361, Learning Rate: 0.0002621440000000001\n",
      "Epoch: 637, Training Loss: 0.10814990103244781, Validation Loss: 0.11412035673856735, Learning Rate: 0.0002621440000000001\n",
      "Epoch: 638, Training Loss: 0.10815294086933136, Validation Loss: 0.11247175186872482, Learning Rate: 0.0002621440000000001\n",
      "Epoch: 639, Training Loss: 0.10744644701480865, Validation Loss: 0.11519351601600647, Learning Rate: 0.0002621440000000001\n",
      "Epoch: 640, Training Loss: 0.10697904974222183, Validation Loss: 0.1161956861615181, Learning Rate: 0.0002621440000000001\n",
      "Epoch: 641, Training Loss: 0.10752753168344498, Validation Loss: 0.11224929988384247, Learning Rate: 0.0002621440000000001\n",
      "Epoch: 642, Training Loss: 0.107712022960186, Validation Loss: 0.11340347677469254, Learning Rate: 0.0002621440000000001\n",
      "Epoch: 643, Training Loss: 0.10850884020328522, Validation Loss: 0.1137765645980835, Learning Rate: 0.0002621440000000001\n",
      "Epoch: 644, Training Loss: 0.10741828382015228, Validation Loss: 0.11072025448083878, Learning Rate: 0.0002621440000000001\n",
      "Epoch: 645, Training Loss: 0.10773515701293945, Validation Loss: 0.11559483408927917, Learning Rate: 0.0002621440000000001\n",
      "Epoch: 646, Training Loss: 0.10847179591655731, Validation Loss: 0.11327026039361954, Learning Rate: 0.0002621440000000001\n",
      "Epoch: 647, Training Loss: 0.10824362188577652, Validation Loss: 0.11165063828229904, Learning Rate: 0.0002621440000000001\n",
      "Epoch: 648, Training Loss: 0.10661007463932037, Validation Loss: 0.11300007998943329, Learning Rate: 0.0002621440000000001\n",
      "Epoch: 649, Training Loss: 0.10957338660955429, Validation Loss: 0.11239609122276306, Learning Rate: 0.0002621440000000001\n",
      "Epoch: 650, Training Loss: 0.10721227526664734, Validation Loss: 0.11149486154317856, Learning Rate: 0.0002621440000000001\n",
      "Epoch: 651, Training Loss: 0.10768909752368927, Validation Loss: 0.11303931474685669, Learning Rate: 0.0002621440000000001\n",
      "Epoch: 652, Training Loss: 0.10697919130325317, Validation Loss: 0.11137255281209946, Learning Rate: 0.0002621440000000001\n",
      "Epoch: 653, Training Loss: 0.10806068032979965, Validation Loss: 0.11576133966445923, Learning Rate: 0.0002621440000000001\n",
      "Epoch: 654, Training Loss: 0.10834485292434692, Validation Loss: 0.11481922119855881, Learning Rate: 0.0002621440000000001\n",
      "Epoch: 655, Training Loss: 0.1084195077419281, Validation Loss: 0.11145741492509842, Learning Rate: 0.0002621440000000001\n",
      "Epoch: 656, Training Loss: 0.10709986835718155, Validation Loss: 0.11222877353429794, Learning Rate: 0.0002621440000000001\n",
      "Epoch: 657, Training Loss: 0.10752101242542267, Validation Loss: 0.11325421929359436, Learning Rate: 0.0002621440000000001\n",
      "Epoch: 658, Training Loss: 0.10796869546175003, Validation Loss: 0.11178874224424362, Learning Rate: 0.0002621440000000001\n",
      "Epoch: 659, Training Loss: 0.10736257582902908, Validation Loss: 0.11131981760263443, Learning Rate: 0.0002621440000000001\n",
      "Epoch: 660, Training Loss: 0.10853487998247147, Validation Loss: 0.11473466455936432, Learning Rate: 0.0002621440000000001\n",
      "Epoch: 661, Training Loss: 0.10831833630800247, Validation Loss: 0.1117999404668808, Learning Rate: 0.0002621440000000001\n",
      "Epoch: 662, Training Loss: 0.10632597655057907, Validation Loss: 0.11033056676387787, Learning Rate: 0.0002621440000000001\n",
      "Epoch: 663, Training Loss: 0.10676731169223785, Validation Loss: 0.11425483226776123, Learning Rate: 0.0002621440000000001\n",
      "Epoch: 664, Training Loss: 0.10842964053153992, Validation Loss: 0.11142851412296295, Learning Rate: 0.0002621440000000001\n",
      "Epoch: 665, Training Loss: 0.10854596644639969, Validation Loss: 0.11252804845571518, Learning Rate: 0.0002621440000000001\n",
      "Epoch: 666, Training Loss: 0.10597244650125504, Validation Loss: 0.11306660622358322, Learning Rate: 0.0002621440000000001\n",
      "Epoch: 667, Training Loss: 0.10650650411844254, Validation Loss: 0.11399990320205688, Learning Rate: 0.0002621440000000001\n",
      "Epoch: 668, Training Loss: 0.10985350608825684, Validation Loss: 0.11384483426809311, Learning Rate: 0.0002621440000000001\n",
      "Epoch: 669, Training Loss: 0.10767290741205215, Validation Loss: 0.1092047467827797, Learning Rate: 0.0002621440000000001\n",
      "Epoch: 670, Training Loss: 0.10702100396156311, Validation Loss: 0.11315720528364182, Learning Rate: 0.0002621440000000001\n",
      "Epoch: 671, Training Loss: 0.10699476301670074, Validation Loss: 0.11339882016181946, Learning Rate: 0.0002621440000000001\n",
      "Epoch: 672, Training Loss: 0.10684297233819962, Validation Loss: 0.11193761974573135, Learning Rate: 0.0002621440000000001\n",
      "Epoch: 673, Training Loss: 0.10883484780788422, Validation Loss: 0.11403831094503403, Learning Rate: 0.0002621440000000001\n",
      "Epoch: 674, Training Loss: 0.10737621784210205, Validation Loss: 0.11152727901935577, Learning Rate: 0.0002621440000000001\n",
      "Epoch: 675, Training Loss: 0.10675954073667526, Validation Loss: 0.11070502549409866, Learning Rate: 0.0002621440000000001\n",
      "Epoch: 676, Training Loss: 0.10864181816577911, Validation Loss: 0.11167862266302109, Learning Rate: 0.0002621440000000001\n",
      "Epoch: 677, Training Loss: 0.10741249471902847, Validation Loss: 0.11391374468803406, Learning Rate: 0.0002621440000000001\n",
      "Epoch: 678, Training Loss: 0.10661850869655609, Validation Loss: 0.11692190915346146, Learning Rate: 0.0002621440000000001\n",
      "Epoch: 679, Training Loss: 0.1084752082824707, Validation Loss: 0.11228570342063904, Learning Rate: 0.0002621440000000001\n",
      "Epoch: 680, Training Loss: 0.10820246487855911, Validation Loss: 0.11338546127080917, Learning Rate: 0.0002621440000000001\n",
      "Epoch: 681, Training Loss: 0.10952726751565933, Validation Loss: 0.11474060267210007, Learning Rate: 0.0002621440000000001\n",
      "Epoch: 682, Training Loss: 0.1088334321975708, Validation Loss: 0.1153596043586731, Learning Rate: 0.0002621440000000001\n",
      "Epoch: 683, Training Loss: 0.10813570022583008, Validation Loss: 0.1113940179347992, Learning Rate: 0.0002621440000000001\n",
      "Epoch: 684, Training Loss: 0.10723864287137985, Validation Loss: 0.11276756972074509, Learning Rate: 0.0002621440000000001\n",
      "Epoch: 685, Training Loss: 0.10572440177202225, Validation Loss: 0.1130872592329979, Learning Rate: 0.0002621440000000001\n",
      "Epoch: 686, Training Loss: 0.10743448883295059, Validation Loss: 0.11103881150484085, Learning Rate: 0.0002621440000000001\n",
      "Epoch: 687, Training Loss: 0.10943618416786194, Validation Loss: 0.11249089986085892, Learning Rate: 0.0002621440000000001\n",
      "Epoch: 688, Training Loss: 0.10793120414018631, Validation Loss: 0.11255908012390137, Learning Rate: 0.0002621440000000001\n",
      "Epoch: 689, Training Loss: 0.10663089156150818, Validation Loss: 0.11132339388132095, Learning Rate: 0.0002621440000000001\n",
      "Epoch: 690, Training Loss: 0.10725375264883041, Validation Loss: 0.10992144048213959, Learning Rate: 0.0002621440000000001\n",
      "Epoch: 691, Training Loss: 0.10938683152198792, Validation Loss: 0.11532645672559738, Learning Rate: 0.0002621440000000001\n",
      "Epoch: 692, Training Loss: 0.10788290202617645, Validation Loss: 0.1095423474907875, Learning Rate: 0.0002621440000000001\n",
      "Epoch: 693, Training Loss: 0.10658232122659683, Validation Loss: 0.11451941728591919, Learning Rate: 0.0002621440000000001\n",
      "Epoch: 694, Training Loss: 0.10710608214139938, Validation Loss: 0.11412610113620758, Learning Rate: 0.0002621440000000001\n",
      "Epoch: 695, Training Loss: 0.1084304228425026, Validation Loss: 0.11252762377262115, Learning Rate: 0.0002621440000000001\n",
      "Epoch: 696, Training Loss: 0.10792450606822968, Validation Loss: 0.11361521482467651, Learning Rate: 0.0002621440000000001\n",
      "Epoch: 697, Training Loss: 0.10848425328731537, Validation Loss: 0.1140841543674469, Learning Rate: 0.0002621440000000001\n",
      "Epoch: 698, Training Loss: 0.10778450220823288, Validation Loss: 0.11418923735618591, Learning Rate: 0.0002621440000000001\n",
      "Epoch: 699, Training Loss: 0.10809969902038574, Validation Loss: 0.10754293948411942, Learning Rate: 0.0002621440000000001\n",
      "Epoch: 700, Training Loss: 0.10833995044231415, Validation Loss: 0.11315502226352692, Learning Rate: 0.00020971520000000012\n",
      "Epoch: 701, Training Loss: 0.108218714594841, Validation Loss: 0.10624559968709946, Learning Rate: 0.00020971520000000012\n",
      "Epoch: 702, Training Loss: 0.10837213695049286, Validation Loss: 0.11469902098178864, Learning Rate: 0.00020971520000000012\n",
      "Epoch: 703, Training Loss: 0.10667174309492111, Validation Loss: 0.11388608068227768, Learning Rate: 0.00020971520000000012\n",
      "Epoch: 704, Training Loss: 0.10898398607969284, Validation Loss: 0.11239095777273178, Learning Rate: 0.00020971520000000012\n",
      "Epoch: 705, Training Loss: 0.10750851035118103, Validation Loss: 0.11356095224618912, Learning Rate: 0.00020971520000000012\n",
      "Epoch: 706, Training Loss: 0.10739452391862869, Validation Loss: 0.11179710179567337, Learning Rate: 0.00020971520000000012\n",
      "Epoch: 707, Training Loss: 0.10592732578516006, Validation Loss: 0.11259625107049942, Learning Rate: 0.00020971520000000012\n",
      "Epoch: 708, Training Loss: 0.10619612038135529, Validation Loss: 0.11384371668100357, Learning Rate: 0.00020971520000000012\n",
      "Epoch: 709, Training Loss: 0.10669799894094467, Validation Loss: 0.11587490886449814, Learning Rate: 0.00020971520000000012\n",
      "Epoch: 710, Training Loss: 0.10828673094511032, Validation Loss: 0.11471226066350937, Learning Rate: 0.00020971520000000012\n",
      "Epoch: 711, Training Loss: 0.10785718262195587, Validation Loss: 0.1126297190785408, Learning Rate: 0.00020971520000000012\n",
      "Epoch: 712, Training Loss: 0.10743158310651779, Validation Loss: 0.11557857692241669, Learning Rate: 0.00020971520000000012\n",
      "Epoch: 713, Training Loss: 0.10805123299360275, Validation Loss: 0.11198388040065765, Learning Rate: 0.00020971520000000012\n",
      "Epoch: 714, Training Loss: 0.1073276624083519, Validation Loss: 0.11339124292135239, Learning Rate: 0.00020971520000000012\n",
      "Epoch: 715, Training Loss: 0.10869935154914856, Validation Loss: 0.1130998358130455, Learning Rate: 0.00020971520000000012\n",
      "Epoch: 716, Training Loss: 0.10918671637773514, Validation Loss: 0.11310312896966934, Learning Rate: 0.00020971520000000012\n",
      "Epoch: 717, Training Loss: 0.10676046460866928, Validation Loss: 0.10983812063932419, Learning Rate: 0.00020971520000000012\n",
      "Epoch: 718, Training Loss: 0.10464129596948624, Validation Loss: 0.1094895601272583, Learning Rate: 0.00020971520000000012\n",
      "Epoch: 719, Training Loss: 0.10711288452148438, Validation Loss: 0.1124846339225769, Learning Rate: 0.00020971520000000012\n",
      "Epoch: 720, Training Loss: 0.10710843652486801, Validation Loss: 0.11340343952178955, Learning Rate: 0.00020971520000000012\n",
      "Epoch: 721, Training Loss: 0.10782299935817719, Validation Loss: 0.10959766060113907, Learning Rate: 0.00020971520000000012\n",
      "Epoch: 722, Training Loss: 0.10807260125875473, Validation Loss: 0.11363991349935532, Learning Rate: 0.00020971520000000012\n",
      "Epoch: 723, Training Loss: 0.10711240768432617, Validation Loss: 0.11189024895429611, Learning Rate: 0.00020971520000000012\n",
      "Epoch: 724, Training Loss: 0.10713152587413788, Validation Loss: 0.11293688416481018, Learning Rate: 0.00020971520000000012\n",
      "Epoch: 725, Training Loss: 0.10784140974283218, Validation Loss: 0.1136694997549057, Learning Rate: 0.00020971520000000012\n",
      "Epoch: 726, Training Loss: 0.1074744164943695, Validation Loss: 0.11006913334131241, Learning Rate: 0.00020971520000000012\n",
      "Epoch: 727, Training Loss: 0.10803212970495224, Validation Loss: 0.11094406992197037, Learning Rate: 0.00020971520000000012\n",
      "Epoch: 728, Training Loss: 0.10670357197523117, Validation Loss: 0.11256391555070877, Learning Rate: 0.00020971520000000012\n",
      "Epoch: 729, Training Loss: 0.10843648761510849, Validation Loss: 0.11037416756153107, Learning Rate: 0.00020971520000000012\n",
      "Epoch: 730, Training Loss: 0.10576950758695602, Validation Loss: 0.11024118214845657, Learning Rate: 0.00020971520000000012\n",
      "Epoch: 731, Training Loss: 0.10811460018157959, Validation Loss: 0.11332569271326065, Learning Rate: 0.00020971520000000012\n",
      "Epoch: 732, Training Loss: 0.10749044269323349, Validation Loss: 0.11230003833770752, Learning Rate: 0.00020971520000000012\n",
      "Epoch: 733, Training Loss: 0.10564526170492172, Validation Loss: 0.10831917822360992, Learning Rate: 0.00020971520000000012\n",
      "Epoch: 734, Training Loss: 0.10770265012979507, Validation Loss: 0.11208444088697433, Learning Rate: 0.00020971520000000012\n",
      "Epoch: 735, Training Loss: 0.10603079944849014, Validation Loss: 0.11030597984790802, Learning Rate: 0.00020971520000000012\n",
      "Epoch: 736, Training Loss: 0.10798975825309753, Validation Loss: 0.11404087394475937, Learning Rate: 0.00020971520000000012\n",
      "Epoch: 737, Training Loss: 0.10624689608812332, Validation Loss: 0.10927947610616684, Learning Rate: 0.00020971520000000012\n",
      "Epoch: 738, Training Loss: 0.10834871977567673, Validation Loss: 0.10937844216823578, Learning Rate: 0.00020971520000000012\n",
      "Epoch: 739, Training Loss: 0.10768281668424606, Validation Loss: 0.11398087441921234, Learning Rate: 0.00020971520000000012\n",
      "Epoch: 740, Training Loss: 0.1053934246301651, Validation Loss: 0.11126802861690521, Learning Rate: 0.00020971520000000012\n",
      "Epoch: 741, Training Loss: 0.10775220394134521, Validation Loss: 0.10741793364286423, Learning Rate: 0.00020971520000000012\n",
      "Epoch: 742, Training Loss: 0.1083778440952301, Validation Loss: 0.11517883837223053, Learning Rate: 0.00020971520000000012\n",
      "Epoch: 743, Training Loss: 0.10674892365932465, Validation Loss: 0.1103464737534523, Learning Rate: 0.00020971520000000012\n",
      "Epoch: 744, Training Loss: 0.10734544694423676, Validation Loss: 0.11488703638315201, Learning Rate: 0.00020971520000000012\n",
      "Epoch: 745, Training Loss: 0.10782257467508316, Validation Loss: 0.11101219058036804, Learning Rate: 0.00020971520000000012\n",
      "Epoch: 746, Training Loss: 0.1056765615940094, Validation Loss: 0.11294668912887573, Learning Rate: 0.00020971520000000012\n",
      "Epoch: 747, Training Loss: 0.10679914802312851, Validation Loss: 0.1108870655298233, Learning Rate: 0.00020971520000000012\n",
      "Epoch: 748, Training Loss: 0.1085934191942215, Validation Loss: 0.11027579754590988, Learning Rate: 0.00020971520000000012\n",
      "Epoch: 749, Training Loss: 0.1062934547662735, Validation Loss: 0.11252705752849579, Learning Rate: 0.00020971520000000012\n",
      "Epoch: 750, Training Loss: 0.10569671541452408, Validation Loss: 0.11179209500551224, Learning Rate: 0.00020971520000000012\n",
      "Epoch: 751, Training Loss: 0.10545437783002853, Validation Loss: 0.10885289311408997, Learning Rate: 0.00020971520000000012\n",
      "Epoch: 752, Training Loss: 0.10565893352031708, Validation Loss: 0.11271604895591736, Learning Rate: 0.00020971520000000012\n",
      "Epoch: 753, Training Loss: 0.10793940722942352, Validation Loss: 0.11503029614686966, Learning Rate: 0.00020971520000000012\n",
      "Epoch: 754, Training Loss: 0.10618243366479874, Validation Loss: 0.1142812967300415, Learning Rate: 0.00020971520000000012\n",
      "Epoch: 755, Training Loss: 0.10669977962970734, Validation Loss: 0.11141926795244217, Learning Rate: 0.00020971520000000012\n",
      "Epoch: 756, Training Loss: 0.10712426155805588, Validation Loss: 0.11586715281009674, Learning Rate: 0.00020971520000000012\n",
      "Epoch: 757, Training Loss: 0.10601530969142914, Validation Loss: 0.11067209392786026, Learning Rate: 0.00020971520000000012\n",
      "Epoch: 758, Training Loss: 0.10685273259878159, Validation Loss: 0.11451498419046402, Learning Rate: 0.00020971520000000012\n",
      "Epoch: 759, Training Loss: 0.10631608963012695, Validation Loss: 0.11335217207670212, Learning Rate: 0.00020971520000000012\n",
      "Epoch: 760, Training Loss: 0.10774692893028259, Validation Loss: 0.11027828603982925, Learning Rate: 0.00020971520000000012\n",
      "Epoch: 761, Training Loss: 0.10697078704833984, Validation Loss: 0.1110047996044159, Learning Rate: 0.00020971520000000012\n",
      "Epoch: 762, Training Loss: 0.10614974051713943, Validation Loss: 0.11470653861761093, Learning Rate: 0.00020971520000000012\n",
      "Epoch: 763, Training Loss: 0.10602445900440216, Validation Loss: 0.11235320568084717, Learning Rate: 0.00020971520000000012\n",
      "Epoch: 764, Training Loss: 0.10620991140604019, Validation Loss: 0.10974621772766113, Learning Rate: 0.00020971520000000012\n",
      "Epoch: 765, Training Loss: 0.10807303339242935, Validation Loss: 0.11187759786844254, Learning Rate: 0.00020971520000000012\n",
      "Epoch: 766, Training Loss: 0.10745065659284592, Validation Loss: 0.11105650663375854, Learning Rate: 0.00020971520000000012\n",
      "Epoch: 767, Training Loss: 0.10589005798101425, Validation Loss: 0.11139150708913803, Learning Rate: 0.00020971520000000012\n",
      "Epoch: 768, Training Loss: 0.10661869496107101, Validation Loss: 0.11209981888532639, Learning Rate: 0.00020971520000000012\n",
      "Epoch: 769, Training Loss: 0.10820533335208893, Validation Loss: 0.11303291469812393, Learning Rate: 0.00020971520000000012\n",
      "Epoch: 770, Training Loss: 0.10812775045633316, Validation Loss: 0.11498643457889557, Learning Rate: 0.00020971520000000012\n",
      "Epoch: 771, Training Loss: 0.10642793029546738, Validation Loss: 0.10936161130666733, Learning Rate: 0.00020971520000000012\n",
      "Epoch: 772, Training Loss: 0.1062699556350708, Validation Loss: 0.11042609065771103, Learning Rate: 0.00020971520000000012\n",
      "Epoch: 773, Training Loss: 0.10719314217567444, Validation Loss: 0.11170981824398041, Learning Rate: 0.00020971520000000012\n",
      "Epoch: 774, Training Loss: 0.10735268145799637, Validation Loss: 0.11251626163721085, Learning Rate: 0.00020971520000000012\n",
      "Epoch: 775, Training Loss: 0.10663753747940063, Validation Loss: 0.10736354440450668, Learning Rate: 0.00020971520000000012\n",
      "Epoch: 776, Training Loss: 0.10665769129991531, Validation Loss: 0.11247406899929047, Learning Rate: 0.00020971520000000012\n",
      "Epoch: 777, Training Loss: 0.10706592351198196, Validation Loss: 0.11329422891139984, Learning Rate: 0.00020971520000000012\n",
      "Epoch: 778, Training Loss: 0.10520336776971817, Validation Loss: 0.1087363064289093, Learning Rate: 0.00020971520000000012\n",
      "Epoch: 779, Training Loss: 0.10643119364976883, Validation Loss: 0.1100306287407875, Learning Rate: 0.00020971520000000012\n",
      "Epoch: 780, Training Loss: 0.10613755881786346, Validation Loss: 0.10929909348487854, Learning Rate: 0.00020971520000000012\n",
      "Epoch: 781, Training Loss: 0.1070810854434967, Validation Loss: 0.11510888487100601, Learning Rate: 0.00020971520000000012\n",
      "Epoch: 782, Training Loss: 0.10711750388145447, Validation Loss: 0.11236850172281265, Learning Rate: 0.00020971520000000012\n",
      "Epoch: 783, Training Loss: 0.10563798248767853, Validation Loss: 0.11113857477903366, Learning Rate: 0.00020971520000000012\n",
      "Epoch: 784, Training Loss: 0.10627920180559158, Validation Loss: 0.11140188574790955, Learning Rate: 0.00020971520000000012\n",
      "Epoch: 785, Training Loss: 0.10597951710224152, Validation Loss: 0.11271902173757553, Learning Rate: 0.00020971520000000012\n",
      "Epoch: 786, Training Loss: 0.10722082853317261, Validation Loss: 0.11254806816577911, Learning Rate: 0.00020971520000000012\n",
      "Epoch: 787, Training Loss: 0.10918714851140976, Validation Loss: 0.1128549575805664, Learning Rate: 0.00020971520000000012\n",
      "Epoch: 788, Training Loss: 0.10729225724935532, Validation Loss: 0.11210839450359344, Learning Rate: 0.00020971520000000012\n",
      "Epoch: 789, Training Loss: 0.106840580701828, Validation Loss: 0.11209660023450851, Learning Rate: 0.00020971520000000012\n",
      "Epoch: 790, Training Loss: 0.10566364973783493, Validation Loss: 0.11091016978025436, Learning Rate: 0.00020971520000000012\n",
      "Epoch: 791, Training Loss: 0.10596945881843567, Validation Loss: 0.11252471059560776, Learning Rate: 0.00020971520000000012\n",
      "Epoch: 792, Training Loss: 0.10859273374080658, Validation Loss: 0.11400863528251648, Learning Rate: 0.00020971520000000012\n",
      "Epoch: 793, Training Loss: 0.10720793157815933, Validation Loss: 0.11095316708087921, Learning Rate: 0.00020971520000000012\n",
      "Epoch: 794, Training Loss: 0.10583948343992233, Validation Loss: 0.11206452548503876, Learning Rate: 0.00020971520000000012\n",
      "Epoch: 795, Training Loss: 0.10581239312887192, Validation Loss: 0.11151077598333359, Learning Rate: 0.00020971520000000012\n",
      "Epoch: 796, Training Loss: 0.1061171218752861, Validation Loss: 0.11201606690883636, Learning Rate: 0.00020971520000000012\n",
      "Epoch: 797, Training Loss: 0.10549505054950714, Validation Loss: 0.11439564824104309, Learning Rate: 0.00020971520000000012\n",
      "Epoch: 798, Training Loss: 0.10731097310781479, Validation Loss: 0.11287035048007965, Learning Rate: 0.00020971520000000012\n",
      "Epoch: 799, Training Loss: 0.10668359696865082, Validation Loss: 0.11355354636907578, Learning Rate: 0.00020971520000000012\n"
     ]
    }
   ],
   "source": [
    "# Training process\n",
    "\n",
    "for epoch in range(epoches):\n",
    "    for x_batch, y_batch, u_batch in data_loader_latent:\n",
    "        optimizer_latent.zero_grad()\n",
    "        loss = mse_loss(model_latent(x_batch, u_batch), y_batch)\n",
    "        loss.backward()\n",
    "        optimizer_latent.step()\n",
    "        \n",
    "    \n",
    "    train_loss = mse_loss(model_latent(x_train_latent, u_train), y_train_latent)\n",
    "    train_loss_value = train_loss.item()\n",
    "    train_losses_latent.append(train_loss_value)\n",
    "        \n",
    "    # Test the model\n",
    "    with torch.no_grad():\n",
    "        val_loss = mse_loss(model_latent(x_test_latent, u_test), y_test_latent)\n",
    "        val_losses_latent.append(val_loss.item())\n",
    "\n",
    "    current_lr = optimizer_latent.param_groups[0]['lr']\n",
    "    scheduler_latent.step()\n",
    "    print(f'Epoch: {epoch}, Training Loss: {train_loss.item()}, Validation Loss: {val_loss.item()}, Learning Rate: {current_lr}')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAA10AAAGsCAYAAAAmKs3KAAAAOXRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjguMCwgaHR0cHM6Ly9tYXRwbG90bGliLm9yZy81sbWrAAAACXBIWXMAAA9hAAAPYQGoP6dpAACozElEQVR4nOzddXjTV//G8XfqLhRKgVLcddhwH7INmSvMlflv9sye+fbMDZiPMQPGYAaDwRgyZLi7Fmih0FKnmvz+OI21KRQ2qHC/rouryTffJCdtgdz5nPM5FpvNZkNERERERETOCK/yHoCIiIiIiEhVptAlIiIiIiJyBil0iYiIiIiInEEKXSIiIiIiImeQQpeIiIiIiMgZpNAlIiIiIiJyBil0iYiIiIiInEE+5T2AysZqtZKQkEBoaCgWi6W8hyMiIiIiIuXEZrORkZFB7dq18fIqvZ6l0HWKEhISqFu3bnkPQ0REREREKoj9+/cTGxtb6u0KXWU0duxYxo4dS0FBAWC+sWFhYeU8KhERERERKS/p6enUrVuX0NDQE55nsdlstrM0piohPT2d8PBw0tLSFLpERERERM5hZc0GaqQhIiIiIiJyBil0iYiIiIiInEEKXSIiIiIiImeQGmmIiIiISKVXWFhIfn5+eQ9DqhhfX1+8vb3/8eModImIiIhIpWWz2Th06BCpqanlPRSpoiIiIoiJiflHe/QqdImIiIhIpWUPXNHR0QQFBf2jN8Yirmw2G9nZ2SQlJQFQq1at034shS4RERERqZQKCwsdgSsqKqq8hyNVUGBgIABJSUlER0ef9lRDNdIQERERkUrJvoYrKCionEciVZn99+ufrBlU6BIRERGRSk1TCuVM+jd+vxS6REREREREziCFLhERERERkTNIoUtEREREpAro27cvDzzwQJnP37t3LxaLhbVr156xMYmh0CUiIiIichZZLJYT/rnxxhtP63GnTZvGCy+8UObz69atS2JiIq1btz6t5ysrhTu1jK+8jh+DxHXgEwhx55f3aERERESkjBITEx2XJ0+ezDPPPMO2bdscx+xtyu3y8/Px9fU96eNWq1btlMbh7e1NTEzMKd1HTo8qXZVU0q7VMHEEaZNvL++hiIiIiFQYNpuN7LyCs/7HZrOVeYwxMTGOP+Hh4VgsFsf1nJwcIiIimDJlCn379iUgIICvv/6a5ORkrrnmGmJjYwkKCqJNmzZ89913bo9bfHph/fr1efnll7n55psJDQ0lLi6Ojz/+2HF78QrU/PnzsVgs/PHHH3Tq1ImgoCC6d+/uFggBXnzxRaKjowkNDeXWW2/l8ccfp3379qf8s7LLzc3lvvvuIzo6moCAAHr27MmKFSsctx87dozrrruOGjVqEBgYSJMmTfjiiy8AyMvL45577qFWrVoEBARQv359XnnlldMey5miSlcldTSrkGggPTuP8PIejIiIiEgFcTy/kJbPzD7rz7v5+cEE+f17b60fe+wx3nzzTb744gv8/f3JycmhY8eOPPbYY4SFhTFjxgxGjRpFw4YNOf/80mc9vfnmm7zwwgs88cQTTJ06lbvuuovevXvTvHnzUu/z5JNP8uabb1KjRg3uvPNObr75ZhYvXgzAN998w0svvcS4cePo0aMHkyZN4s0336RBgwan/VofffRRfvjhB7788kvq1avHa6+9xuDBg9m5cyfVqlXj6aefZvPmzfz2229Ur16dnTt3cvz4cQDee+89fv75Z6ZMmUJcXBz79+9n//79pz2WM0Whq5KyWEyR0ovCch6JiIiIiPzbHnjgAS699FK3Yw8//LDj8r333susWbP4/vvvTxi6LrzwQu6++27ABLm3336b+fPnnzB0vfTSS/Tp0weAxx9/nIsuuoicnBwCAgJ4//33ueWWW7jpppsAeOaZZ/j999/JzMw8rdeZlZXF+PHjmTBhAkOHDgXgk08+Yc6cOXz22Wc88sgjxMfHc95559GpUyfAVPDs4uPjadKkCT179sRisVCvXr3TGseZptBVSVm8vQHwwlrOIxERERGpOAJ9vdn8/OByed5/kz1g2BUWFvLqq68yefJkDh48SG5uLrm5uQQHB5/wcdq2beu4bJ/GmJSUVOb71KpVC4CkpCTi4uLYtm2bI8TZdenShXnz5pXpdRW3a9cu8vPz6dGjh+OYr68vXbp0YcuWLQDcddddXHbZZaxevZpBgwYxcuRIunfvDsCNN97IBRdcQLNmzRgyZAgXX3wxgwYNOq2xnEkKXZWUxWL+YltOYf6wiIiISFVnsVj+1Wl+5aV4mHrzzTd5++23eeedd2jTpg3BwcE88MAD5OXlnfBxijfgsFgsWK0n/tDe9T4WiwXA7T72Y3ansp6tOPt9PT2m/djQoUPZt28fM2bMYO7cuQwYMIAxY8bwxhtv0KFDB/bs2cNvv/3G3LlzufLKKxk4cCBTp0497TGdCWqkUUl5eZnQ5a1Kl4iIiEiVt2jRIkaMGMH1119Pu3btaNiwITt27Djr42jWrBnLly93O7Zy5crTfrzGjRvj5+fHX3/95TiWn5/PypUradGiheNYjRo1uPHGG/n6669555133BqChIWFcdVVV/HJJ58wefJkfvjhB1JSUk57TGdC5f8Y4FxVFLosCl0iIiIiVV7jxo354YcfWLJkCZGRkbz11lscOnTILZicDffeey+33XYbnTp1onv37kyePJn169fTsGHDk963eBdEgJYtW3LXXXfxyCOPUK1aNeLi4njttdfIzs7mlltuAcy6sY4dO9KqVStyc3P59ddfHa/77bffplatWrRv3x4vLy++//57YmJiiIiI+Fdf9z+l0FVJeXvbG2kodImIiIhUdU8//TR79uxh8ODBBAUFcfvttzNy5EjS0tLO6jiuu+46du/ezcMPP0xOTg5XXnklN954Y4nqlydXX311iWN79uzh1VdfxWq1MmrUKDIyMujUqROzZ88mMjISAD8/P/7zn/+wd+9eAgMD6dWrF5MmTQIgJCSE//3vf+zYsQNvb286d+7MzJkz8fKqWBP6LLZ/MgnzHJSenk54eDhpaWmEhYWV2zj2bFlNg8n9SCWEiGcPlts4RERERMpLTk4Oe/bsoUGDBgQEBJT3cM5ZF1xwATExMXz11VflPZQz4kS/Z2XNBqp0VVL29O6FMrOIiIiInB3Z2dl8+OGHDB48GG9vb7777jvmzp3LnDlzyntoFZpCV2XlpZbxIiIiInJ2WSwWZs6cyYsvvkhubi7NmjXjhx9+YODAgeU9tApNoauS8vY2Pzovm0KXiIiIiJwdgYGBzJ07t7yHUelUrBVmUmbO6YUKXSIiIiIiFZlCVyVl0fRCEREREZFKQaGrkvLytocuNdIQEREREanIFLoqKa+iSpePRZUuEREREZGKTKGrkrJPLwRAW62JiIiIiFRYCl2VlJdL6LJZC8pxJCIiIiJSHvr27csDDzzguF6/fn3eeeedE97HYrHw448//uPn/rce51yh0FVJefk4u/1bCwvLcSQiIiIiciqGDRtW6r5WS5cuxWKxsHr16lN+3BUrVnD77bf/0+G5efbZZ2nfvn2J44mJiQwdOvRffa7iJkyYQERExBl9jrNFoauSslicP7pChS4RERGRSuOWW25h3rx57Nu3r8Rtn3/+Oe3bt6dDhw6n/Lg1atQgKCjo3xjiScXExODv739WnqsqUOiqpLxdK12aXigiIiJSaVx88cVER0czYcIEt+PZ2dlMnjyZW265heTkZK655hpiY2MJCgqiTZs2fPfddyd83OLTC3fs2EHv3r0JCAigZcuWzJkzp8R9HnvsMZo2bUpQUBANGzbk6aefJj8/HzCVpueee45169ZhsViwWCyOMRefXrhhwwb69+9PYGAgUVFR3H777WRmZjpuv/HGGxk5ciRvvPEGtWrVIioqijFjxjie63TEx8czYsQIQkJCCAsL48orr+Tw4cOO29etW0e/fv0IDQ0lLCyMjh07snLlSgD27dvHsGHDiIyMJDg4mFatWjFz5szTHsvJ+Jz8FKmI7JsjA1itqnSJiIiIAKbBWH722X9e3yCwWMp0qo+PD6NHj2bChAk888wzWIru9/3335OXl8d1111HdnY2HTt25LHHHiMsLIwZM2YwatQoGjZsyPnnn3/S57BarVx66aVUr16dZcuWkZ6e7rb+yy40NJQJEyZQu3ZtNmzYwG233UZoaCiPPvooV111FRs3bmTWrFnMnTsXgPDw8BKPkZ2dzZAhQ+jatSsrVqwgKSmJW2+9lXvuucctWP7555/UqlWLP//8k507d3LVVVfRvn17brvttjJ931zZbDZGjhxJcHAwCxYsoKCggLvvvpurrrqK+fPnA3Dddddx3nnnMX78eLy9vVm7di2+vr4AjBkzhry8PBYuXEhwcDCbN28mJCTklMdRVgpdlZSXl/NHp+mFIiIiIkXys+Hl2mf/eZ9IAL/gMp9+88038/rrrzN//nz69esHmKmFl156KZGRkURGRvLwww87zr/33nuZNWsW33//fZlC19y5c9myZQt79+4lNjYWgJdffrnEOqynnnrKcbl+/fr83//9H5MnT+bRRx8lMDCQkJAQfHx8iImJKfW5vvnmG44fP87EiRMJDjbfgw8++IBhw4bxv//9j5o1awIQGRnJBx98gLe3N82bN+eiiy7ijz/+OK3QNXfuXNavX8+ePXuoW7cuAF999RWtWrVixYoVdO7cmfj4eB555BGaN28OQJMmTRz3j4+P57LLLqNNmzYANGzY8JTHcCo0vbCS8vJ2hi6bVXt1iYiIiFQmzZs3p3v37nz++ecA7Nq1i0WLFnHzzTcD5kP1l156ibZt2xIVFUVISAi///478fHxZXr8LVu2EBcX5whcAN26dStx3tSpU+nZsycxMTGEhITw9NNPl/k5XJ+rXbt2jsAF0KNHD6xWK9u2bXMca9WqFd7ezg7ctWrVIikp6ZSey/U569at6whcAC1btiQiIoItW7YA8NBDD3HrrbcycOBAXn31VXbt2uU497777uPFF1+kR48e/Pe//2X9+vWnNY6yUqWrkvJ2nV5YoDVdIiIiIoCZ5vdEQvk87ym65ZZbuOeeexg7dixffPEF9erVY8CAAQC8+eabvP3227zzzju0adOG4OBgHnjgAfLy8sr02DYP+7haik1/XLZsGVdffTXPPfccgwcPJjw8nEmTJvHmm2+e0uuw2WwlHtvTc9qn9rneZj3N4kFpz+l6/Nlnn+Xaa69lxowZ/Pbbb/z3v/9l0qRJXHLJJdx6660MHjyYGTNm8Pvvv/PKK6/w5ptvcu+9957WeE5Gla5KysvLQqHN/EJpTZeIiIhIEYvFTPM723/KuJ7L1ZVXXom3tzfffvstX375JTfddJMjMCxatIgRI0Zw/fXX065dOxo2bMiOHTvK/NgtW7YkPj6ehARnAF26dKnbOYsXL6ZevXo8+eSTdOrUiSZNmpToqOjn53fSpSwtW7Zk7dq1ZGVluT22l5cXTZs2LfOYT4X99e3fv99xbPPmzaSlpdGiRQvHsaZNm/Lggw/y+++/c+mll/LFF184bqtbty533nkn06ZN4//+7//45JNPzshYQaGr0rJYLFiLfnwKXSIiIiKVT0hICFdddRVPPPEECQkJ3HjjjY7bGjduzJw5c1iyZAlbtmzhjjvu4NChQ2V+7IEDB9KsWTNGjx7NunXrWLRoEU8++aTbOY0bNyY+Pp5Jkyaxa9cu3nvvPaZPn+52Tv369dmzZw9r167l6NGj5Obmlniu6667joCAAG644QY2btzIn3/+yb333suoUaMc67lOV2FhIWvXrnX7s3nzZgYOHEjbtm257rrrWL16NcuXL2f06NH06dOHTp06cfz4ce655x7mz5/Pvn37WLx4MStWrHAEsgceeIDZs2ezZ88eVq9ezbx589zC2r9NoasSs4cum0KXiIiISKV0yy23cOzYMQYOHEhcXJzj+NNPP02HDh0YPHgwffv2JSYmhpEjR5b5cb28vJg+fTq5ubl06dKFW2+9lZdeesntnBEjRvDggw9yzz330L59e5YsWcLTTz/tds5ll13GkCFD6NevHzVq1PDYtj4oKIjZs2eTkpJC586dufzyyxkwYAAffPDBqX0zPMjMzOS8885z+3PhhRc6WtZHRkbSu3dvBg4cSMOGDZk8eTIA3t7eJCcnM3r0aJo2bcqVV17J0KFDee655wAT5saMGUOLFi0YMmQIzZo1Y9y4cf94vKWx2DxN+JRSpaenEx4eTlpaGmFhYeU6luP/rUGgJY/DNy2nZr1m5ToWERERkbMtJyeHPXv20KBBAwICAsp7OFJFnej3rKzZQJWuSkzTC0VEREREKj6Frkqs0D69UPt0iYiIiIhUWOd06LrkkkuIjIzk8ssvL++hnBYbpruNzabQJSIiIiJSUZ3Toeu+++5j4sSJ5T2M01ZoMT++k7XxFBERERGR8nNOh65+/foRGhpa3sM4bTZ1LxQRERERqfD+Ueh65ZVXsFgsPPDAA//ScIyFCxcybNgwateu7WgH6cm4ceMcXUQ6duzIokWL/tVxVHTOlvGnt5O3iIiISFVg1XshOYP+jd8vn9O944oVK/j4449p27btCc9bvHgxXbp0wdfX1+341q1biYiIICYmpsR9srKyaNeuHTfddBOXXXaZx8edPHkyDzzwAOPGjaNHjx589NFHDB06lM2bNzv2OOjYsaPHDdx+//13ateuXdaXWmE5uhcWFpTzSERERETOPj8/P7y8vEhISKBGjRr4+flhsVjKe1hSRdhsNvLy8jhy5AheXl74+fmd9mOdVujKzMzkuuuu45NPPuHFF18s9Tyr1cqYMWNo0qQJkyZNwtvbG4Dt27fTr18/HnzwQR599NES9xs6dChDhw494RjeeustbrnlFm699VYA3nnnHWbPns348eN55ZVXAFi1atXpvDyPxo4dy9ixYyvU+imro5GGPt0RERGRc4+XlxcNGjQgMTGRhISE8h6OVFFBQUHExcXh5XX6kwRPK3SNGTOGiy66iIEDB54wdHl5eTFz5kx69+7N6NGj+eqrr9izZw/9+/dn+PDhHgNXWeTl5bFq1Soef/xxt+ODBg1iyZIlp/WYJzNmzBjGjBnj2ACtIrBiQqzNqkqXiIiInJv8/PyIi4ujoKCgQn04LlWDt7c3Pj4+/7iCesqha9KkSaxevZoVK1aU6fzatWszb948evfuzbXXXsvSpUsZMGAAH3744SkP1u7o0aMUFhZSs2ZNt+M1a9bk0KFDZX6cwYMHs3r1arKysoiNjWX69Ol07tz5tMd1ttksFrBpHrOIiIic2ywWC76+viWWs4hUFKcUuvbv38/999/P77//TkBAQJnvFxcXx8SJE+nTpw8NGzbks88++1fm2xZ/DJvNdkqPO3v27H88hvJk1ebIIiIiIiIV3ilNTFy1ahVJSUl07NgRHx8ffHx8WLBgAe+99x4+Pj6llnQPHz7M7bffzrBhw8jOzubBBx/8R4OuXr063t7eJapaSUlJJapfVZm9ZTxqGS8iIiIiUmGdUqVrwIABbNiwwe3YTTfdRPPmzXnsscccjTJcHT16lAEDBtCiRQu+//57duzYQd++ffH39+eNN944rUH7+fnRsWNH5syZwyWXXOI4PmfOHEaMGHFaj1kZWS1eRdMLFbpERERERCqqUwpdoaGhtG7d2u1YcHAwUVFRJY6DWWs0ZMgQ6tWrx+TJk/Hx8aFFixbMnTuXfv36UadOHY9Vr8zMTHbu3Om4vmfPHtauXUu1atUc7eAfeughRo0aRadOnejWrRsff/wx8fHx3Hnnnafykio1qzZHFhERERGp8E57n66y8PLy4pVXXqFXr15ufe3btGnD3LlziYqK8ni/lStX0q9fP8f1hx56CIAbbriBCRMmAHDVVVeRnJzM888/T2JiIq1bt2bmzJnUq1fvzL2gCsY+vVAt40VEREREKi6LzWazlfcgKhN7y/i0tDTCwsLKdSzbX+hI08KdbOz7Ka37XlGuYxEREREROdeUNRuc/g5fUu5s9s2RtU+XiIiIiEiFpdBVidks9s2RNb1QRERERKSiUuiqxNRIQ0RERESk4lPoqsRsRRtBq5GGiIiIiEjFpdBVidko2hdNlS4RERERkQpLoasSs1k0vVBEREREpKJT6KrEnKFL0wtFRERERCoqha5KzN4yHpsqXSIiIiIiFZVCVyWmlvEiIiIiIhWfQlclpjVdIiIiIiIVn0JXJWaz//jUMl5EREREpMJS6KrE7JUutYwXEREREam4FLoqMUfoUiMNEREREZEKS6GrElMjDRERERGRik+hqxJTpUtEREREpOJT6KrUtDmyiIiIiEhFp9BVianSJSIiIiJS8Sl0VWbqXigiIiIiUuEpdFVizkqXpheKiIiIiFRUCl2VmaYXioiIiIhUeApdlZi9ZTxqpCEiIiIiUmEpdFVijtCl6YUiIiIiIhWWQldlZrGYr5peKCIiIiJSYSl0VWaqdImIiIiIVHgKXZWZWsaLiIiIiFR4Cl2VmGNNF7ZyHYeIiIiIiJROoasy8zI/PosqXSIiIiIiFZZCV2WmfbpERERERCo8ha7KTI00REREREQqPIWuyqyo0mVR6BIRERERqbAUuiozx/RChS4RERERkYpKoasSc3Qv1JouEREREZEKS6GrErN4aXqhiIiIiEhFp9BVmamRhoiIiIhIhafQVZk5Kl2aXigiIiIiUlEpdFVmqnSJiIiIiFR4Cl2VmKWoe6GXQpeIiIiISIWl0FWJ2bxU6RIRERERqegUuioxi0KXiIiIiEiFp9BVmdmnF6JGGiIiIiIiFZVCV2WmSpeIiIiISIWn0FWJ2RtpaHNkEREREZGKS6GrErOv6VL3QhERERGRikuhqxKzefkA4KXNkUVEREREKiyFrsrMyxcAb/LLeSAiIiIiIlIaha7KzMcPAG9rQTkPRERERERESqPQVYlZvP0B8Lap0iUiIiIiUlEpdFViXr5FlS6FLhERERGRCkuhqxLz9rVXujS9UERERESkolLoqsR8iipdPqp0iYiIiIhUWApdlZiPvdKFKl0iIiIiIhWVQlcl5uNnQpevKl0iIiIiIhWWQlcl5ghdqnSJiIiIiFRY53TouuSSS4iMjOTyyy8v76GcFh/fAPNVjTRERERERCqsczp03XfffUycOLG8h3HafO2VLkshWK3lPBoREREREfHknA5d/fr1IzQ0tLyHcdp8/QMcl22FeeU4EhERERERKc0ph67x48fTtm1bwsLCCAsLo1u3bvz222//6qAWLlzIsGHDqF27NhaLhR9//NHjeePGjaNBgwYEBATQsWNHFi1a9K+Oo6Lz83WGrvy83HIciYiIiIiIlOaUQ1dsbCyvvvoqK1euZOXKlfTv358RI0awadMmj+cvXryY/PyS3fW2bt3KoUOHPN4nKyuLdu3a8cEHH5Q6jsmTJ/PAAw/w5JNPsmbNGnr16sXQoUOJj493nNOxY0dat25d4k9CQsIpvuqKyc+l0pWn0CUiIiIiUiFZbDab7Z8+SLVq1Xj99de55ZZb3I5brVY6dOhAkyZNmDRpEt7e3gBs376dPn368OCDD/Loo4+eeIAWC9OnT2fkyJFux88//3w6dOjA+PHjHcdatGjByJEjeeWVV8o89vnz5/PBBx8wderUMp2fnp5OeHg4aWlphIWFlfl5zoSCQiuW56vhbbGRetcGImrGlet4RERERETOJWXNBv9oTVdhYSGTJk0iKyuLbt26lXxwLy9mzpzJmjVrGD16NFarlV27dtG/f3+GDx9+0sBVmry8PFatWsWgQYPcjg8aNIglS5ac1mOezNixY2nZsiWdO3c+I49/Ony8vcjHB4D8fFW6REREREQqIp/TudOGDRvo1q0bOTk5hISEMH36dFq2bOnx3Nq1azNv3jx69+7Ntddey9KlSxkwYAAffvjhaQ/66NGjFBYWUrNmTbfjNWvWLHXKoieDBw9m9erVZGVlERsby/Tp00sNVWPGjGHMmDGONFtR5ONDAPkU5OaU91BERERERMSD0wpdzZo1Y+3ataSmpvLDDz9www03sGDBglKDV1xcHBMnTqRPnz40bNiQzz77DIvF8o8GDpR4DJvNdkqPO3v27H88hvKWb/EFjquRhoiIiIhIBXVa0wv9/Pxo3LgxnTp14pVXXqFdu3a8++67pZ5/+PBhbr/9doYNG0Z2djYPPvjgaQ8YoHr16nh7e5eoaiUlJZWoflV19umFBZpeKCIiIiJSIf0r+3TZbDZycz2/6T969CgDBgygRYsWTJs2jXnz5jFlyhQefvjh034+Pz8/OnbsyJw5c9yOz5kzh+7du5/241ZGBRZf81WhS0RERESkQjrl6YVPPPEEQ4cOpW7dumRkZDBp0iTmz5/PrFmzSpxrtVoZMmQI9erVY/Lkyfj4+NCiRQvmzp1Lv379qFOnjseqV2ZmJjt37nRc37NnD2vXrqVatWrExZkOfQ899BCjRo2iU6dOdOvWjY8//pj4+HjuvPPOU31JlVph0Y+wUNMLRUREREQqpFMOXYcPH2bUqFEkJiYSHh5O27ZtmTVrFhdccEGJc728vHjllVfo1asXfn5+juNt2rRh7ty5REVFeXyOlStX0q9fP8f1hx56CIAbbriBCRMmAHDVVVeRnJzM888/T2JiIq1bt2bmzJnUq1fvVF9SpVZg8QUbFOTnlfdQRERERETEg39ln65zSUXapwtgxwsdaVK4k3W9P6Zd/6vKezgiIiIiIueMs7JPl5S/Qi+zpqtQlS4RERERkQpJoauSKyxqpGEtUOgSEREREamIFLoqOWtRpcuq7oUiIiIiIhWSQlcl5whdqnSJiIiIiFRICl2VnD102QpU6RIRERERqYgUuio5mz10FarSJSIiIiJSESl0VXI2L7P/mU3TC0VEREREKiSFrkpOlS4RERERkYpNoauSs3mb0EVhfvkOREREREREPFLoquy8/c1XVbpERERERCokha7KzlHpUugSEREREamIFLoqOYtPUaVLjTRERERERCokha5KztvXdC+kUPt0iYiIiIhURApdlZzFPwwA3/zMch6JiIiIiIh4otBVyVmCqwMQVJBavgMRERERERGPFLoqOe8QE7pCCtPKeSQiIiIiIuKJQlcl5xNqQleYVaFLRERERKQiUuiq5PzDagAQTjrYbOU8GhERERERKU6hq5LzD48GwAcr5KSW72BERERERKQEha5KLjg4hExbAAB5GUfKeTQiIiIiIlKcQlclF+znTYotFICc1KRyHo2IiIiIiBSn0FXJ+Xh7kWoxe3XlpqvSJSIiIiJS0Sh0VQHpXuEA5Gt6oYiIiIhIhaPQVQVkepvQZVXoEhERERGpcBS6qoBc7xAACnMyynkkIiIiIiJSnEJXFWD1CQSgMDeznEciIiIiIiLFKXRVATafIPM1L7ucRyIiIiIiIsUpdFUFfqbSZctX6BIRERERqWgUuqoC32AALApdIiIiIiIVjkJXFeAdoNAlIiIiIlJRKXRVAb4Bpnuhd/7xch6JiIiIiIgUp9BVBfgGmtDlVajQJSIiIiJS0Sh0VQH+RaHL16rQJSIiIiJS0Sh0VQEBwaEA+BbmlPNIRERERESkOIWuKiAwyIQuf5tCl4iIiIhIRaPQVQUEh4QBEEAO2GzlPBoREREREXGl0FUF2EOXD1ZshXnlPBoREREREXGl0FUFhIaFOS4fz8oox5GIiIiIiEhxCl1VQGBAIPk2bwAyM9LLeTQiIiIiIuJKoasKsFgs5Fj8AcjKVOgSEREREalIFLqqiNyi0JWt6YUiIiIiIhWKQlcVkWcJBCAnW5UuEREREZGKRKGrisj3DgDUSENEREREpKJR6KoirD5BAKSnp5XzSERERERExJVCVxXh7R8MQL39P2mDZBERERGRCkShq4pIbjAMgFYZi2H/3+U8GhERERERsVPoqiIsHUbxV2Erc+Xg6vIdjIiIiIiIOCh0VRH1qgWx0tYMgILEDeU8GhERERERsVPoqiIignzZ59MAgPwEhS4RERERkYpCoauKsFgsWGLM9ELv5G1gLSznEYmIiIiICCh0VSm3DOvPcZsffrY8EvZuK+/hiIiIiIgICl1VSqs6kaR5RQCQnJRQvoMRERERERFAoavKyfYJByDj2OFyHomIiIiIiIBCV5WT5xcBQE7akfIdiIiIiIiIAApdVY41oBoA+ekKXSIiIiIiFYFCVxXjFWxCV2F2cjmPREREREREQKGryvENrQGA9/GUch6JiIiIiIiAQleVExAeDYBvXmr5DkRERERERACFrionNNKEruDCNPILreU8GhERERERUeiqYkIiawIQSQZHMnLLeTQiIiIiIqLQVcXYG2lEWjJJTMsp59GIiIiIiIhCV1UTFAWYSteh1OPlPBgREREREVHoqmqCa1CIFz4WK2lJ8eU9GhERERGRc55CV1Xj40eqfx0AbEe3lfNgREREREREoasKygxtAMB12+6DP54v59GIiIiIiJzbFLqqoILIxs4ri94Em638BiMiIiIico5T6KqCvKObuh/ISCyfgYiIiIiIiEJXVRRTv4X7gdVfgbWwfAYjIiIiInKOU+iqggIa9WZFrWudB+a/DMvGld+ARERERETOYQpdVZGXF7ZBL/FI/u3OY3sXl994RERERETOYQpdVVSNUH+mF/bkG9sQc+DIlvIdkIiIiIjIOUqhq4qqHuJHAT68kTvSHDi2F3Izy3NIIiIiIiLnJIWuKirE3wd/Hy+OEUZhULQ5eGRr+Q5KREREROQcpNBVRVksFqqH+AOQFdHMHEzaXI4jEhERERE5Nyl0VWE1Qk3oyvCrYQ5kJ5fjaEREREREzk0KXVWYvdKVYQswB3IzynE0IiIiIiLnJoWuKqxGqB8AaVaFLhERERGR8qLQVYXZK13HChW6RERERETKi0JXFWZf03Ukz1S8FLpERERERM4+ha4qrE5EIACJx33NAYUuEREREZGzTqGrCqsTaULX/mxvc0ChS0RERETkrDunQ9cll1xCZGQkl19+eXkP5YywV7oO56jSJSIiIiJSXs7p0HXfffcxceLE8h7GGRMa4Et4oC+ZmPCl0CUiIiIicvad06GrX79+hIaGlvcwzqg6EYFkKHSJiIiIiJSbUw5dr7zyCp07dyY0NJTo6GhGjhzJtm3b/tVBLVy4kGHDhlG7dm0sFgs//vijx/PGjRtHgwYNCAgIoGPHjixatOhfHUdVEBsZSKatKHTlZ4G1sHwHJCIiIiJyjjnl0LVgwQLGjBnDsmXLmDNnDgUFBQwaNIisrCyP5y9evJj8/PwSx7du3cqhQ4c83icrK4t27drxwQcflDqOyZMn88ADD/Dkk0+yZs0aevXqxdChQ4mPj3ec07FjR1q3bl3iT0JCwim+6sorrloQWfZKF0BeZvkNRkRERETkHGSx2Wy2f/IAR44cITo6mgULFtC7d2+326xWKx06dKBJkyZMmjQJb2/TRW/79u306dOHBx98kEcfffTEA7RYmD59OiNHjnQ7fv7559OhQwfGjx/vONaiRQtGjhzJK6+8Uubxz58/nw8++ICpU6eW6fz09HTCw8NJS0sjLCyszM9TXuKTs+n9+p9s8x+Nv6UAHtwE4bHlPSwRERERkUqvrNngH6/pSktLA6BatWolH9zLi5kzZ7JmzRpGjx6N1Wpl165d9O/fn+HDh580cJUmLy+PVatWMWjQILfjgwYNYsmSJaf1mCczduxYWrZsSefOnc/I458pcVFB/HdYSzXTEBEREREpJ/8odNlsNh566CF69uxJ69atPZ5Tu3Zt5s2bx+LFi7n22mvp378/AwYM4MMPPzzt5z169CiFhYXUrFnT7XjNmjVLnbLoyeDBg7niiiuYOXMmsbGxrFixotRzx4wZw+bNm094TkXVoHqwc12XQpeIiIiIyFnl80/ufM8997B+/Xr++uuvE54XFxfHxIkT6dOnDw0bNuSzzz7DYrH8k6cGKPEYNpvtlB539uzZ/3gMlUHD6iGODoaF2al4l/N4RERERETOJadd6br33nv5+eef+fPPP4mNPfEaocOHD3P77bczbNgwsrOzefDBB0/3aQGoXr063t7eJapaSUlJJapfAnUiA9lLLQByVn1TzqMRERERETm3nHLostls3HPPPUybNo158+bRoEGDE55/9OhRBgwYQIsWLRz3mTJlCg8//PBpD9rPz4+OHTsyZ84ct+Nz5syhe/fup/24VZW3l4WfQq/BarMQvP1HOLa3vIckIiIiInLOOOXphWPGjOHbb7/lp59+IjQ01FFtCg8PJzAw0O1cq9XKkCFDqFevHpMnT8bHx4cWLVowd+5c+vXrR506dTxWvTIzM9m5c6fj+p49e1i7di3VqlUjLi4OgIceeohRo0bRqVMnunXrxscff0x8fDx33nnnqb6kc0Kzdt3Y/Fc9Wlv2kn1wI0GR9ct7SCIiIiIi54RTbhlf2pqpL774ghtvvLHE8Tlz5tCrVy8CAgLcjq9du5aoqCjq1q1b4j7z58+nX79+JY7fcMMNTJgwwXF93LhxvPbaayQmJtK6dWvefvvtEm3r/22VrWW8XU5+IUtfuYh+1qVsbf8kzUeeXudIERERERExypoN/vE+Xeeayhq6ABaMvYs+R75lefSVdOnYCep0hNhO5T0sEREREZFKqazZ4B91L5TKJaxWYzgCXZKmwG9TzMFn08p3UCIiIiIiVdw/3hxZKo/Yhi3KewgiIiIiIuccha5zSPW6zUoezMs++wMRERERETmHKHSdQyzhJZuWkHmo5DEREREREfnXKHSdS3z8OG5xb+tPxuHyGYuIiIiIyDlCoesck+8T7H5AlS4RERERkTNKoescU+Ab6n7g6E7Y9CMU5JXLeEREREREqjq1jD/X+IeCa++MP180X9tcCZd9Ui5DEhERERGpylTpOsd4B4Z7vmHDFEhYe1bHIiIiIiJyLlDoOscEhkSUfmP8srM2DhERERGRc4VC1znGLzii9BtT48/aOEREREREzhUKXeca/zDHxV8Lu1Jos2CrXrRpcuo+93OPbIMfboMj28/iAEVEREREqhaFrnNNgDN0/eh9AS1zv+BAp/+YA8UrXV9fZtZ6fXf1WRygiIiIiEjVotB1rnGpdNWMiiIXP7bkRJoDh9bD/FfBZjPX0/abrym7zvIgRURERESqDoWuc42/c5+u2jVrALDymMuGyfNfgaQtZ3tUIiIiIiJVlkLXucZlemHTujEALN6f437OoQ1nc0QiIiIiIlWaQte5xjfIcbF9wzoAbE5MJ7fNtc5zDq13TjEUEREREZF/RKHrXGNx/shrREXRqEYwNhtcuPcqnsy/GYD8hA2Qm+F+P6v1bI5SRERERKTKUOg61/j4u1z2o3dTs65r15EsNlgbAGA5vBEyD7vfb+Jw+OZKKMg7WyMVEREREakSFLrONXW7Qlx36DAagAcGNKVhddNIY5utLoU2Cz45yZC4zv1+exfBjtmQtOlsj1hEREREpFLzKe8ByFnm7QM3/+a4Gh7ky4/39GBzQjqfLNzN7t21aWI5CDv/8Hz/nLSzNFARERERkapBlS4hLMCXrg2jiA7zZ4stzhxc963nk3PSz97ARERERESqAIUucagRGsAWa70Tn5Sr0CUiIiIicioUusQhOtSl0gUQ1QTuWwuDX4YWw8yx4l0NRURERETkhBS6xKFGqD9LrS1Z49sBmgyCq7+Bag2g2xgIqm5O0vRCEREREZFTokYa4hAd6k8ufozxeool1w1wvzEgzHzV9EIRERERkVOiSpc4RIcFAJCQlsOoz/7mxV83czD1uLnRvyh0qXuhiIiIiMgpUegSh9rhAbSpEw7Aoh1H+fSvPTzz40ZzY4A5rkqXiIiIiMipUegSB4vFwmc3dKJbwyia1QwFYP72IxzNzHWpdBWFLpsNjmwHa2E5jVZEREREpHJQ6BI30WEBfHd7V2Y/2Ju2seEUWm3M3JAI/iaEOSpdqybA2M7wbnv4qA9kHC6vIYuIiIiIVGgKXVKqgS1qAjB5xX5m78o2B+2Vrl8fMF/T4iFxrQlhIiIiIiJSgkKXlKpJdAgAmxLSefevokpWbjrsX17yZL+gszgyEREREZHKQ6FLStWoKHQBpFMUqjIPw1eXlDw5//hZGpWIiIiISOWi0CWlqhflrF5l2FwqWXmZUK8n3LHIeUyt5EVEREREPFLoklL5+3g7LmcQxFFbUQfDYe/CDb9ArbbQ70lzLDejHEYoIiIiIlLx+ZT3AKRiqxHqz5GMXKx4MTLvBcL94Ie2owjwKsrrjq6GCl0iIiIiIp6o0iUnNOn2rtw3oAkbnh1Eun8tNuXWYG9ylvME+/5duRlgtcLsJ2HjD+UzWBERERGRCkihS06oUY0QHrqgKaEBvtSvHgzAvuRsx+1HC/wAsOWmw+55sPQDmHqzCWAiIiIiIqLQJWUXV80009iXnMXyPSn0eHUe90/fBUDqsWTISnaenLKrPIYoIiIiIlLhKHRJmdm7GS7dlcyVHy3lYOpxMm2BAFhz0k07ebsDK8tjiCIiIiIiFY5Cl5RZvWpmeuGf2444jmUU7d8VaM2G9IPOkw8WhS6bDXLSz9oYRUREREQqGoUuKbM4l3277DKKKl0B1mxIO+C8YcuvkJcFP98D/6sHR7afrWGKiIiIiFQoCl1SZi1iwggNcN9lIBMTurywQvJOlxsOwd8fwpqvwWaFRW+czaGKiIiIiFQYCl1SZuFBvix8pB9Xd67rOHYcfwpsRb9GR7aar+2vM1/3r3DeOTvlLI1SRERERKRiUeiSUxIZ7MdzI1rxyOBmRUcs+FiKtYeP62a+Zh5yHjuu0CUiIiIi5yaFLjll/j7ejOnX2PONYbEQ3dJcTk90Hs9O9ny+iIiIiEgV53PyU0RObFzBcAb7rKHRLV9AtYZQkGNucK10ZR9zXrbZwGI5u4MUERERESknqnTJaevROAqA1wquZkj+a9hiO0FwFATXKHlybhrkZsCBVfBKLCx68yyPVkRERESkfFhsNputvAdRmaSnpxMeHk5aWhphYWHlPZxylZqdx6yNh3h82gYARnWtR0ZOPr2b1uDSub1LTikMrQ0ZCc7rz6adxdGKiIiIiPy7ypoNVOmS0xYR5MdVLp0Mv1q2jx/XJvDKb1uxhdQseQfXwAVwbO+ZHaCIiIiISAWg0CX/iMXD2qwjGbksPuR98jvvXnAGRiQiIiIiUrEodMk/1r2RWdv1xU2dCfA1v1KHbRGeT46oB13vNpf3LDwLoxMRERERKV/qXij/2PvXnMeRzFyax4RROyKQ3UeySLBFeT45rhs0vwiWjTOhS50MRURERKSKU6VL/rGoEH+ax5iFg7XDAwHYbo31fHJINMR2Bp8AyEqCI1vP1jBFRERERMqFQpf8q2LCAwDYaotzHLN5+zlPCIkGH3+I62qu71nkvG3Tj5C4/iyMUkRERETk7FHokn+VV9FMwT22GMexQotzFqstONpciOtmvh5cab5unw3f3wAf9TobwxQREREROWsUuuRf5edjfqUKXJcLFuQ5Lh6xhZsLdTqarwdXm69bfnaer63jRERERKQKUeiSf9WdfRoRExZAgK8XPxSaqtWrhdc6bt91PNhcqN3BfE3eATlpcGS780GKb6osIiIiIlKJqXuh/KtiI4NY+p/+5BZYeef3WHr9tYJUWyhPeU8EYHOqD92AzWm+RPvWonp+Iuz6ExLXOR8kIxGCq3t+AqsVvPRZgYiIiIhUHnr3Kv86i8VCgK83V53fkP22mmQQxNiC4XxccBGrU/zZkpjOhe8t4tfjbcwdvr8BCnOdD5Ce6PmBE9fBa/VhyQdn/DWIiIiIiPxbVOmSMyY2MtBx+fWCq82FDYnM2GBC1VsFV3Cxz99UJ839jhmlhK5ZT5ipiL8/Cd3vORNDFhERERH516nSJWeMr7f7r5ePl/smyOkEs9DSueQd45fBb49Darzz2KoJsO+vMzBKEREREZEzS6FLzqinLmpB29hw/ny4Ly9f0qbE7Qtymzoup/oVtZlf9y38PR7GdoV9S0wI++V+9zt+0Fl7eomIiIhIpWCx2dSf+1Skp6cTHh5OWloaYWFh5T2cSmfZ7mQ+XLCL+duOAFCLZJYG3AvA5IK+XOUzv+Sd2l4F6yeXPF6/F1z9DRTml954Q0RERETkDClrNlClS86qrg2j+GhUR966sh3t6kaQSBR76l7Cvmo9mVh4Abm2omWGwTWcd/IUuADys+GNZvB6I8jNOPODFxERERE5DQpdctb5+3hzaYdYGkQFAfB746eZ3PRNNtka0DP3PRJHfg8PbID+T534gXwCoeC4uXx0xxketYiIiIjI6VHoknJTLdgfgJSsPPalZANwhAh2BXcA30CIaXfiBzjqsqFyQc6ZGqaIiIiIyD+i0CXlJirED4DkrDzik7Mdxw+mFl2udZLQlZXkvJyd8m8PT0RERETkX6HQJeUmKtiErqmrDrDhoHOvroPHiqYMhtaE6kXdDS9+G4C11oa8z9UlH+y4QpeIiIiIVEzaHFnKTbWi0FXc9sOZ/LIugQta1iRg9M+Qmw41mnHTgmCWHvHFihf3Bkxyv1N28lkYsYiIiIjIqVOlS8qNfXqhXbu6EQDM2nSIe79bw8Pfr2PqjkJsRdWufdQkB3/y8IXASPcHU+gSERERkQpKoUvKjb2RBkDb2HC+u+18aoQ6j/26PpGHv1/Hwh1HAcgvtDrvHBLj/mBa0yUiIiIiFZRCl5Qb1+mFdSODCPLz4dPRnaherAK2NTEdgPwC5z7etvBY9wfLTgabDY5sA2th2QZgs8G8l2D5J6f3AkREREREykChS8pNWIBzSaE9gLWrG8HKpy5wO2/i0n18tGAXh9KdbeHzYjq4P1h2Mmz5GcZ2gd8eLdsA9i2Gha/BzIfBaj35+SIiIiIip0GhS8qNxWJxXK4dEeh22+uXt3VcPph6nFd+2+p2e3qN4qErBf56x1xe8WnZql3xS52Xc1LLMmQRERERkVOm0CXl6s4+jWgeE8p1XePcjl/RqS6fju5U6v2OhLVxP5CdDBF1ndcPrDBfl38Ck6+HdZNLPkj8387Ls/4Dhzae6vBFRERERE5KoUvK1eNDmzPrgd6EBfiWuK1utaBS77cp2crhmr2dB3JSIXm38/qO3+HIdjN1cMsvMOMh9ymENpt7pWv9JPiwxz94JSIiIiIinmmfLqmwYiOdUw79fLzIK3CGpkemrseL2wnkBuYHPUYN61E4vMF558T17g+Wlwlp+yGynrmenWKOiYiIiIicYap0SYUV7O/8TGDqnd24oVs9fLyc68CseJFFIJvy65S88+GNsHGa+7EjWyFlt6lyZR7y/KT5Rc06tv8OKz//py9BRERERESVLqnY5v1fH45l59E2NoK2sRHk5FuZvHK/2znbbLH0ZZ37HTMSnZcbDYBdf8DUm011a8AzUKu95ydM3Qc1msG3V5jrdTpBrbaezxURERERKQNVuqRCa1gjhI71qjmuRwSXXPu13erSQCMoCqo1dLleHeK6msv26YR/PG/ClScpeyA3w3k9Nb7sgy0sgGOlPK6IiIiInLMUuqRSaVQjpMSxNbbGzitevlCrnfN6VGOIblHygf7+yPMTHNsD6S5VsuPHyj64qTfBu21hx5yy30dEREREqjyFLqlUOtaLLHFst60274c8AD6B0P5aqOfShTCqMTQZBF3HwEVvQY/7zfEjRft++Qa7P1jSFvepiXsXwRtNYe6zJx/clp/N1yXvl/n1iIiIiEjVp9AllUrD6sEej3+S2R3b4/Ew8L9Qv5fzhoi64OMPQ16GzrdAo/7udwyNcb++fgokrHa5PhkyD8Nfb5d9kBb9tRIRERERJ707lErFYnF2L+zXrAYz7uuJxQLpOQWk5NjMDTWaOc6JzzF7fS3fk8KRjFyI7WKmINr1uA+imkC3e6BuVyg4Dgvf/GeD9PL+Z/cXERERkSpFoUsqnTkP9uba8+N468r2tKodTu1ws59Xxxfn8umi3eRbbdyXdw8/FPbkrs0tWB1/jCs/WsrF7y8CvyCo7zL9sHpTuHclDH4Jmg0xx/IyPDwrkJNe+qBsNudli0KXiIiIiDipZbxUOk1qhvLyJW0c1xvWCOZg6nEAXpyxha4No/jZ2p2frd0hKY/Jy02L+cPpuSzcfoS9NR/n2tA6+ORlQJ2OAGTk5BMS2QBLyadzyjgEAWGeb8tJc162nPBRREREROQco0qXVHrF13lNXuG+j9fcLYcdl0d/vpxn/kzh4dzbyL1sAjk2H35cc5AOL8xh3HobJYTHQUhNc9neYOPwZtg6w/081y6HBTnut9k8PK6IiIiInDNU6ZJKr16Ue+j6apn7XlnJWXkl7vPj2gRW7D3mqJABfLDOypgAl5Nu/QNqNIdJ15pmGhmHzPHx3czX2xdA7fbm8vEU5/1cq15rvoFZ/4H+T8L5d5zqSxMRERGRKkCVLqn0aoT6lzhmscA1XeJOeD/XwAVwnAD3E+p0BP8QCKttri96A9Z+57w9aTNMvwvWTXavdNlD18HV8NPdkJsGy8Z5HkTiOshOgS2/QkHJcFjCL/fD9DtVPRMRERGpRFTpkkpvSOsYLu8YS8d6kQT4evHLukRu6lGfmLAAvlsef3oPWr+Xc21WcHXz9eh2+PFO5zlrvzX7eK37Fi791Hk8ZbcJR4HVnMdS90NetmnkYbdrHnx1ifP6RW+ZtvalyUmDVRPM5f5PQXhs2V6LzQb52eDnud2+iIiIiJxZqnRJpefr7cUbV7Tjmi5xXHJeLJ/f2JleTWrQqEYIXeqb4FMnItDtPpd2qMMzF7d0XH9sSHMA/s/7cWwN+8OlHztPrtbI8xMfWOm87Dq9EEw4WvKe87qtEBLWuJ+zYWqx69+D1Qo/jYHfHjfHkndBYYG5nF3KFEa7ha/DrCdKVsF+GgOvNTRh8ETyj8MXF8GC1098noiIiIicEoUuqbK8vCx8d3tXPh3dic9u7OR221tXtueyjrFEh/rTolYYN/WoT7CfNz9ktWV5z0+dUwoB2lwB/Z6CAf91f4ICl+mJvz1acgDWorBUp+i5Dyx3vz39oPv1wGqQuBbWfA1/j4eVn8P7HWDm/5nbXUNX1lH3++5bCvNehGVj3cOVzQZrvzHNPdZPKTlGVwdXwb6/4O8PT3yeiIiIiJwShS6p0ry9LAxsWZPmMSVbvYcH+jLv4b5Mv7s7Ab7e9G0eDcBd36zmu+Xx/LwuwZzoHwJ9HoF2V5/6ACLqQdOi/b/mPgsTR5rwZLOZ9Vyu0g/CviXO678+aL7apxRmJztvyy4Wuha6VKdcw1x6gvNyYGTp41zwOky9xfnYZVlfJiIiIiJlotAl54wXRrYG4KmLWjiOhfj7EOBrNjN+dHAz4qoFkZKVx3+mbeC+79awap9LdSm01qk/aVQjiKznvL77T5gyGtZ8ZZpvePmYLolQFLoWe36cglz30JXlctlmc5/q6Bq0Dm1wXs7PLv2x/3wRMg85j7leFhEREZF/RKFLzhnXnx/HX4/145aeDTzeXi8qmHHXdXA79vUyl0YcFgt0OkGji2smlTwWWd9Uu1ztXQQ/32su1+kEkUXjyToC22Z6fuykzaVXuo4fMx0S7dIPQtIW+KgPLPify3mpnh/bNaQ5jiV6PldERERETplCl5wzLBYLsZFBWOxdCT1oXSecyzs6uwJOX3OQ/5uyjgXbj5gDF73p3qnQVVRjGPau+7HIBu6VLlc128AlH0JQNffj9s2YXX1xIfz+pPP6ka2w6Uc4tte9YQeYwDTtNrM+LGG183hOqudxFF9bBpDhIYj9Exunwf7lJz6nsAD2LjYNPURERESqEIUukWLeuKIde165kKY1QwD4YfUBbvi8KDBYLBBex/MdI+pBxxvhape9vCLrew5R1/8AdyyEag2crent7vyr5PnFpwZu/gm+vwHebQd/ve1+W3oCHNpY8jGyjsIPt8GC19yPp3kIXaVVuvYtgQOrSnZIzM+BXx8ye5YVF/83TL0JPrvgxPuLrZkIEy50r86dSVnJkJd1dp5LREREzmnap0vEA4vFwoVtarH98A7HsczcAkL8fdz333Ll42e+hsY4j0XWLxmqbvgVGvRyP+YfBrnp0GIYBNcAv1DIyzi1QQdWM63rt83wfPvWX52Xu98LvkVt9NMPlDy3eKVr3ouQsBZ2zjHXez8K/Z80Le1/ud9MmQRY+Rm0vsx0UNz6KzTs6x6i0hOcoXXTdLOB9MDnwMsLdi8wx5O2mvb6ieugww0lv3//huOp8HpDE4gf3v7vP76IiIiIC1W6REpx6XmxBPg6/4psOli0bsp1OmBRZ0JbTFvSjuebY64NNyLruz9ogz4lAxfAtVOg820wYlxRNa2MGx+7iogr+7lJm83XY3th0dslb3etdOWkme6I9sAFcLCoccfUm5yByy5xHXzSD/54znzd9YfztsObzFebDb6/0UyN3DG76DGLpkJmJMKnA02YW/1l2V/TqTi03nzNPGwaiYiIiIicQQpdIqWIiwrij//ry/kNTMjaYA9dLq3XEzo/zjt13uDGwqdo99zvTFi8B8JqQZ/H4YLnISCM43mFLOwyntzYHm5rvvIKrNjs0+3qdYOL3oCAMHYdySTXO9hx3ro2T3ge4IVvuF/vfKvphlgWW36FH241zTY8VdQyioWu4nLSTbWoeNt7MB0a8zI9P+/hommPGS7dEdMOQGYSpBU1LUmNd+5xNus/kHH4pC/nlFlc/unLPAOPLyIiIuJC0wtFTqBORCA9G1fn7z0pLN2VjJ+PF9PXHGR07KPsSTzKe58lALUBExKe/WUzHy/czTe33UdMWACBwP9mbWXCknB8vO7h27RwulSDvUezGPb+X7SrG8EnozsR6OfN18v2MWvjIf7aeZSJvrn0Np3sGbGiNZe2n89bW/uaAzXbkFynL28c6Mrlo7bRsaa3qTw1HWKaduycC4vfA06wfuqvt078wl07Gnrqepibbhp1gKnmXfA8bPgetvzivtdYcX88B75BpumIXeo+Z5ULzBRJu/xs+LgvPLgRvIq+Icf2mbDXYtjpTz3MdQmaGYcgONo8Zmwn5/OcCVt+BZsVWg4/c88hIiIiFY4qXSInMbRNDBYL/LE1iWd+2sSa+FQe3Nme97IGejw/IS2Hfm/Mp+OLc3h99la+XW4qOAVWGy/O2My+5Czu+GoVGbkF/LXzKP/92VR/nvpxI3/tNK3gUwh1e8xpaxPg9vlw5UQOXjOHjku68d3yeN6ctxdCakCzoSaANOhtAtB9q/lHMhKdTS9Kq3QlrDGXa58HLUfA+XeZ68k73M+NbgmjfnReX/GJs+IFcHSne5fFEmNJcB/D15fClFFm6uHMR2HJ+yXvs+lHEzytVnO9sMA09CgsKPmaMhLNRtSfD4LlH5c+jn/qeCpMvs6M3dP3VERERKoshS6Rk2gcHcqIdrU93tYhLoLPbuhEu7oRfHFTZ8b0a+S4LTuvkLF/7iKvwLzx9/P2Yv2BNPq8Pp9th52VlqmrDrDhgPub8DcLruC4dyhjC1wqIkXhZvFO5x5djimPxVVrWPYXOOC/cMtc92MFOWb/L6vVc0DILRa6wNkaPzXe/dwazaFRP7higrmevBN2uKwPS94BB1eVfI7zRoFv0TRL13b3yTvN11/uh+Ufwe9PuXdFLMw3nR3nPA1LPzDHFrxqQtWiN4seL915fsYhWPetuWy/3VoIvzwAa74uOS4o6nxYymbTpbGvZwOzlu6fOlEnSDDdKrfOgBWfwuwnzfdFREREyoVCl0gZPDe8Nf8Z2py3r2rHtLu7O47/58IWDGhRk5/G9KBfs2hGda3vuO2aLnVpVtNUrO7o05Dbe7sHoQvbxNC3WQ2sNvi/79e63bbfVpPefMbrBVeXGMu2Q87AlpFTQEpWnudBXzEB/ELcNm3+rbAzv1r6miuhteCSj00nQ9cmHPbujDP+D16uBTMfLvnY+dlmTy2A2h2cj+flW/Jce8v8VpdAbGdzeZ9LW/yUPaYKBeAT4DxeozkERpjL9imOrmHJlWvr9+RdzssLXjMBauHr5vr8l4sex3UzaZeplN7+sPAN+Kg3rPoCfhpT8rl2zoW3W8EXQ0y7/ZmPmGrdybhW91L2nPz84mw2Z9DaMRdea2i2DijNZ4Ng0rXm57j0A1g69tSfU86O48cg80h5j0JERM4grekSKYPwIF/u6GOqWDabjas61SUrr4AOcZFu58WEB/DAwCbsTznOc8Nb4+ttITEth5phAWTmFDBx6V7Scwr44qbO9GsWzer4Y8zfdoTth0s2njiSVeB2/dXftvLgBU3cQhfAjsMZnN8wir1HszieX0iLWmHmhlaXQIsRph17u2tI3bGUp1JuJjvfnwsGdMa/7aVQs6U5N7Qm3DgT/IJN0DieApummdtcm2oMeAb+eN5czj5qQlZsJ3Pdyxsi6pp28a5c10g1GQwHVpjLTYfA/r/NG868DPD2M01K7M/XqD+s/cZs3pyTZlrJ55bSRj8nFfzNvmokuVSU8jLM4xeX6xK67OMB0z5/3gvu5xbmm+pajebm8teXmeOJ6+C7q00nxL2L4e5ia9nSE0wVrusYiO3o7JgIp17pslphwkWmAciNv8L0283PaMpoeLaUamfKLvfri96EHvefmRb85e3wZrNdQe9Hze9yRbN1BqTuh653lrzNaoX/1TeXn0gEv6CzOjQRETk7FLpETpHFYuF/l7ct9fYHBjZ1u147wuyHFR7ky7e3deXAseP0axYNQIe4SHo1qc6iHUfd7jP1zm5c/uFSt2MfLtiFn48XmxNNtadasB8pWXlsO5xBm9hwLh2/hMzcAn6+pwfv/7GTi9vWYmibovb1l3zIvZ8uIzklGYDNzcbQrkaEe6m7fg/zNbSWe1XGrtMt0Ov/YOGbkF9UWarTgR83pvDF4j20rxvBcxH1Soauej2cl3vcZ/bpSo2H8+80097WFk3hq9fd7Fe25WcIq2MCYUCEuW3Fp2bfr/C6JccFJliFx8LKz836LFfFX0tetnula99iz49p92YzyE423SIb9nW/zR6kXIMemLVjX19mWvPvX24agbhuWH3sJJUuayHsWQBx3cE3wHR2jC8KddnJZjNqV3OeMR0gh38A3qX8s56bDllHICT6xM99pmQegXXfQftrIbj6v/vYX19m1v4d2WZCaUVis5mKI5jtImq2cr/dtdNnRiJENUJERKoeTS8UOYta1wlnSOsYt2P/GdrCcfmaLnX57f5edKrveQPm9/7Y4ZhOeHFbE6jenbuDDxfsJiUrj7wCK8PfX8yMDYnc9c1qrFYbOfmF7EvOYsU+Z8XnknFLeGduKZsCh7nsM+Yf5rwcEF701XnMWrcrD0xey7oDaXy5dB/5YS7TFOO6waWfmCYfdj7+5k1338fN1EHXLn49HzRNQLrfZ5qGgHN6oX1j57T95muL4e4B6MOesG4S/PZ4ydeza5779aPbSp+m6Em2CaokrHWuJyvO16U6kZ0Crzdy7oWWtt+EsKQtznOKV7ryj7vvjbZ+Mnx1CXx7ZdFjuGxgnZ3stm0B2Smw+F0TaOwbY5e23qv4eruz6YebzTq76R6qPf+UfTPv4nvGVQRZLh+oeKq6un4AYLOe+fGIiEi5UOgSKWcta4fx32EtaRIdwj39mzimB7aqHVbqfYa2juH23g2pExFIclYe7/3h7BiYV+h847Yq/hg3T1hBn9fnk5Pv/obuvXnOALH7SCabE4qCSGhR05CwOtDNZU2TPXS5BLGMiOZuj3k0sL7zSuOB0PbKE09na9gPml0EHW80G0dXawCDXnBWY+zPWVx0Cxj9E9Tt6jw2/Q6wFTqv29eXLX7X/b4HVp5e98C1X5vphJ6E1DSh7/1O8EYT98YfgZEmrBW6bMK8ewH8cJtpsQ/w5TB4q7kzjK2fYr7uWQAbpsKPdzvvm3XUNDqx2+uyPm7zz+Zraa+vPEPXnoXmq+sm2xXZyRqVlFWay/fcde2hnevPytPtIiJSJSh0iVQAN/VowJyH+lCnaCoiwMejO9G3WQ238y7tUId5/9eH8dd3JDYyiPHXdzjh494+cSVLdiWXevvh9ByO5xUycuxiRoz9i4Opx/k4pR1bbXH82eD/zB5cdh4qXUf9Yt0eb22N4TDkVej/NHS+5SSvGvDxg2u+NZtGewpn9umFxdXpaL4GFrvdvqny0Neg+UXO48HR0OV2c3nRW87NmYP+pWluuemmfX3yDucY7I4fM2vXwKwL8w0CbLBhCky+3jTDsK8r2/Sj+eoaNn+4xexlZpeeYNbT2bk209g+y3RJnHSd53HaK4Vni80Gy8bDzj/Kfp+/P4LvroXcUjbYttsxx9lBM9jl78mab4r2qfsHkneZauXCN05+7smkunzPs1NK3u4auvJPsSNmWVmt/16IFBGR06LQJVJB1YkIZMJNXbizqIHHJ6M78daV7WlYI8RxTtvYCLo3igLgueGt3FrWAxzLdm8T3rB6sNv1hduP8M7c7aTnFJBfaOOGz5fz8gobQ3Jf5b3E5iTYnNMcf9iSic1mA4uzMcZBL/epkjuP2aDrXdD7YccUuFkbDzFn82HHOYVWG/uSy/iJfmmVrkb9zVfXaXZ2vkEmYLm+EW86GC54AcLjitb+FE31u/gtaH0ZXPdD2cZj5xfifj072b05B5hGIf5F498+y3yt1929egimGYZd/nHTrMO1o2Jxrg05ADZOdV7Oy4Tx3d27Q7pK/ZdDl81mGqusmmACRfE39vv/hlmPm73VTmTvX/BGMxMgf3vUTJP8+0PIOOy5eUriOvjmcrNxts0GQVHO236620xj3L8cvrkCNp7izxZMA5Ts5JJNVU5VajzMf8V5/WTTC091G4KyyE6Bd1qbSrCIiJQbhS6RCu7Rwc1Y8nh/LmjpuSvb2Gs78O1t5zO6Wz0eGNiURwY344sbO9O1oQlMDaoH075uBFHBflzUtpbbfR+Zup6PFjobX+xMclYX1sSncvVk51qiH7dkmtuznZWzhByXFu/AnqPuYepoZi53fr2K2yauJCkjh2/+3sf/TVlLn9fnM2fzYcbP38Wrv23Fai3lU3jXSpZ9umCbK8G76LKn0BVe11TNXENXjWamIYXrGjIw+5ld/jk0cdnourHnTa/d1O1y8nMCIpyt+LfNNF9j2kDPh0wTkSsmQEyxhiwLXoXPLihb6IpsADValH6eJ6nxZm3ajIdLNjxJTzANOqzF1hWl7IZj+9yPbf4JfrjVBJtFb5o9015rALP+4zzn26vg88Gex/FWK9Nu3/XczEPuAXTTj/BuO/h8iPNYfo6Zmul6XvpB9+mWdj/dAzt+h6k3m+trvzMVNPvrzs+Bv97x3O4/M8l5ee9imHaH58AEZo+5P1+BAg9bN/x0DxzZ6rx+3FOlK9Xl9Z2B6YUrPjXfo/WT//3HFhGRMlP3QpEKzsvL4uiA6ElksB/dG5lpcr7eFsb0awxA4+gQvli8l5t71qdmWABWm42CQhtr96dSLyqIr5edfH3PIZdKlzdWFmw/QpMs535CiWnmzW5ogA8ZOQVMW3OQpjGh3NmnEQmpx/lyyV7HuU9N38jvLhWvhyavJSPXTMVrEh3CpR3qsCkhnWYxofh6F30e5Dq9cOQ404ijqcubcE/TD+2NQFw75FUv6igZ1825YTK4Nwq59BNY8ZnpAJh52FRL7OuQXHW62Txe8QYdxQVGmhb6hzc4j9XpaFqCD/2fuX5sb8nKlX3KXGkSi86PqAshMc6q3YlYvM16tx2zzR+AwjwYXjQNL3kXjD0frEWV0YveMtNDM5PgvaLNr32Dzdq7vo87Q0/xDZf/Hg9DXzWdCu3VPU/SD5hzzxtttinI8zCV0P59O7wRjmw3XS3/fNl93R6YBiKuIcnu6Dbn5cICc9+0eFNFq9HcGYa2/Ay3FftZ5ro0Wplwofm6Yza0uwYGvWS2YbD7pKjq6uMPvR5yf5w9C9yv/5NK16bpEFEP6px4SnEJrqHPWui+hUNVcnC1WTvZ9sryHomIiEcKXSJVVN1qQTwzrKXbMX8f+OqW8wHo2bg6n/21hzv7NKJpzVBSsvJYujuZrg2jGDnWtFHPw7nZ8VZrXaw7jnKLXwiWok/n3y1q4DG0dQxTVpqq2Lg/d9KwejBvz93BlkTnm1fXwAU4AhfA67O3kXY8n+d/3cwdvRvynwuLKjiu0wujW0JMa/cX6Wn6ob1ZRJBLB8jqTczXuG5up87YnU/jOhk0iwk1b9bsb9jCakG1RiVDV0gMXPy2c+0VmM2k7RWMul1h/7Ki45Hub3DD65asbNUs9nrKwv5cYXXK3v69Tkc4sNz9WPwy5+Xts5yBC8ym0p1uhg3fO4/lZ8GysRDqMqX0kEugtCssOHHgskvdD2M7l238W38tfarfvBdPfv+UXe4NLVyDyMFVJc/31N3y+DFYNs6sFazfs+TtO+eWDF1+oWavONfHKPFcrmu6PFS6Nv9sqlT2Dp6l7ctWmqPOJjvkpLn/vQBY8oHZxqHVJaf2uKX5+2MTNi//3ATRE7HZIHEtVG/2z/cn+6Sf+RpSExr2+WePJSJyBmh6ocg5akjrWnx/Z3cGtKhJ3WpBtKsbwZ19GtG+boTbeZ1zxjEw9zUOEcVfO47wvP9DbLfWYVSesz176zrhrHn6AgDScwq4/atVboHrZA6l5/D8r6bF+kcLd1NQ1IFxS6LLG8xqDUjJyuPndQkU2qcjurbYblrUmr6rvdOfS2OOiHrma3CU6ZjoF8LqLm8z5vttDH7HQzULTCXILrSoemafnthiuGkWcuNM9+lhtdo5LwdGQOdbndcbDyjZLCTaPRS78XIGXup0hEs+cr89rI6z02Rx511f7HlaQGyxgHN0u/MN/95ia8AyD8GfL8HcZ0s+tmtzieIbMAN8dxX8fI/ncbnaOffk59idrKp4MsVfn6vQWlDg0lnSZit9KiGYqZZLPjBhzbUyVfw+hQXOCp79Z+naSOPgKjNtMdnle1i80mUthCmjnIHLPr4TyUkzXTELcs0aQdetClymBjvG8PuT8P2N/956st8eMeM90XTGjMPw6QVm+ujHfd2ni54O1+9JRdw2QEQEhS4R8aB5TCgATWuGEBtXn7SQRvRtVgOrDb7YH8OgvNdZZHVWbaJD/YkM9jthm3u7e4qmPwJUD/HjgYFNSpyzdHcy36/cz92/Oz/5T8rx5snpG7jvuzXOPcZcOwVe+SVcPw063sSfW5OYltHCTAPscIN7xen6afDITiaknec4VOhhTVlBLeft3PAL9H8KBvzXXPfyMs1C6vdwD36uUxoDI00L/Ms+g9gu0OOBkt+MsFJCE0At16qYxb1ZhP2+rnuq2deiWbzg4nfh2inO2/xDYeCz5rZGA4pCqM1Uu3LSnRtE3zYP2hcFtoWvuwdPu+INQ4rzFKZcu2DaeZpSaOcaOKHkBtdtroSR4088jpONyS4jEV6s6Qx2GYfc2/sX99PdJqhMuNi9G2RqvPt6uKwjgM18z6/6yhxzDWaf9If1k0wXS7vi3Qs9tfh3bSu/cy4sHeseOqbeYrpiLnoL9i1xr2AW755on6oKEO++GbvbfYpPIy2N67o2T1M+wYz1iyGm8mrvyrlzzul3VyzIM2sLT/a8dtZCWPah5yrt2ZKdAtt+M8HcVf7xk9837SCs/sr9gwIRqRQUukSkhI9HdeL6rnF8fcv5fH9HN5Y+3p/XL29H42jTta9bwyim3d3dcX5spJkadFuvhm6P4+fjxYj2tQkPdL6Jvq13Q0cXxUs7xHJTjwZ0aVCN6FDnVKQP5u3kiekb2GOrxZW5T9Mn9y3enruD3zaaVu/vz9vJn9uSzEbLITHQ+TZmbkmh6xSYvzOFmyas4KEfd7Jm+O8w/D1sNhvrD6SyZOdRbBYL+Aay/5jzDW5imnmz89KMzQx6ewE7kzLo8lss30bcCXcuNtMTez8C/sW6Fhbn2tjDfrnN5XDrHBPAirNYnOfFdTPVs8u/gPq9XCp2mGBXPHSFx7pXuhoPhBtnwH1rwNsH4lz2MPP2M1Pi7lkJV06E2E7m+LdXwqt1TXXENxhi2kGfR5z3q9kG+j154tdcFjf8CiPGQvtSWtkXZ98SwK54Fck30EyHK2sjEXsjE9fpqG7ruGxmM2ownRHLIj/bvcFIXiYsd6lGZhZtSxAc7dya4HiKaWRS2nMU36fL02bc9umlNht8fRnMfsJ9Gqx9H7RVE0pWCFN2wfJPnN9P1/WDu+eXfK6MQ/BWS9MpEsy6qZmPlh5s7K8ZSm9/n7qvZBMXcJ8GeSJ52fBhL/jlAXN91uPw+SDn7ckeqq/HU50NUzZ8D7MeMxuql5dJ15o9/5a+7zx2YBW8UtesPTyRT/qZSvKKz87sGM9FNpv5u1mW8CtyGhS6RKSEuKggXhzZhuiwAHy8vfDx9qJGqD+/3tuTT0Z34qPRHekQF8nk27vywohWjgrXiPa1mftQb+Y+1IdHhzRjw7ODePfq82gb63yzGx7oy809G3BeXAQ39ahPeKAvU+7oxvInB/LtbWa92d97UsgvtNGrSXUGDr2EfbYYvlvu/qn/TV+s4KEZB5jWbw5c9AZ3f7OaQ+k53PjFCsc5D05ey+aEdJ6YvpHhHyzm2k//5ofVB8nJL2TjQWfFZl9yNsfzCvlk0R62H87kwcnrSMmx8sSh3myy1nV73u+Wx/Ppot2mfT44pxBe8Lx7MCptj7Hi7lgIl35qAlPvh6H1pXDjryasuSoe2opXugLCTbCyV5WKt7UHiGpkgqPrNEi7Wm1NWIusb6pzjfrD1V+XrVNjcTfOMJW20KK1cWF1zJTHEWPhoa0miJxI8dBVXIcbTPC6/iTt4Id/4H69YV/oMNqMpXYH8HHvvonN5r6O7WRc908DEwDs0/kyitYwhtZ0BuvUePi4D3zUu/THmzIathaFRE9BJDvFBKdX6zmPHbVXfl2ajATXKFnhm/0EzHwYPh1owovr2kRP0/K2/AIFx00gS08w3SmXfwQf93N/LjvXrpulbU/gaQ0dmDb9vz7kPs0xdT98cyXE/+2shG35xTSfWfWFObayWPg4vNF8395pA3OeMef8cCt80NFU/g6udp7rqePk6bIWulc6138PX1xomsoUZ68qLv/EeWzWY6YqueB/pT+HzWaa/ID7lNnDm05vw/eT2TrDfEhQXMKa8q0UnikbfzB/N6fdXt4jkSpKjTREpMwCfL3dWtef3zCK8xs6g4bFYqFxtJma2DjaOY3wv8NaMuKDxQxvbyoz13etx/VdXd402h+vQRS+3hbyC22O+1UP8WfuliSW7ynZbnva6oNMW32QeduOlrgNYG9yNhe+5/5m8sc1Bwnx93Y8B8AzP21k1xFnlWGDSyC76L2/eGxIc+7s05DthzP5zzTzZiMpI5cnLmwBg18hp+WVHApuwfc/fIejTuSpnb0nEXHO1vIuNiWksTv8Oi5On4RlyKtF3RDrOd/oh9Ux0wbtir8JdplSabXZ3D9h89TAw7XJR5vLnaEvoh4MfsVU+45sNW+Oh/zPvEm063yreeMXv9Q0FrE3mrhnpRmHvdufxWKCYo1mkHWCaWC1z/N8vPFAM5YaRd0oXZtC+ASagAAQ1dhUJttdDUved3YyDK8Lg19y3icoyrRTt0tY46yKubpmspkKuGm6+3H7HmBd7zadKLfNhHFdzQbh9mmnITElm1eUxr7R9eaf4PH9ZrNtgJ4PwvbZkLTZVLpmPux+v7wsU3Vz3TTbgrNhSMN+sPtPZ4UreSe87L59BEd3mjf1rusOXaeXvuVSVUw/YDanbubSSRQgzbnFBEmbYXwPaNQPBrk0O3ENPa7sXTXrdjE/NzBrCu0dN/3D4ObZ7tU01+ezy0mFP54zAXfxu+Znbq/+/fmy6Vxp90os3DTTWfkFM23P28/zZu2lyUk3++NF1jcfmABMK/ow5q+3YUgp1avjqc7L+R62PXA974db3avX4XXM18ObzXOH14UHNnged8Zh8++Hj18ZXxCm4jPpWnP5ghegx31FYzlm1uEBPLrHfLhzssfNTjF/txr1L/37uvg9E5hHjDMf/pSHha+br1t+PjvPl5dt/p042QyK8padUvZ/w+SEVOkSkTOucXQoq56+gJcvaXPC87y9LDx9cUsaVA/m21vPp3F0KBFBfky5oxv39jch7r/DWvL40OZu9/t1feJJx3BFx1gA/tp5lDu/dn/j5xq4PPnfrK3cNnElY/90Tvf6dNFukjJySMgs5MkVAfR9axEL451vUlcm2UjJcl4vtNp4cPJaHp26jtTsPJ79eRNr96eW+py3T1zFvYcvZIDPRIgzFcCcIJc3yoGRzv3K7NdLse1QsaYmMR5+Dm5ryFxYLNDtbmhyAXS/Fx7ZBeffYcKE3QUvwDWToO9/4K4lzuP+IaYiVVyN5iWPuXINoa7n1unoDFzg/tiFudDsQrM59o0znG/cz3OZ0lg83FqLran55nIzLS4s1gQ3u2ZDzLTPjjdCy5HO4/aKRY1mZlNwu1mPm6oSOCtd0a3cn6t4s5PiVnzirHRVb2bCLJRclwWw6w94v4OzfT04KxFBUc4tEzxpfz1gMV0Ws4p9eFF8fzZX9iDjyrXSdXij+bPkffdzSqt02SVtMWvIfr7PfePv3HRY87X7mDwFZDDVMDvXqZf7lrhXJwtzYc5/3cf/ehOYfueJx1jc7j/N+r69i8z4Xac42qdZFuSWXLfm2q3S9XexeAVu2Xjz/Xbt4GmvCCYU/VuWtt98P5J3mefJPw4fdIYXY0xg/u3Rsr+eTT/C7087r9sbuWQeMWOxm/0EvBQD809QnQOz/vHrS2HTNM+3H9trtuhYP9nZZTXziNl4PWVPyfOPbIP3O8H6KSVv+ydyi60ztRaeeKqhzVZyXV5ZFeSaD2g+7AlZybDg9bJPsT2b/nrb7MHoWhWX06ZKl4icFQG+ZdsfaHS3+ozuVr/E8YcuaMoVHesSGxnImv3ONT7eXhbCA31Jycrjmi51CQv0ZVdSJk9e1JJpqw/w/rydBPp688LI1mxMSHfrqtizcXX+2um5Sgbw4fUdWH8gjXHzdzF3i3tlxmqD0Z8tZ+shZ0vwdJxtr6+auIVCthPi78MH157H/pRspq8xVZW9R7NZvjeF6WsOsvaZC7BYLHy9bB/JmXkcy86jXd1wDqYeByzsLnr4jJx8NuXGYP+se+z8XXy8cDc/932XetmboNlQx3MnZ+by/aoD2N86Hs71w231k6dW82VtX29vFjJirAkd7a4pavcdZPbwKosazU58u2tb+oh6zopNyxGl38dmhau+NlWfAJeGLt3uMZ/G71vsHpjAfSqbxdvZ3a9e95LbBVgsMOxdc/mFaGezjdodTHDx8jbhc/NPcHCly2upZe7b5nL4Y5M59vAOM/1vzdelv57NPzvfvEe3cH7S7Glal6f95OyimpRcD2gXGAkjPjAt3tP2wxuNzVTXtleYsbmuUStuxafQ+nLzRjkoygTx0jb1LswHr6K3G/bmHd5+zkraJR+ZaZ0755oq3MZpsPrLko+TlwHJLuvBln9c+vjsXJud2ArNZtduY3MJOH9/ZBrFrJ8El57gtYN5nPmvmGqza6OYcV3dz8s+at5Uf9AR6vWAq7/x/HiujWUyEtwf09P6OPvaPtegYK9MXfWN+cDDPu0UzHTMYe+c+DWB+Tvx/Q3ux/b/baZrFm/ssu4783X+y6baHezh98xaCElFv/cbp8GGH8y/F5d+4qx6uU6zzCiqZM7+j/mdWDcJHtrs/pjLxpkq8LTboMUwzx/snA7X7R0K82HiSFPxu3eV+fCkuMnXm9vH/G32GzwVhzY6PwAY391UcJeNg8c8hMx/09Kx5oOLfk+YMbt+cOdqyy+mu669g+1PY6DVyLI9x55F5kMr1+nvxaXGm3WJ59954vOqGIUuEakULBYLcVEm1LSpE0H1EH8ycvJZ9p8BBPp5s2TXUc5vEEWwv/OftfsHNCE80JeuDaMI8PVm3HUd+HltAuPm7yTY34eXLmnNKzO30rNJdYa3r836/Wnc891qUrPziQkLYFDLGIa0rkVogC//m2Xe+FcP8ef+AY15+qdNboELYJ8thg8KRpBqC6EQEzIzcwu4feIqAv2coXP5XvOGKe14Pm/8vo3thzOZ47KPmb+PFz5eFgqKuiqOm7+T9//YSXD+YL7xW8lPhT0YN9tMmeszqwYtaw3nu35WwgPNc9z85UrW7U/lsPcohnivYGnUpbzy9kLa1Q2nW6MoflmXyJstrydy2xSz5io8lqMhTXhi4kou7xjLoFYuoac0TQaaP6fDNXR1GA2rJ7rfHlnPTNHzDzPTFu1Tz2oWqxYV5+XtFrhsNhsH03Kp3fFmvDrfUvJ810rDedc73+jX627+/PoAnDeq5P0Cwp3TIzuMck6H6nEfdLrJTFuzs0/b7HQTbP7RrKezh16/EPNm2/4VzO2J68z+VWACTUxbZ+g6Uft7T6o3cZ8aFBLjnKJXo7l541utgTOcTLvVbI3w05jSH9PL16w/+sJleuGS90xw9eSF6uY1DnnVfM8t3qZqaa8UhtWBbmNM6DqyDWJLCW8pe9z3WPPUaMQuMNJMhTuyzf148b3QClwqGa7bP+QfNw0/ts40P9fie44tea9s7emP7YXN081Ytv5astpVkGcCkWs4TDtgQlfievMhgqfqpv2Yy2b1DpNLaViTccj9Aw273fNNZfHit0uvmHvqpOlqzVfQ8wH3Y4c2wOcuvyM5ac7vWZsrzDYWQdVg5x/Oc+zB3R6OXaf/OrhMUVw/BToWhcTcDJh0HTS/GM4/xXVZNpt7gJ33Iuwr+ruWsKbkVFqbzVkB/ON5UwWP9tDYp7AAfrnPvNZONzmPu34wY//7eNzDz/nfZq/AL//I7Ct5y+yS5+xfbgKlq7xM8zOp38NMd/3xLvNBUuvL3M/bOdc0+KnW0DR1Ks3Um+HACvPv2W1/lH6eJ8dTzb/BpzIFuIJQ6BKRSsfPx4uf7ulBQaGVyGCznqB/85KfRPp4e3GrS0fFBtWDuX9gE67rGofVZiM6NIAPRzmbNvRsUp2vbj6fnUcy6Ns0Gi8v84/6RW1qOUJXh7gIRpxXh6d/2uRxbG8UXFXiWF6hlbzjVg9nw9g/S3Zbyy1wP/e1WeaN43HCGZz3WonzNyemM2XFfm7rbV7ruqJpi18UDuWLwqGEr08l7Xg+2w5nODaxfqLlaMY/+io5XkE8OHktv708HzCbWO999SKPYy3upRmbycor5JrOcYz5djU3dq/PzT1Nw4+E1ONk5hbQtGao231en72VP1cewTExrO9/oPkw+PYKc91SNOvdPl0vO8V80u+651kZ/bI+kfu+W8N9A5rw0AUeptjV6WSmM4XUNFMnHaGrhwkrsZ09V+VcQ1fxpiD+oaZboX19lb0RSWCkaZri6topZiPpOh3MXllgmn3kpMOxok+8G/U3a+Ls0wtd36x50u9Js5bJHuKqNzXh1e6S8c5Ojfb961y7YPoEeu4AaHf55+Zn4rquzHU9XWnyMmFR0R5vkfWLba8Q4Xx9yTtKdjcc/LJ5sxi/zL0FPkC7a2HdtyWfL66bmW7nqUpk8YZb55pOgIc2wv4V5uez60/nOau/MnuOgfkkvvh0UE8dGF2F1jYVq2P73FvuZxSbCr37z5JT/1L3m/t81Mtcj/TQ+fT4CUJXaRLWmmB5aIPZQsLb14SHiUUV5DnPuHdNLY23f8ltFRKKvcG22WD6Xe4VvEMuWxR8e6X5e9dhNBxx2UvO/v0JrWm+f2CCqeu6Mde1iwdXOkPX6ommartnAUQ1NGv4hr0HMa3NtM/8bFOZtr9ZTztgQnnjAWZqrc1lXezid1zG5OFDANfX9feH5s81k03wWvKeWaN34evm+7L2G/OnQW/TzAjgwEn+Hv8btv1m/v6Hx5oPcopPK9+/rOQ6Tih9+4gJF5rXaF/7um1mydC1aoL5erK/HweKGl6d7N+z9EQTSu3rfHfMhW8ug4HPlQz5lYBCl4hUSnUiTn9KSfUQ/1JvaxMbThuXbouAo8IG0KdZDcICfHnqoha8OGNL8bsD4ONloXmtUDYedF9L1b5uhGMdV5s64W4NO0rjZTFTGdvVjWDDgVRqhQcWTT10tzkxnazcAry9Sn76l3a85D5LC3YeI9c7iE8X7na04rdLTDtOrfCS39/8QitHMnKpHRHIgWPZfLLIBINv/zafgj//62Zu6lGf9QfSuOKjpViAJY/3JyrEH5vNhsViKQqZ/qyIvYTOdQLN9DuX/cqs/uGs3ptCp/pFb8KDqpVtWhRm3dy01QdIzc7ntt4NeXDyWgDe+2MHg1rWZP2BNK7pUheL/U3GpR+bIND9PhOyRo43n5bb143FlDLlMjDCeTnEw7Qj1z2UPE3ltKvfw/zZ41IxiWpsGoasKJpyZZ82erKF7BYveCLBTLVa/J7zePUmpmrXeKCZXtnIZd2Xfcqf65su/1Bn4xG72xeYT66xOdvu7/jd/AHo/yQ0vwiyj5lK45L3PU8PPLbXOSbX7p4B4eZ3wF7xK175tDeQKB64AJoONo099iw0gdXe+j62kxlf8XV7YEKfY/qeDT7zULG1By4wYe+862Htd+b7ENu59PVuDXrDhW+aRhcv1zZr0Q67fEBzuNiHNZ7WyqQdcN+bzh7A6/UwoWjydaZytvcvUz0F83M92Sbi37l8IFS/p/ndct3jzDfINEA5kdE/maYd39/gPtXVvh7JajW/T3sWwuFiU2GLd1jMPOxsXmG3c64JJK4h7ug298Bg7wwK7hU414rg10VhYOpNcN1U+KiPCYptroDLPjUVqI/7mtB63Q8nbmaR7mHNcPGNxu3PFVjN/D6CCXOuXWTnPgsthptKcmlhIyu55DTNglwTCldPNOtrt/1mqpYnCh2755ttCQC63G6m4nb1UL3OTin5fEme/18DzL+Xtdo7r+fngK9LF9jkk4QtO9fpxSfyzRXmd3LMcqje2Ew7BZj7X/N3oM9jRdPbKweFLhGRMvhpTA+W7U7m6s6mGcMtPRvQKDqElXtTGPvnLq7sFEtYgC+XdYwlyM8bPx8vnvt5M7M2OQPN1Z3rOkLX3X0bcdc3nju5WSzmA8gWtcJ4+6p27ErKYmjrGHYfzSIswIcjmbnc+uVKEtOcHc+mrznIrI2H6NWkusfHLC47r5B5W5IYN79kVWPWxkPc1KMBNpuNt+Zsx8tioXlMKI/9sJ70nALu7tuIJjU9v0mJT8nmkanryCuq1m1MSMfP24tbv1xBoWNqlYUpNR+k8yXO1vWzmjxHt+3/4470MSz7cClf33I+Pcv4Wuz+2HKYR6aaT9N7NK7utun1xe+bqULRof4MtHfgrNaAgovfJyE1hzgw+76VhWvlKKRGydvPu8588l2nU8nbPHF901CtkXljVqutmVrY1L5WzyUYjZrurFYFRkKTQaZDoX1tS2R95xve+r3MG0rX9vr2cNLpZnO91//Bhqkm1GQluTfhuGIC1G5fcsw1WztDV3RLE0bsudBToxZXUY3d33AFRJhf+i63mYX7xUW3Ar9Q9zU3dvW6Q9MhZhra7vnO0BXZwAR612l7dnU6lr27KJhAkbAWfixaJXnf2pKVnvA40wkxMNL5Bj60lqnc7F7gPK/4mjx7lc7iZULxpmlmw+zi0xnBfChgX4OTdQQmuFSkO95k3oB+Pth5LKi6GU/GoZLfu6TNJnS5rgfMPgorvyj9+3DLHGflNqKe+2tJ2uRc61i9mZl2C+Z3s+lgmPF/pT+uK9fpo3Z7FpntHao3MT+HBJd/N1PjzYcMR7Z6Xtt1dLtZK2X/eW343rxZd91OYfssiD5Bc5+0A+bvR3QL5xRnT6ErP9u9srryc2flHkxXxC0/w5ZhphLk5Wue1/X7+HpDaHWp+dm0vdKE4i+HOyvJf493htfmF5sgAmZ881+Fyz8zU5Rdf672tY/LxpYcc9ImqNfT2WEWTrwdwIEVZp2o3dFtzi1IbDazF6Dj+3Hc+TMpyDNTl0NrwdD/md9NRyUz1/Pve8pu579jiWvNa/VyWYO2+B1z36Gvlj7eCkahS0SkDNrVjaBd3QjHdYvFQr9m0fRtWoM7+zQiNKDkguQPR3VkzDermbEhkYcHNaW2S3WuW6MoqgX7OToc9mxcnTXxx6gZFkDL2mH8uj6RetWCaB4TRvMY8ybfvjl1dFgAsx/sTdtnf3d7vuP5hfzusjasNC1rhbE5MZ0nf9xIdl4hdasFcul5sSzccYQ18am8PHMLLWuFERXiz/vzSq6bGTd/FzFhAR4eGT7/y+x1Zrc5IZ1pqw+Qlefe0n7XkUy+WLyHlKw8rupclyd2tyIl9xPs4eKN37fRo3GUoyqVmVtAiL/7f1mFVhvzrJ25wGsFK61NeXWW8w3b33s8vCkC/tyW5AxdwAd/7uSduTt49+r2jGhfx+N9Fu88ytJdyYzp15iFO47QPdeKfdLkpC05fLf6Lz4Z3Ylo+/dk4HMmWBRv3FEaL5fXFdXYVNI6jHY7ZaN3M1oDxy2BBLpWq3o+5Gznbdf/STM9btALnj/Bv/pbSDvorOhVbwJPHzGVmfxsWPSmOd79PlO58aS6yxuv6Jbut9mnUJWmehP3io+9GjDwWTO1zrVrIZhP0mu2NA0dwEwdtE+BslcSoxq5r9+KrGeqZ6WFrrKsBxn+Pvx8r6k62atN4DkYRtaDCPc9/YhqbEJXmks1prQ3tBe+YTb83rfYBIU5T5c8J6y258odmMYs4S7PX6cj3PqHeZ1WK2z5ybyBz0mHtV9DUtHfFdfKlqcpYYHVnFMZXRuyeAo49mBzdJtpmgGmY+h5o2HGw4Ct5H3svHxKf22z/2Omlw54xmwH4Cplt8v3qpSfafEgWXz/uvxszz+XpkNh+28mGK/71nwgctmnJaeIFmff2qO0NX/27pqNB5gQXfy5N00zf2q2gs8ucL/NtVqYutcE5WqNTFMRmxU+GwRPHS5bFQngy2HQ7ymzBi7toPmdLb4OsrjdLtNwD210hq7Dm6DAZeuD7BQzjThlj+m+ad8SY+Bz7g08UnabDybsazpbjTDbTLhO903ZY0Jd8b/PvR4q2+usIBS6RET+AYvF4jFw2T0/ohXD29fmghY1ySu00qleJI1qhBAR5Mfk27sycek+HrqgKZHBfhzNzMXXy4tj2Xlk5BRwU4/6pT5uWLHnbF0njN5NajBu/i78fLwclSZPRnWrx3+mbXAEvqs7xzGmX2Nu7dWAmyesYMXeY1z18bIS9xvZvjZhgb5MXLqPQ+nu+woNaRXDrE2H+HKp+7Sr/83y8Mk1sDo+ldXxqQAuwc7C0NYx/LbxEGv3pzJpxX4uOa8Oj05dz8/rErioTS2eG9GKHYczeXvOdvx9vViXdxuXeTfnl8LuHHVp/f/N354X/k9fc5C9yVk0iQ7l2eGteGeumRZ1/6S1HkOXzWZa/Sdl5PJB0ZYBUwOPYq9hPf6z+WT3pgkriArxp3GNEJ4Z1tJUbcrKdU1VKdMRVxU24aW8J9lFXebnFRA06kczzaiLh4YBzYa6dbMswS/YvfU+mDfngdXcP6k/Uat51ype8eYMsV1M9cq1MYWr2ueZ/aXsXD9lj+1UMnSBeaNmb9xx4etm37IGfdzPcZ32GVHffKruGGNt5yfrsaVUINtdY9402vdja3OFCQt5mbDPZZ2Lfeqkxdu5Diiy5L6DRLcs+cbbPm0wtJZ5A23/ftdsbaaQnn9nyWAB5s2wt6/542kNXXAN96mufiHOYOnlZQJdq0tMY5C1XzvXUXmaStbhBjOVMaga/PqgM3S5rsM7WQdSezUqONqsxwqNOXFYiWlTcl2YG5vn70vxczwpzDXV0B1z3Ndt2ZU2na7TzSZ02aXsMusAoWi6bSmaX2SqayfT6lIzNtctDlxtnXHi+6/8wjTzqNPRuS9gQY4JMAdP9L0s5s8XTSVp669mTag1v/TKMrj/HF2nwbpuJwDm92bqzWbtmKusJPc96rb95v6zXfK++aDA9QOB7bPMNOPcoin7Fm/T9fRE07crIO3TJSJyBkWF+DO4VQxeXhYCfL2Zeld3/ne56WjXpGYoL4xs7WgGUj3En/AgX+pXD+bLm7u4bTx9Inf3bcSv9/bi0SHNWfJ4f9Y+cwGPDmlWojJk17up+5S4S84zYSM0wJc3r2jv8T5Na4bw8qVteHhwMyKDTOCrFuxHaIAP7etG8M7V7bmyk7NrX4e4CMflQF9vPry+I2EBJ/6c78I2MYy/viNPXWS6gL00Ywtvz93Oz+vMm+UZGxLp9OJcrvlkGcv3prBox1HSCeHXoJEcxX0d3s6kzBKPD2Za5eKdyUxYspfOL7l/4r0pwXyKnJKVx1dL9zLig78Yv2AXSRnuU8lsxaeWAZsS0lm4/QifL97DnqNZ3D5xJWv3p5JXYOWYy35tB45lM2fzYRLTXN40h9SA2/6Ee1ezfO8x2j47m+lr3Df+TUg9zlJrK5KsYayNTzWbDl/4GvgGYLPZ+GLxHlbtO8bJWK02jri8nqW7kp3bKKS7PGdsF2hgGjkcTs8hKaPY5r01mpJ5zc/k3La4ZNXIPwTuXwtNXKa6tb7cfG1zhfOT8SIHU48zfv4ujucVlqya+Rf9XOt1MxW6i94yb9B7Pwx1O7uf6zrtKaiae9t11ymS9umPff/jfv9a7ZxBzj/MVHPs08nsn9K7sm8CDibkFVezZclj9jbuIdHOjbp9Ap3n2qfvgVn3cs9Ksxn5pS7t8T1NjQyu7r6psHcpGxbbw9KhDfDL/c4NuF0NfBbaXWX25nPddN11Wm3Xu6Ht1WabhnpF34c2V5r9+oqPC0485TS2M1z22alN+Sw+npMZOb70N+iHNznDQ6+iBjHn3+U5SNsVrwra9/Ab/ZP7ekW7ao3MFElXLYeb7qgXv4Nbla7NleZraWHMzt498eAq9/tPHO7svFhW9sf6s+h3Mjy29HNdLRsHY883lc31k91v2zS9ZOACs0Yu16Vi5ylMLxvrXlE7uBK+vNhcjm4FTx81W1tUMqp0iYhUUhNu6syv6xO5u59zI1/7FMa7+zbmjt6NeHfudt4rqiRd1KYWV3au69aEpH/zaLdpj3FRQfRpWoMF251d0T68vgO9m9YgyM/8l/HpDZ2YtzWJW3o2JNjfGwsW/Hy8eO3ydvRvHs2iHUe5olNdRo41bZ9fv6ItQ1rH0Kp2GDM2JLJ451E2JaTz2Q2dsNrgjq9W0Tg6mCcvMm88b+rRgKmrDrD1UAYfLTBvbq7pEsea+GMl2vQDXN4xlp/XJnhsMOLqqYta8PLMLdiXeh0pFqYuG7+Ei9vWZsb6RI7nmzeb6w6YNwf+Pl70alKduVuS8MdDQwcXI8cuJu14Pr9vPkyt8ACSM/P4cFQHYsICufj9RVhtZn+5fs1qkJVbyNjrOlCtTgcAXpq0mPScAh6cvI6R7es4plcecHlty/em0L2xs+owb2sSz/1iPhXe+sIQx554hVYboz//Gz9vLz4e3Ym/dh5l9sZDTFqxn29vPZ+61YK45hPzpmjHS0Px7f+UaZV90VscbXE9ucetJB9JZcTYxdhs8PTFLbmlZwO2Hcpg48E0XpiRS4PqaUz31PAuMNLsBbRjtnlzfsHz0OJi06kSTGhc/hFYvLj1y5VsSUwnPiWLV0b0MGEtJJrU6ucR3LQvjppu85N01QypYTbntld5XN/oh9WGe1ebbQXs60d6PWymcb5V1Oo7qolpSOHth7V+L/OpdFxXU7Wxt/XucodZR7b2G+j9CHxWtF4ryBkYjmbmMn31QUbVbYbjbXZobVP5s1e2/MNMJaVWO1Og8S+asGrv0gZmXVr1Ju5TOaGoalSso16A+4cOHtfhgQmi9kqZvdOcf5izggDuTVtsLhVz13DtF+zcy6xWO7OvXOdbS05rswedRv2dawDtwmJhwNPOjcxv+9NMMfz2Smeo8Q83b9BdK5V2ZZ1CF1HPvKbgGs4qTcN+JmxlJTmnRfqHmSDe+lKo0cK9Q+HJ1GgBXYvW/CV5qO43HQJDXjZbMaz52uxzaJ+i2ekms4bzlwfMeid7QxTXbo9ujzXUvQIHnHDq5ukIq+3eVfJEjmw1TUQAGl9gtjmIX+Kcplyc6/5xri56y1Rjv7rEuWWGJ+dd514dr0QUukREKqm+zaLp26z06RXeXhbu6tuYv3YeJSTAlw+uPc/xJv69a85jxvoEXrqk5CfQb1/VnnUHUlmz7xiJaTkMbFETH2/nf3Id61WjYz3P3fSGtK7FkNa1sFptXNgmBh8vLy5sbaZ51a0WxJ19GnFH74bkF9rw8zGPufIp9+5x3l4WxvRrzL3fOafI/OfC5gT4eDNhyR6OZuZxa68GvPDrFppGh3BX30a0rh3OD6sP8ORFLXh7znZ+XV9yKtOtvRoyqGUM2w5ncNtE9+5h9o2yp64y1R5710i7Z4e34poucdz4xXKm7OxLW6897PBvBcUKQODeLdLe7OTeb9dwaYdYx2MWWm2ODbe/XLKXB4ta2hdanW9yf9t4iAvbmO9dgkvoWrHXfT8f1wrXr+sTubyj+ZR6b3IWi3eatW13fb2auVuc6/2+/nsfl3Vwfpq9Jj6VLj0eJLnBMNZnVeM/7/1Fek4+A1vUdGwt9dPag9zcoz43fL7cMb10TXwqRzJy2X0kk08W7aZNnQj6N48mr9BKx3rtyXlgG5M3ZpKyLI0bu19EpLcPhVYb3k2HwDWTsNVsxZZXzZqW75bv58WRbfC+bgpv/r6N93/ZyaUdMnjzCpuz42SRpIwcnpy+kZu613cLoHlRLXjqxw10aeDN5XFtHcePWwIJLFprlp6Tzxuzt3F15zj2JlvYV+dVbm6Wi3/jAWCxsDh0MHe+s4rbe+dybbXzcKs31+9pKhTFN4oNcU6xvHTcEuJTsknuGo1ju/A+j5puffNfMdftVZo6Hd0exm2TXde1e0UycwsIGfySadddryf8dLepxtm/PzfOMFW5ng+WuC9gQufIsWbal110C+d6ORd5BVZ8bYWlrZRyioiD7veYy74B7nvBBbuELlfB0fBQsU6O1Ypa47t2/Lv5N/Nau94N399kOmCm7DbdJGu1M80jrphgGotMGWXWnbk2ufAPN9U4cK909XvSVEq/vdoZYFpfaqqF9upmQJj5+WQegUZ9nV01Www3DTHcXo/L1MviARicayuHvmaCe6227rfX6Qh3Fk1FPdGWDU8fNR0vS4Suf1lYLVO1nHx96WvtirN4m7345v73xOfZm6X4hZomQpmHTWW2xXATjm+bB88X/f9S/AOBHveXbVuDCkqhS0SkCgv082ba3T1KHB/erjbD29X2cA8zbbBfs2j6nSDQnYyXl4Vx13X0eJvFYsHP58Rv5S5qU4vNiel8tmgP13et51jDdntvZ5OG969xVgUualuLi9qagHJbr4bM2JBIs5qh5BVY2X00i+YxppIQFxVEXFQQG54dxKXjlrAjKZPujaKYeHMXJizZy9tzt1M7PJCpd3XjrTnb+WLxXvo1q+EIKE2iQ/hs2wB222qxIecE6zpchPr7kJFbwFfLzHq3BwY2cawlA3j3jx38sPoA13SJ4+AxZ7i659vVTLmjG53qV3MLXYt3JvP67K1sO5RB4+hQtxA2eUU8nepF8tfOo9QIdXYEcw1cADM3HGLmBmdnzbfmbOPAseMcTD2OzeZ802ef2glmCuX2w5kl1vP9uS2J//22leSsPOZuSeLtueaT7LeubEf68Xye/dUszM8rtLLxYBo7kzKZeld3ajcd4rYpOECjJ2YS4u9DZq55ozdt9UF+WpvAFzd2dpsW+8S0jczdcpg5xfaV+21jIlNWHmDKygMcvqAR9ibZvy7bSLO2qbSNjeCdOTuYuHQfEx3rD+PIqNuIR4uCy4wNiWTkFvDmnO1MJI8VrrPCwout+7vwDdNRr2gNnc1mIz7FVLP+2J3N413vNh322l+LLT0BS1HoSs7KpdTJwzf8ahop9H/KcWhLYjp/7TjKSzO3mKYvg140Nzy4yb2jW/2e7tMePWl9Gaz60uxnBe6hq2gKXFJGDoPeXshicgku5WFKFdXYGbrs3T2rNzXTCFPjTQv3E01fazrYVHl8AkwAsoeg66aYr9kppjLo7eu+4fC9q01ge9NlLeLj+5yBNMglGNm3qej7mDPAeFofecscU1HLzYCjO6H9NSYgnGrosodpv+CSgau4E01r9PYt+9S/kwkIN0HJ06bMYXXM7/QTCfBi0f8DsV3MvoZQstrWoDdc8ILpMHiy7S3soSs4ykyt3DHHNPKw/654eZvf/Xkvmmm1y8aZDpy3/Xni1v6VgEKXiIhUOF5eFh4b0pxHBjUrU6M5V+3qRvDLPT2pEepP+vF8Plywm/sGNHY7JzTAl29uPZ9x83dxS88GeHlZuLlnA0Z3q4cN8PX24rEhzbmgRU06N6iGb1Glr2ZYAFa8WGJ138MrKtiPZjGhLNlVsmvis8Nb8X/fr3NcH96uNr+uT3Rbd3bg2HFen+3sGtaoRjC7jmQxYcleAv28OZxupkD5eXuRV2h1bKptr5bZrdp3jBu/WM7eZA+bAp/Ast0e3ngVU2i1MWHJ3hLHH53qeRrU49M20Lm+c9rdt3/HO6qAD01eS4PqwUxaUbK7oD1wuT7vu3/sINjfm4TUHC5uW8ux9QKYoDN/+xHik7Pd7vv6nF2MKQpMmVY/bv1yJcufHMja/SXXvi3elczP6xJoWD2Ywy5bMRwhkrXWhrT3Kpru5tohEEo0THGd/hoTHgBDXnFcn50QQFErEHwTV7NqXwqJaTlc3LY2eQVWJi7dy4AWNWnQoJdjPR3Aoh1HGPWZcz+tR6audzZ98dRFsBQ7kzL5ftV+aocHMrpaIyyO0FXU8j9lt5neBUxdZfa7S/ALpYmX+36DOfmF3Pn1KlrVDuORwR5arbvsu+eodFkspgoHntuDu+r1sHldzS50HLLZbI5puaW+qbd3zexyh5m62vVuR+CyWm1sTszA8bfW3nSk9nlw6SemM5493Lny8gavQDOem4tCRpaHzqjBLutkPYauUwgLEScIXeAe/Jtf7FyPFR7n3inzZB7ZbapYr8aZKZb2rovgbELj4w83/w4bf4BuY+DdosA48Fn30HWDy/qzQJefT3jdkh0H7d0jAyPNNONG/UqOrdfD0OkW87NuNtTzJs6VkEKXiIhUWF4eNnsui9Z1zBufmmEBvHllO4/nRIcF8Oxw9zdartMoA3y93aauAVzctjYfL9xNgdXm6P54RcdYXr60DW/8vq1E6OpcP5KL2tbi44W72XY4gwbVg2lQPZjPbujEpBX7Ge9hn7R6UUG8c9V5DPvgL35dn+iYKuntZaF+9SBHS/6HLmjKr+sT2H44k1B/H+pEBrL1UEapgSsswIflTw6k+dOzSv2+vXZZW2LCA9h6KJ2k9Fw+/WsP0aH+dG0Yxc/rEpiy0ryBGtSyJo2jQxz7vPl6W7i9d0NHGAQzPc0+vRHcp13+vSeFv/c4g17HepHUDPNnw8E09qeUXJu3MymTqz9eRn6hjdmbDnE007ke7+WZWxwbdRd3a97/cY33PMYVDOdIRi7xydnEe3j8dftTue+7NY498lzdn38P0/z+SzrBfPDrAS4+D/o1i+b9P3YwY0Min93YmToRgRzPK+TpH53d3FKy3NccrY5PJaOgN1f4LOTdnKF8Nt50RKwTEciWxAxenLGFTxftYcZ9PYlPyWbe1iRa1wnng2LbNvh5l76exWq1kZlXwIz1iWTlFnBrL2c19rlfNrFox1EADvnl8VjRw8xLiaLNiO+osfcXU3EAMnIKil77GH6s8w1+FzzjeJzfNiYyf9sR5m87wkMXNHNsyJ6WnU/a8XziXEOH63TJk4UtO98As3eci1GfLWf/sWxm3d+bQD/vE99/yCumCUi97o5DC3cc4dihVFrb7+radKTtlSUe4tNFu/l2eTwfXt+RpjVNlfxweg7P/7KZ23o3pH2djkUNLIq4Bg2P0wtDSx4rRXZgTQK9fLHYNwMvPsUuLNaELYArJ5pOnofWm9A4+XpzvE4nE6Q3TCn5BE2HmKqUt4/5c98a0ykxqBr8cEvRc7gEu7jzzR8wG0n7+Jv9xeyVr7rnuz++a9v47ve5bzTuKrVkQExKzyE0wNf8jF3DdRUIXKDQJSIiUmYx4QEsf3IgeQVWevxvHkcycqkdEYivtxe1XPYue+2ytqw7kMr9A5sQ4OvNT/f0YOXeYzSoEYzFYqFeVDCPDWlORk4+K/ceo1eT6o7g0KV+NVrXCaNFrTBnZ0HMlMsLWtbkoSlreW54a649P467+jZi2uoDxEYGsXZ/KlsPldxj57rz45i5IZErO9clwNebiCBfUrM9NwO5rGMs3l4WejetQaHVRr3qwTSPCSXE34clu5IdYWdgi5p0qh/Jp3/twcsCzw5rRc8m1R2hy75GzpO4akGOKXjPDmvJ+Q2jaFYzFC8vC9/+Hc8T080ar89u6MSPaxP4ZV2CW2Arvl6vtMAFMNfakblW5zTXWZsS3QJbccUDF8A+Wwx9c98mDx9y1ybyw9pEPruhE2/OMdMo35u7g/9d3pbPF+9hpcv6ukNp7tMw9yVn8WnB7Xxf2Ic1NmdzjOs+/dsxffZQeg4PTlnHzsMZJBS7v12Iv1kXd92ny/D2snBPvyY8/P067ujTkC2JGXy33PlmNiu3kK4Nq/HzugRH4AI4WBABRQ0OH16QT8SmQ8x72PnmePcRE+w32+qzdNBP9HGZ2rn3qDPUbzyYRpuiDziu/mQZWxLTea9hMMOLbk/PLcDbYmHb4Qw6xDmrnoVWGzM3JNKrSXUigkrptOh4DQWO36Xle1PcxuKRl7cJXS6S0nNJtrlXyPYlZ7E5IZ0hrWPc1gwWWm2Mn7+L5Kw8npq+kcl3dMVisTBx6V5mbEgkJSuPr0f/xNCXp/E2b1K/ejDBru36PYSulHxfHv1yJXf3a+T2fSguOTOXS8cvpWX+/Yz1H4tXZH0Y8YFZg3fB80Wvzwuu/sZ5p+YXmj/HXLbraH2pqTLaQ1f1ps4GFnXPd98yIryOCduue+eFuWy34KqJy/rbq74yU/+63et+Tmxnszm8f5h7h9HgaNO4xK6e+7T3nUkZXPjuXwxoEc346z1PTa/sFLpEREROkZ+PF09c2Jw3Zm9ncCvTRMG1KndxO9Mp0i7A15ueTaqXeJwXR5pGJolpx/lh9UFa1grj6WEtsVgsfHlTZ+ZvP0KdiEC6N3JuFD20dYyjIufr7cVVneMA6FQ/kqT0HOZuSWJAi2jHmqWBLWry4sjWjvtPvr0bv6xLcOw79t415/F/U9ZyW6+GjqoFmMraqK7OqU5zH+rNq79tZXNiOgNaRBMV4s+KJwbi62MhyM8Hm0tiGdWtHiv3pZCTbxqDdKlfjeV7U2hQPZipd3Zj9OfLqR8VzOhu9d2+bz1dKou9mtRgQIuabDqYxu6jWVQP8eOmHg1Yuiu5RKBrHB1CXLUg5m01b+reuKIdDaoHc9n4JW7n2dfStakTTr/m0Yyfv5NujarjZTFBucvLf5T4GQFkEOR2/ZYvnY1Yvl+1n8Gta/JN0Zq9RwY34/XZ20jOyiMnv9DRTTI+5ThWvFhua+H2WNl5hWS7bB6+0KVzqCfJWbks253smBJqryY+89OmEufa19cVt8nm/LmmEEbK0Sz2HM2iflQQFovFbYPznUmZbkFn40Fnu+8RYxczpl8jujaMcnxA8Mju9tSK6suU1Kbs/Hw5UcH+zN1ymHevbk+fpjWICPLjyyV7ef7XzZwXF8EPd3Z3/A5k5hbw35820a1RFG3qhPPF4j00qemsEh04ZgJfanYe01YfJO14Pnf0aUiQn1kH+Mj364gO9eehC5qxJzmL9/7YgZfFQsvaYUwoGE5jy0Fiet1AG+CKD5eSlJHLq5e24eOFu2lYI5hnLm7F6M//JrmoSrl8bwq/rE9keLvarC/qZLpibwrrkwrYnledYbzEnU0ack+BlfHzd1AzLID1u/bzWrHv9/O/xzM3zY+5W9zXIO46kklEoC9RIaYK+MzPm9iXnM0+OvD78D8Z0q6eaTbxgOcpvDn5hXy4YBfD29WmYaRLUPIJgBbDKOh+PzvSvKjfvj+B35jOodtSbbjusvbT2oO898cOPr66FY4Vs6Ge1/u6CY1xBsEi+YVWCpqOIPAyzPYLrvvJRdZ3hq4e95s2+y4mLNlLXqGV3zYeIr/Qiq+3F2/+vo3FO4/yxU1dCA8sfT/MykKhS0RE5DRccl4sl5znXNRuGo9son3dCEd7/bKqFR7IiicH4mXBEY6iwwK4slPdEuf6lDK9zN/Hm+dGtOa5EXAsK88RumIjA90+yW8WE0rTmk0ptNkoKLQyrG0tBrWsib/PidswRwT58epl7k0AwoOcb4QsFguTbu/KlsR0BrWsyXl1I1m62wSC/w5vyc/rErijdyOqBfsx475eeBIXFcSk27sS4u/j6G45on0dxi/4//buOyzKK98D+HcGmKEjRcpIEbCggERBI8pGUYPBkpuYGDVFjMne6Fowptiy0eTG4G72uib3qlGTdeNqgsm1RmPBhhoLWYoCKhYQEFGiSJc2c+4fAy+MAxaYCRC/n+d5n0fe9/DOmZ8DzG/OOb9zGZ+ND0J4T2fMCO+G3MIKZN+uwMzvklBUUYOZ4d2gMJVLSdeYPm46CWS9+uTmpRB3vBbaFbOHddOJp6OVQnrD3ZiN0hSdrMx0pj7ampuipLIWGgFM/ac2CbO3NMMbYd74/OAlVNdq8PTf42GlMIWqkwVybpfr3LOHizVk0I4C1Zs9rJu0xUOIlz2WPh+Ioxd/RWFFtTQVtUYt8E0Ta+seRpBHJyhMZPjlKjCpehEKRCfpWvjfjmC4nzOefUKFrFsNfb1cUIpbZVWwVprCzESOM9eKdO658vAVJOc0nKuCAuNv1xWlaHQ+OjYFnSzNcOTdofj6uHZ0MjmnCD4Lf8J3fxyIUF9HfFFXVGZL0jW421vg2h3dqaAXb5Qi+3Y5Jqw5JRV0UZjK4W5vgfU/X5XW+pVW1iLrdrnUr7S8YpTACn+seRfvmvZA9xq1tP/e/K3akdXMW+WoqtVI03PrC+B8uvs8nu7lIiVdtRqBtUe1a/w0kONQxm3YWGQ32uRd4K/3bMmV1Wh24N1qNSwUJrh2pwKRK47B2twUi8f2RmxCrvTzAgCphTI8o7CERiOw4eRVHLxQgD+P6Y1OlmYoLK/Gt6dzpJ/xFQcuoZebLepXWWms3bD6+DUk54/FgfMFGJRfjm/rrq0/dR3TQstRo9bArZMFomNTAACL92Ri44RNELVV2HrhLnq5laC3SncvtFq1BhoB6Wfz8IUC/OPnLCx51h9O1kqMWB6P22VVmD40AO8FdtaWj6/XI0K7/YGqb5ObxxffbViPeSG/FAFdbPHPn6+itKoWe9PypQ+XOjImXURERAbg4WCJE/OHwf4B06Wa01SS0FL2Vgr813MBuFVahW7O+ov4ZTJtoZJ69aMxrTXQxxED6zb1/nRcIF796jReGegJf5Ud/FVNrHVp5h6NRY/ojj+F+0rFTABtrD0cLHFqwXAUlFTB09ESao3AW0N84GZrrvd8zExkqFFrR+IcrRR4MVibzN6bwE4Z1BX/HXcRPVyspdGesUEqLH8pCGYmcmg0AkcuFiBAZQdnW3MUV9TgpTUnpcSpfgpnda12hK8+SasvsCGTAate7od/ncrGX17oAxdbcxy6cBPTNiYh2Mserw/2lpIubycr9HS1Qc+6ypsvD/DExLWnkFd0F/vvqfrYWFSoF2YO644hnx3WGUGzszDDuxE9EOrjiO8ScvDnJvZ7PnihAAcv6BZn+S4hF98l5EImAxwsm05KT1y5DYWpHLtmheGlNSebnb5aVFGDTadz9PbU+2B7Kib095CSGQB6CRcAfHMyG9+czNY5t/rIFb3iK3Hnbkp77QHQqbiZklvc5H5/AHSmYP5zan/M/i4FeUV38fnBSzpTXPekNVT+vHCjFLd/bjzFVf/nuAwNBU/+svcCPhzTG0cv3kK1WoPC8mop8Wkso66Pq+OvSEV2FmxNRU5hhd4eg4C2wmW0/E8YaZ+PGd+oIdAw1fjEdTXqN42zQiWmbUzEhRulGNqzYQTzdnk10GsMDp2/iXc2aT9EaDwqd7dajYgV2g8Rts8YDHMzEyzalorrxZUY/cUxfPZikNSvDSezMffpnihTKxq2rZebAs+t0ut3vdRGyXxK7h242CpRWvf/euhCgZR0CSFwp6IGDlYt+z3blph0ERERGUjjjabbWuOpgW3B28kKP8/X32uqJcyaGd0zNzOBp6N26p+JXIYFkb2abDc60A3HL99Gda0aq18NbrYYw4zwbnDrZIGBPg7aaU0/X8X7I3tKjy+XyzDMr2H9jp2lGd5+ujumbUwCALwyQBvzxklbY6625ogMdENkYMNUsGcC3PD9W6HwdLCEvZUCb4R541+nsjE1zFvnez0cLOHtZNXkJuCDfB1xMvM2+nSxw/Sh3dDZRolds8KgEQLfns5FhL+LTjIb6qtfsF5lZ44B3g44lVkIL0dL/OWFPtKbc0C73u12eTXkMqB/VwedQigAsHhsb/RwscFAb0fsTdcmJb3cbHGloAzV6ob95+pH7Ib27Iylzwdi5N+P4sqv5fj0J/1Nhe+3NvDziU8gOjZFL+ECIL1Zb8qB8zf1tlC417H3w+HhYImpYd74r13n8GW8ts9KUzmqajV67euTjamDvdHJ0gw4pnu9QjQMff3zxFX09ewkjWrdW7yls40Sv5ZWIS2vBG9vTsG25DzpWuM9+ZqyQxOGHU0UV2ycCP4q7KT/0yMZDVNZy+ti1rgy6I6UPOw+m4/3RvZE+vUS6UOEzb/k4uUnPaV1h5U1GsT81LCZcmllLf730GWsOnIZGXWZxtE8gcLkPDzXtwsuF5RhxYGLUGsEPv6PAJiZyHQKAO1NvwGfzg0fFh2/dAtVtWooTU3wr1PZ+HBHurRZe0ciE6KpZaPUnJKSEtjZ2aG4uBi2trYP/gYiIiJqE0cyCrD5l1x88lwATOVyyOSQilYYilojsGzPebjZWUiJUmL2HWw4eRU7Uq7rtH2qR2dsmDrgvvcTQkCtEU1OIz1w7ia+Pp6F10K9cLOkEh/9eA4AsOnNJxHq4/hI1T63JGqn8Y30d4WjtQIjernojRBm/lqGFQcu4YVgd/irbJF1qxy25mbwcrTEn7enwV9li0sFZYjwd5XWfa06chl/3asdZcmKGQWZTIYJa07qJWkLIv3w1hBf7E27gembEiGENmmZPbwb/rY/A8P8nBGgsmtynZ2NuSnOfBiBietOISGrEH/8gzdK7tbCy8kS25PzpITXVC5DbaNdzoO97B+YuDTud1lVLfp9HCcljS8/6YlvTzcUKhkf7I4f6jZUD/LohB0z6opD3LkKnNkMHPkUANCnch1KYAUfJytk3irHqEBXJGTdwa2yKvzPpL44n18CCzMTaQrhvWsR+3l2gr2lQhqFnNjfo8ntFu5nqDwZA+Xn8VntBKjR9IcO6R+NxIc70rEl6dp97+Viq8SqV/rhhboKnPczweQwBsvT8E7NdKhlptg5MwxLd5+Xks45I7rD28lKGu2r3xbDy9ES2Y0SsU1vPonB3ZwwKOaglOwdfncovJ0eeSc5g3vY3IBJ1yNi0kVEREQP4/8Sr+FIRoFUcXFP9B/Qy80w7x32pOZj+ibtCNsvi0bobIbdlsqqajEnNhmDfJ2kJPROeTX2pN2QKlMCwPYZg/GERycA2mTyyq9leCPMWy/Z7Dpfu7/X92+Fws/NBl8dzcRTPTojpKsDfi2twuWCMgz0cZDWLc74Ngm76+I9e3h3fHFQWzxFLtPG6ZuT2Th3vRjX7tzFX1/sgzl1b/YjA12x8vAVjOnjhv99uZ/0+BPXnpSKlqx8uR9+SsvH7rP5mBHui9GBKoz6Qjus9ePMMAS6N5pCm3Ma+EcEAMC38l94JtAdk0O9MGHtKamJnYUZTi8crpfsDvz0oDQlUiYDkj54GnK5DCev3IabnTmCPDrh7LUijFt1QiepfJCujpaoUQtpuu29o6bfvxWKj3elIy2vpJk7aEeU1c08polchimDukpr9hSmclgrTfW2T2jOjHBfuNiaN1kUZmJ/D3RztsYnuxtG1N56ygcLRjU9uv1bYtJlJEy6iIiI6FHEnbsJDwcL+Lka7n1D9u1yDPnsCICGkZn2rLJGjYExB1FUUQMrhQnOLI5otihMYzm3K3D1djmeelCp+DqHLtzE9I1JmDbEF9OG+KLXh9p96cb164LlLz2h1766VgMTuQwyaKce9vOyh5N1QwIb89N5rKlba3Z64XAoTORIyS3CkB6dIZfLsCc1H51tlAjpes+mzZXF2o2HAWRMu4Yu9hZQmMjh9+c9qM9Z1k0OwdO9XXCvqH8kIL6uiuWsYd3wTkRPvTaAtuppaMyh+8bjpRB3fP9v7cjVAG8HbHzjSWiEwMvrTiGprtBIqI92emr9puz1Gm/vAGj3lJsyqCuWNppKODO8m1QJdVzfLnj76R6I+kcCMm+V472RPfFskAoJWYVYdeSyzr17utjoFJEBgF2zwuDpaIk+S/ZL5/7Q3UlnrV29mHGBmNjfo1287h82N+CaLiIiIiIjauqNdWt5OVoh9j8Hwsla2S7eeD6IuZkJfpwZhg0nr6Kvp/1DJVyAtqJl/bq9hzHMzwXpH42U7v+nob7IKazA0rrtGe6laFS1M6Ju+4fGRgW6Yc3RTDhZK+FStxdfuJ+zdL3x+jwd5nbAOxmAqTl6WjSUvZ8Z3g27zuZjfqRfs68LPzcbKelqqhBOPTc7C7w9ogf+fuAiZoT7SvvkDe7miP8I6oJB3RzR2UYpJV0KE7n0fJ8JcEVSThG6Olriby8FSWvrAG1lzr9PeAIhXg5Iyr2DHcl5WDi6FxytlJDLACulKX6+cgshXvaYHNoVOYUVSM0rxvxRfnC2MceBuUOQX1IJlZ05ZDIZPBwscbO0Upp2+nzfLlgy1h+jvjgmjbZNGuABf5UtZDIZRvq7YF/6TdhZmGHpc4EYt/qEzv56Hg4WmDSg41Uz5EjXI+JIFxEREdHjIzG7EC625nC3f/jkrzW2JV/D25vPAAB2zw67b+XPWrUGqXnFCOhih5TcImTdKtfbauJv+zLwZfwV7Jg5WLpXjVqD7/+di4jeruhso0RidiEWbUtDVa0GHz3r/9Ajiw/rVOZtTKybWnl5aSRMTeRIzrmDhKxCvD7YWyf5zSu6i11nruOFYHc4WStRVlWL3MIKFN+twcc/nsOcEd2bTJDbCqcXGgmTLiIiIiIyluScO3h+lbaYxvmPn2m22ubD0mgEajVCJ7FpihDCqKOmP/w7F77O1ujnaW+0x2gLnF5IRERERNTBBLl3wvhgd7h1smh1wgVotzpQPERlS2NPUx3fxGbvjxMmXURERERE7YRcLsNn44PauhtkYA+3ipGIiIiIiIhahEkXERERERGRETHpIiIiIiIiMqLHMul6/vnnYW9vjxdffLGtu0JERERERL9zj2XSNXv2bGzYsKGtu0FERERERI+BxzLpCg8Ph42NzYMbEhERERERtVK7S7qOHj2KsWPHQqVSQSaTYfv27XptVq1aBW9vb5ibmyM4OBjHjh377TtKRERERET0ENrdPl3l5eUICgrC66+/jhdeeEHv+ubNmzFnzhysWrUKgwcPxpo1axAZGYlz587B09MTABAcHIyqqiq9792/fz9UKtUj9aeqqkrnXiUlJY/4jIiIiIiI6HHW7pKuyMhIREZGNnt9+fLleOONN/Dmm28CAFasWIF9+/Zh9erViImJAQAkJiYarD8xMTH46KOPDHY/IiIiIiJ6vLS76YX3U11djcTEREREROicj4iIwIkTJ4zymAsWLEBxcbF05ObmGuVxiIiIiIjo96ndjXTdz61bt6BWq+Hi4qJz3sXFBTdu3Hjo+4wcORJJSUkoLy+Hu7s7tm3bhv79+zfZVqlUQqlUtqrfRERERET0+OpQSVc9mUym87UQQu/c/ezbt8/QXSIiIiIiImpSh5pe6OTkBBMTE71RrYKCAr3RLyIiIiIiovagQyVdCoUCwcHBiIuL0zkfFxeHQYMGtVGviIiIiIiImtfupheWlZXh8uXL0tdZWVlISUmBg4MDPD09MXfuXLz22msICQlBaGgo1q5di5ycHEybNu036Z8QAgBLxxMRERERPe7qc4L6HKFZop05fPiwAKB3REVFSW1WrlwpvLy8hEKhEP369RPx8fG/Wf9yc3Ob7B8PHjx48ODBgwcPHjwezyM3N/e+OYRMiAelZdSYRqPB9evXYWNj80jFO4yhpKQEHh4eyM3Nha2tbZv25feI8TUuxte4GF/jY4yNi/E1LsbXuBhf42pP8RVCoLS0FCqVCnJ58yu32t30wvZOLpfD3d29rbuhw9bWts1fcL9njK9xMb7GxfgaH2NsXIyvcTG+xsX4Gld7ia+dnd0D23SoQhpEREREREQdDZMuIiIiIiIiI2LS1YEplUosXrwYSqWyrbvyu8T4Ghfja1yMr/ExxsbF+BoX42tcjK9xdcT4spAGERERERGREXGki4iIiIiIyIiYdBERERERERkRky4iIiIiIiIjYtJFRERERERkREy6iIiIiIiIjIhJVwe1atUqeHt7w9zcHMHBwTh27Fhbd6lDOHr0KMaOHQuVSgWZTIbt27frXBdCYMmSJVCpVLCwsMDQoUORnp6u06aqqgqzZs2Ck5MTrKys8Oyzz+LatWu/4bNov2JiYtC/f3/Y2NjA2dkZzz33HDIyMnTaMMYtt3r1avTp0we2trawtbVFaGgo9uzZI11nbA0rJiYGMpkMc+bMkc4xxi23ZMkSyGQyncPV1VW6ztgaRl5eHl599VU4OjrC0tISTzzxBBITE6XrjHPLde3aVe81LJPJMGPGDACMbWvV1tbigw8+gLe3NywsLODj44OPP/4YGo1GatOhYyyow4mNjRVmZmZi3bp14ty5cyI6OlpYWVmJ7Ozstu5au/fTTz+JRYsWiS1btggAYtu2bTrXly1bJmxsbMSWLVtEamqqmDBhgnBzcxMlJSVSm2nTpokuXbqIuLg4kZSUJMLDw0VQUJCora39jZ9N+zNy5Eixfv16kZaWJlJSUsTo0aOFp6enKCsrk9owxi23c+dOsXv3bpGRkSEyMjLEwoULhZmZmUhLSxNCMLaGlJCQILp27Sr69OkjoqOjpfOMccstXrxY+Pv7i/z8fOkoKCiQrjO2rVdYWCi8vLzElClTxOnTp0VWVpY4cOCAuHz5stSGcW65goICnddvXFycACAOHz4shGBsW+uTTz4Rjo6OYteuXSIrK0v88MMPwtraWqxYsUJq05FjzKSrAxowYICYNm2azjk/Pz8xf/78NupRx3Rv0qXRaISrq6tYtmyZdK6yslLY2dmJL7/8UgghRFFRkTAzMxOxsbFSm7y8PCGXy8XevXt/s753FAUFBQKAiI+PF0IwxsZgb28vvvrqK8bWgEpLS0X37t1FXFycGDJkiJR0Mcats3jxYhEUFNTkNcbWMObNmyfCwsKavc44G1Z0dLTw9fUVGo2GsTWA0aNHi6lTp+qcGzdunHj11VeFEB3/9cvphR1MdXU1EhMTERERoXM+IiICJ06caKNe/T5kZWXhxo0bOrFVKpUYMmSIFNvExETU1NTotFGpVAgICGD8m1BcXAwAcHBwAMAYG5JarUZsbCzKy8sRGhrK2BrQjBkzMHr0aIwYMULnPGPcepcuXYJKpYK3tzcmTpyIzMxMAIytoezcuRMhISEYP348nJ2d0bdvX6xbt066zjgbTnV1NTZu3IipU6dCJpMxtgYQFhaGgwcP4uLFiwCAM2fO4Pjx4xg1ahSAjv/6NW3TR6dHduvWLajVari4uOicd3FxwY0bN9qoV78P9fFrKrbZ2dlSG4VCAXt7e702jL8uIQTmzp2LsLAwBAQEAGCMDSE1NRWhoaGorKyEtbU1tm3bht69e0t/TBjb1omNjUVSUhJ++eUXvWt8/bbOk08+iQ0bNqBHjx64efMmPvnkEwwaNAjp6emMrYFkZmZi9erVmDt3LhYuXIiEhATMnj0bSqUSkydPZpwNaPv27SgqKsKUKVMA8PeDIcybNw/FxcXw8/ODiYkJ1Go1li5dikmTJgHo+DFm0tVByWQyna+FEHrnqGVaElvGX9/MmTNx9uxZHD9+XO8aY9xyPXv2REpKCoqKirBlyxZERUUhPj5eus7Ytlxubi6io6Oxf/9+mJubN9uOMW6ZyMhI6d+BgYEIDQ2Fr68vvvnmGwwcOBAAY9taGo0GISEh+PTTTwEAffv2RXp6OlavXo3JkydL7Rjn1vv6668RGRkJlUqlc56xbbnNmzdj48aN+Pbbb+Hv74+UlBTMmTMHKpUKUVFRUruOGmNOL+xgnJycYGJiopetFxQU6GX+9Gjqq2jdL7aurq6orq7GnTt3mm1DwKxZs7Bz504cPnwY7u7u0nnGuPUUCgW6deuGkJAQxMTEICgoCJ9//jljawCJiYkoKChAcHAwTE1NYWpqivj4eHzxxRcwNTWVYsQYG4aVlRUCAwNx6dIlvn4NxM3NDb1799Y516tXL+Tk5ADg72BDyc7OxoEDB/Dmm29K5xjb1nvvvfcwf/58TJw4EYGBgXjttdfw9ttvIyYmBkDHjzGTrg5GoVAgODgYcXFxOufj4uIwaNCgNurV74O3tzdcXV11YltdXY34+HgptsHBwTAzM9Npk5+fj7S0NMYf2k+SZs6cia1bt+LQoUPw9vbWuc4YG54QAlVVVYytAQwfPhypqalISUmRjpCQELzyyitISUmBj48PY2xAVVVVOH/+PNzc3Pj6NZDBgwfrbdNx8eJFeHl5AeDvYENZv349nJ2dMXr0aOkcY9t6FRUVkMt1UxMTExOpZHyHj/FvW7eDDKG+ZPzXX38tzp07J+bMmSOsrKzE1atX27pr7V5paalITk4WycnJAoBYvny5SE5OlsrtL1u2TNjZ2YmtW7eK1NRUMWnSpCZLkbq7u4sDBw6IpKQkMWzYsHZRirQ9mD59urCzsxNHjhzRKatbUVEhtWGMW27BggXi6NGjIisrS5w9e1YsXLhQyOVysX//fiEEY2sMjasXCsEYt8Y777wjjhw5IjIzM8WpU6fEmDFjhI2NjfS3i7FtvYSEBGFqaiqWLl0qLl26JDZt2iQsLS3Fxo0bpTaMc+uo1Wrh6ekp5s2bp3eNsW2dqKgo0aVLF6lk/NatW4WTk5N4//33pTYdOcZMujqolStXCi8vL6FQKES/fv2kktx0f4cPHxYA9I6oqCghhLYc6eLFi4Wrq6tQKpXiqaeeEqmpqTr3uHv3rpg5c6ZwcHAQFhYWYsyYMSInJ6cNnk3701RsAYj169dLbRjjlps6dar0c9+5c2cxfPhwKeESgrE1hnuTLsa45er30zEzMxMqlUqMGzdOpKenS9cZW8P48ccfRUBAgFAqlcLPz0+sXbtW5zrj3Dr79u0TAERGRobeNca2dUpKSkR0dLTw9PQU5ubmwsfHRyxatEhUVVVJbTpyjGVCCNEmQ2xERERERESPAa7pIiIiIiIiMiImXUREREREREbEpIuIiIiIiMiImHQREREREREZEZMuIiIiIiIiI2LSRUREREREZERMuoiIiIiIiIyISRcREREREZERMekiIiIiIiIyIiZdRERERERERsSki4iIiIiIyIj+H84J7gP8IFZDAAAAAElFTkSuQmCC",
      "text/plain": [
       "<Figure size 1000x500 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Plot the training and validation losses\n",
    "plt.figure(figsize=(10,5))\n",
    "plt.plot(train_losses_latent, label='Training Loss')\n",
    "plt.plot(val_losses_latent, label='Validation Loss')\n",
    "plt.legend()\n",
    "plt.yscale('log')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(0.2513, grad_fn=<DivBackward0>)\n"
     ]
    }
   ],
   "source": [
    "x_data_scaled_tensor = torch.tensor(x_data_scaled, dtype=torch.float32)\n",
    "y_data_scaled_tensor = torch.tensor(y_data_scaled, dtype=torch.float32)\n",
    "u_data_scaled_tensor = torch.tensor(u_data_scaled, dtype=torch.float32)\n",
    "x_data_latent = model_psi(x_data_scaled_tensor)\n",
    "y_data_latent = model_psi(y_data_scaled_tensor)\n",
    "err = y_data_latent - model_latent(x_data_latent, u_data_scaled_tensor)\n",
    "err_norm = torch.norm(err)\n",
    "y_data_latent_norm = torch.norm(y_data_latent)\n",
    "ratio = err_norm / y_data_latent_norm\n",
    "print(ratio)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Parametric Koopman"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "layer_sizes_k = [128,128,128]\n",
    "k_para = ParaKoopmanLayer(layer_sizes = layer_sizes_k, input_dim = u_dim, K_dim = n_psi_train)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "class ModelKoopmanLayer(nn.Module):\n",
    "    def __init__(self, layer_sizes_k):\n",
    "        super(ModelKoopmanLayer, self).__init__()\n",
    "        self.k_para = ParaKoopmanLayer(layer_sizes = layer_sizes_k, input_dim = u_dim, K_dim = n_psi_train)\n",
    "\n",
    "    def forward(self, input_x, input_u):\n",
    "        psi_x_expanded = input_x.unsqueeze(1)\n",
    "\n",
    "        output_x = torch.bmm(psi_x_expanded, self.k_para(input_u))\n",
    "\n",
    "        output_x = output_x.squeeze(1)\n",
    "\n",
    "        return output_x\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_koopman = ModelKoopmanLayer(layer_sizes_k)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the optimizer\n",
    "optimizer_koopman = Adam(list(model_koopman.parameters()) , lr=0.001)\n",
    "scheduler_koopman = StepLR(optimizer_koopman, step_size=50, gamma=0.8)\n",
    "\n",
    "mse_loss = nn.MSELoss()\n",
    "\n",
    "train_losses_koopman = []\n",
    "val_losses_koopman = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 0, Training Loss: 0.1332898586988449, Validation Loss: 0.13864359259605408, Learning Rate: 0.001\n",
      "Epoch: 1, Training Loss: 0.0970064103603363, Validation Loss: 0.10356081277132034, Learning Rate: 0.001\n",
      "Epoch: 2, Training Loss: 0.08035942912101746, Validation Loss: 0.08868575841188431, Learning Rate: 0.001\n",
      "Epoch: 3, Training Loss: 0.06475647538900375, Validation Loss: 0.07037606090307236, Learning Rate: 0.001\n",
      "Epoch: 4, Training Loss: 0.05781722068786621, Validation Loss: 0.06416426599025726, Learning Rate: 0.001\n",
      "Epoch: 5, Training Loss: 0.05304282531142235, Validation Loss: 0.058488015085458755, Learning Rate: 0.001\n",
      "Epoch: 6, Training Loss: 0.05157061666250229, Validation Loss: 0.056218136101961136, Learning Rate: 0.001\n",
      "Epoch: 7, Training Loss: 0.04869601130485535, Validation Loss: 0.05267907306551933, Learning Rate: 0.001\n",
      "Epoch: 8, Training Loss: 0.04783199727535248, Validation Loss: 0.05166715756058693, Learning Rate: 0.001\n",
      "Epoch: 9, Training Loss: 0.052008599042892456, Validation Loss: 0.057217709720134735, Learning Rate: 0.001\n",
      "Epoch: 10, Training Loss: 0.048061903566122055, Validation Loss: 0.05171141400933266, Learning Rate: 0.001\n",
      "Epoch: 11, Training Loss: 0.045069560408592224, Validation Loss: 0.04792996868491173, Learning Rate: 0.001\n",
      "Epoch: 12, Training Loss: 0.04511796683073044, Validation Loss: 0.04852057993412018, Learning Rate: 0.001\n",
      "Epoch: 13, Training Loss: 0.0453215092420578, Validation Loss: 0.04793424531817436, Learning Rate: 0.001\n",
      "Epoch: 14, Training Loss: 0.03792985901236534, Validation Loss: 0.041544314473867416, Learning Rate: 0.001\n",
      "Epoch: 15, Training Loss: 0.038633525371551514, Validation Loss: 0.04247539862990379, Learning Rate: 0.001\n",
      "Epoch: 16, Training Loss: 0.039327796548604965, Validation Loss: 0.041830044239759445, Learning Rate: 0.001\n",
      "Epoch: 17, Training Loss: 0.04084383323788643, Validation Loss: 0.045606259256601334, Learning Rate: 0.001\n",
      "Epoch: 18, Training Loss: 0.038826070725917816, Validation Loss: 0.04327544942498207, Learning Rate: 0.001\n",
      "Epoch: 19, Training Loss: 0.03446052968502045, Validation Loss: 0.0373169369995594, Learning Rate: 0.001\n",
      "Epoch: 20, Training Loss: 0.040800511837005615, Validation Loss: 0.04431857168674469, Learning Rate: 0.001\n",
      "Epoch: 21, Training Loss: 0.03666069731116295, Validation Loss: 0.039514731615781784, Learning Rate: 0.001\n",
      "Epoch: 22, Training Loss: 0.03630545362830162, Validation Loss: 0.04013632982969284, Learning Rate: 0.001\n",
      "Epoch: 23, Training Loss: 0.03578180819749832, Validation Loss: 0.04073317348957062, Learning Rate: 0.001\n",
      "Epoch: 24, Training Loss: 0.03426462039351463, Validation Loss: 0.03773139417171478, Learning Rate: 0.001\n",
      "Epoch: 25, Training Loss: 0.039765603840351105, Validation Loss: 0.04365213215351105, Learning Rate: 0.001\n",
      "Epoch: 26, Training Loss: 0.0349368080496788, Validation Loss: 0.03762102127075195, Learning Rate: 0.001\n",
      "Epoch: 27, Training Loss: 0.03137534484267235, Validation Loss: 0.03530732914805412, Learning Rate: 0.001\n",
      "Epoch: 28, Training Loss: 0.037404865026474, Validation Loss: 0.04057905077934265, Learning Rate: 0.001\n",
      "Epoch: 29, Training Loss: 0.03124612756073475, Validation Loss: 0.03509627655148506, Learning Rate: 0.001\n",
      "Epoch: 30, Training Loss: 0.035428453236818314, Validation Loss: 0.039189521223306656, Learning Rate: 0.001\n",
      "Epoch: 31, Training Loss: 0.03321458026766777, Validation Loss: 0.036140091717243195, Learning Rate: 0.001\n",
      "Epoch: 32, Training Loss: 0.032931849360466, Validation Loss: 0.03630414605140686, Learning Rate: 0.001\n",
      "Epoch: 33, Training Loss: 0.033754728734493256, Validation Loss: 0.037025753408670425, Learning Rate: 0.001\n",
      "Epoch: 34, Training Loss: 0.033883314579725266, Validation Loss: 0.03882381319999695, Learning Rate: 0.001\n",
      "Epoch: 35, Training Loss: 0.03225718438625336, Validation Loss: 0.03539971634745598, Learning Rate: 0.001\n",
      "Epoch: 36, Training Loss: 0.03387787565588951, Validation Loss: 0.03667907789349556, Learning Rate: 0.001\n",
      "Epoch: 37, Training Loss: 0.03212985396385193, Validation Loss: 0.03597371280193329, Learning Rate: 0.001\n",
      "Epoch: 38, Training Loss: 0.03471741825342178, Validation Loss: 0.03784279152750969, Learning Rate: 0.001\n",
      "Epoch: 39, Training Loss: 0.03041905164718628, Validation Loss: 0.033316273242235184, Learning Rate: 0.001\n",
      "Epoch: 40, Training Loss: 0.03588453307747841, Validation Loss: 0.03840881213545799, Learning Rate: 0.001\n",
      "Epoch: 41, Training Loss: 0.032130464911460876, Validation Loss: 0.033349908888339996, Learning Rate: 0.001\n",
      "Epoch: 42, Training Loss: 0.030236050486564636, Validation Loss: 0.032544057816267014, Learning Rate: 0.001\n",
      "Epoch: 43, Training Loss: 0.03287656232714653, Validation Loss: 0.034239113330841064, Learning Rate: 0.001\n",
      "Epoch: 44, Training Loss: 0.026509201154112816, Validation Loss: 0.03071979619562626, Learning Rate: 0.001\n",
      "Epoch: 45, Training Loss: 0.03133225813508034, Validation Loss: 0.03459203615784645, Learning Rate: 0.001\n",
      "Epoch: 46, Training Loss: 0.030986880883574486, Validation Loss: 0.03402495011687279, Learning Rate: 0.001\n",
      "Epoch: 47, Training Loss: 0.028067627921700478, Validation Loss: 0.032332371920347214, Learning Rate: 0.001\n",
      "Epoch: 48, Training Loss: 0.028786035254597664, Validation Loss: 0.03200589120388031, Learning Rate: 0.001\n",
      "Epoch: 49, Training Loss: 0.03542957827448845, Validation Loss: 0.03882061317563057, Learning Rate: 0.001\n",
      "Epoch: 50, Training Loss: 0.027515392750501633, Validation Loss: 0.030403105542063713, Learning Rate: 0.0008\n",
      "Epoch: 51, Training Loss: 0.026811670511960983, Validation Loss: 0.0291578508913517, Learning Rate: 0.0008\n",
      "Epoch: 52, Training Loss: 0.025027388706803322, Validation Loss: 0.028232300654053688, Learning Rate: 0.0008\n",
      "Epoch: 53, Training Loss: 0.025785308331251144, Validation Loss: 0.02720894291996956, Learning Rate: 0.0008\n",
      "Epoch: 54, Training Loss: 0.02650449611246586, Validation Loss: 0.029165050014853477, Learning Rate: 0.0008\n",
      "Epoch: 55, Training Loss: 0.029153823852539062, Validation Loss: 0.034263890236616135, Learning Rate: 0.0008\n",
      "Epoch: 56, Training Loss: 0.02533840760588646, Validation Loss: 0.027275199070572853, Learning Rate: 0.0008\n",
      "Epoch: 57, Training Loss: 0.027217935770750046, Validation Loss: 0.02891598641872406, Learning Rate: 0.0008\n",
      "Epoch: 58, Training Loss: 0.02715899609029293, Validation Loss: 0.030513131991028786, Learning Rate: 0.0008\n",
      "Epoch: 59, Training Loss: 0.025685373693704605, Validation Loss: 0.027991551905870438, Learning Rate: 0.0008\n",
      "Epoch: 60, Training Loss: 0.031107783317565918, Validation Loss: 0.033454954624176025, Learning Rate: 0.0008\n",
      "Epoch: 61, Training Loss: 0.03137410804629326, Validation Loss: 0.034205056726932526, Learning Rate: 0.0008\n",
      "Epoch: 62, Training Loss: 0.027074284851551056, Validation Loss: 0.028491798788309097, Learning Rate: 0.0008\n",
      "Epoch: 63, Training Loss: 0.027747830376029015, Validation Loss: 0.028510022908449173, Learning Rate: 0.0008\n",
      "Epoch: 64, Training Loss: 0.026272516697645187, Validation Loss: 0.029218221083283424, Learning Rate: 0.0008\n",
      "Epoch: 65, Training Loss: 0.02451390027999878, Validation Loss: 0.027255209162831306, Learning Rate: 0.0008\n",
      "Epoch: 66, Training Loss: 0.02740243449807167, Validation Loss: 0.028487298637628555, Learning Rate: 0.0008\n",
      "Epoch: 67, Training Loss: 0.02557084895670414, Validation Loss: 0.026770981028676033, Learning Rate: 0.0008\n",
      "Epoch: 68, Training Loss: 0.025639817118644714, Validation Loss: 0.02875029295682907, Learning Rate: 0.0008\n",
      "Epoch: 69, Training Loss: 0.024717209860682487, Validation Loss: 0.02732030302286148, Learning Rate: 0.0008\n",
      "Epoch: 70, Training Loss: 0.026251135393977165, Validation Loss: 0.02907678484916687, Learning Rate: 0.0008\n",
      "Epoch: 71, Training Loss: 0.02880546636879444, Validation Loss: 0.03417977690696716, Learning Rate: 0.0008\n",
      "Epoch: 72, Training Loss: 0.024410884827375412, Validation Loss: 0.026395514607429504, Learning Rate: 0.0008\n",
      "Epoch: 73, Training Loss: 0.02619907632470131, Validation Loss: 0.030179359018802643, Learning Rate: 0.0008\n",
      "Epoch: 74, Training Loss: 0.025171132758259773, Validation Loss: 0.0263674333691597, Learning Rate: 0.0008\n",
      "Epoch: 75, Training Loss: 0.028004242107272148, Validation Loss: 0.03150274232029915, Learning Rate: 0.0008\n",
      "Epoch: 76, Training Loss: 0.0237897802144289, Validation Loss: 0.025655578821897507, Learning Rate: 0.0008\n",
      "Epoch: 77, Training Loss: 0.023344269022345543, Validation Loss: 0.02660895138978958, Learning Rate: 0.0008\n",
      "Epoch: 78, Training Loss: 0.0254506953060627, Validation Loss: 0.029373275116086006, Learning Rate: 0.0008\n",
      "Epoch: 79, Training Loss: 0.022514689713716507, Validation Loss: 0.025703925639390945, Learning Rate: 0.0008\n",
      "Epoch: 80, Training Loss: 0.024617256596684456, Validation Loss: 0.02714959904551506, Learning Rate: 0.0008\n",
      "Epoch: 81, Training Loss: 0.023620054125785828, Validation Loss: 0.026084696874022484, Learning Rate: 0.0008\n",
      "Epoch: 82, Training Loss: 0.02430483140051365, Validation Loss: 0.027153007686138153, Learning Rate: 0.0008\n",
      "Epoch: 83, Training Loss: 0.023640785366296768, Validation Loss: 0.02744480036199093, Learning Rate: 0.0008\n",
      "Epoch: 84, Training Loss: 0.02411237731575966, Validation Loss: 0.025241941213607788, Learning Rate: 0.0008\n",
      "Epoch: 85, Training Loss: 0.02632070891559124, Validation Loss: 0.027829859405755997, Learning Rate: 0.0008\n",
      "Epoch: 86, Training Loss: 0.03033960796892643, Validation Loss: 0.03353681042790413, Learning Rate: 0.0008\n",
      "Epoch: 87, Training Loss: 0.027585307136178017, Validation Loss: 0.030275290831923485, Learning Rate: 0.0008\n",
      "Epoch: 88, Training Loss: 0.023589670658111572, Validation Loss: 0.026802681386470795, Learning Rate: 0.0008\n",
      "Epoch: 89, Training Loss: 0.027273153886198997, Validation Loss: 0.03075205162167549, Learning Rate: 0.0008\n",
      "Epoch: 90, Training Loss: 0.023090558126568794, Validation Loss: 0.02531328797340393, Learning Rate: 0.0008\n",
      "Epoch: 91, Training Loss: 0.027587076649069786, Validation Loss: 0.030711369588971138, Learning Rate: 0.0008\n",
      "Epoch: 92, Training Loss: 0.02723858878016472, Validation Loss: 0.03038770891726017, Learning Rate: 0.0008\n",
      "Epoch: 93, Training Loss: 0.02503287047147751, Validation Loss: 0.026393739506602287, Learning Rate: 0.0008\n",
      "Epoch: 94, Training Loss: 0.022861625999212265, Validation Loss: 0.024569248780608177, Learning Rate: 0.0008\n",
      "Epoch: 95, Training Loss: 0.022182036191225052, Validation Loss: 0.025191791355609894, Learning Rate: 0.0008\n",
      "Epoch: 96, Training Loss: 0.02435206063091755, Validation Loss: 0.02918478660285473, Learning Rate: 0.0008\n",
      "Epoch: 97, Training Loss: 0.029186509549617767, Validation Loss: 0.03160068765282631, Learning Rate: 0.0008\n",
      "Epoch: 98, Training Loss: 0.025202106684446335, Validation Loss: 0.027459125965833664, Learning Rate: 0.0008\n",
      "Epoch: 99, Training Loss: 0.021916605532169342, Validation Loss: 0.02356836572289467, Learning Rate: 0.0008\n",
      "Epoch: 100, Training Loss: 0.021269917488098145, Validation Loss: 0.022851960733532906, Learning Rate: 0.00064\n",
      "Epoch: 101, Training Loss: 0.026166386902332306, Validation Loss: 0.028604384511709213, Learning Rate: 0.00064\n",
      "Epoch: 102, Training Loss: 0.0217891838401556, Validation Loss: 0.024224763736128807, Learning Rate: 0.00064\n",
      "Epoch: 103, Training Loss: 0.020717229694128036, Validation Loss: 0.022925468161702156, Learning Rate: 0.00064\n",
      "Epoch: 104, Training Loss: 0.021635007113218307, Validation Loss: 0.023487553000450134, Learning Rate: 0.00064\n",
      "Epoch: 105, Training Loss: 0.020517466589808464, Validation Loss: 0.023601310327649117, Learning Rate: 0.00064\n",
      "Epoch: 106, Training Loss: 0.021806342527270317, Validation Loss: 0.02526981383562088, Learning Rate: 0.00064\n",
      "Epoch: 107, Training Loss: 0.021146854385733604, Validation Loss: 0.02343250997364521, Learning Rate: 0.00064\n",
      "Epoch: 108, Training Loss: 0.02359076961874962, Validation Loss: 0.02596334181725979, Learning Rate: 0.00064\n",
      "Epoch: 109, Training Loss: 0.020500944927334785, Validation Loss: 0.023049069568514824, Learning Rate: 0.00064\n",
      "Epoch: 110, Training Loss: 0.02405548095703125, Validation Loss: 0.02569027990102768, Learning Rate: 0.00064\n",
      "Epoch: 111, Training Loss: 0.02249062992632389, Validation Loss: 0.0257341880351305, Learning Rate: 0.00064\n",
      "Epoch: 112, Training Loss: 0.021262751892209053, Validation Loss: 0.024665527045726776, Learning Rate: 0.00064\n",
      "Epoch: 113, Training Loss: 0.02017662301659584, Validation Loss: 0.022299427539110184, Learning Rate: 0.00064\n",
      "Epoch: 114, Training Loss: 0.02040930651128292, Validation Loss: 0.023234983906149864, Learning Rate: 0.00064\n",
      "Epoch: 115, Training Loss: 0.022250428795814514, Validation Loss: 0.024474384263157845, Learning Rate: 0.00064\n",
      "Epoch: 116, Training Loss: 0.026188774034380913, Validation Loss: 0.028591761365532875, Learning Rate: 0.00064\n",
      "Epoch: 117, Training Loss: 0.03487028554081917, Validation Loss: 0.036470431834459305, Learning Rate: 0.00064\n",
      "Epoch: 118, Training Loss: 0.023802945390343666, Validation Loss: 0.025901611894369125, Learning Rate: 0.00064\n",
      "Epoch: 119, Training Loss: 0.02076583541929722, Validation Loss: 0.023656126111745834, Learning Rate: 0.00064\n",
      "Epoch: 120, Training Loss: 0.021592896431684494, Validation Loss: 0.023486629128456116, Learning Rate: 0.00064\n",
      "Epoch: 121, Training Loss: 0.022139860317111015, Validation Loss: 0.023352205753326416, Learning Rate: 0.00064\n",
      "Epoch: 122, Training Loss: 0.022192511707544327, Validation Loss: 0.026103364303708076, Learning Rate: 0.00064\n",
      "Epoch: 123, Training Loss: 0.021695274859666824, Validation Loss: 0.023937495425343513, Learning Rate: 0.00064\n",
      "Epoch: 124, Training Loss: 0.021524498239159584, Validation Loss: 0.023935480043292046, Learning Rate: 0.00064\n",
      "Epoch: 125, Training Loss: 0.019665926694869995, Validation Loss: 0.022187404334545135, Learning Rate: 0.00064\n",
      "Epoch: 126, Training Loss: 0.0221065916121006, Validation Loss: 0.025177691131830215, Learning Rate: 0.00064\n",
      "Epoch: 127, Training Loss: 0.019856439903378487, Validation Loss: 0.02245224453508854, Learning Rate: 0.00064\n",
      "Epoch: 128, Training Loss: 0.02088315412402153, Validation Loss: 0.025535715743899345, Learning Rate: 0.00064\n",
      "Epoch: 129, Training Loss: 0.021313153207302094, Validation Loss: 0.02260364219546318, Learning Rate: 0.00064\n",
      "Epoch: 130, Training Loss: 0.020602649077773094, Validation Loss: 0.0230410173535347, Learning Rate: 0.00064\n",
      "Epoch: 131, Training Loss: 0.02498934231698513, Validation Loss: 0.02834431454539299, Learning Rate: 0.00064\n",
      "Epoch: 132, Training Loss: 0.02255469560623169, Validation Loss: 0.025413429364562035, Learning Rate: 0.00064\n",
      "Epoch: 133, Training Loss: 0.021476948633790016, Validation Loss: 0.02427300624549389, Learning Rate: 0.00064\n",
      "Epoch: 134, Training Loss: 0.021649070084095, Validation Loss: 0.024844851344823837, Learning Rate: 0.00064\n",
      "Epoch: 135, Training Loss: 0.02133863791823387, Validation Loss: 0.024197086691856384, Learning Rate: 0.00064\n",
      "Epoch: 136, Training Loss: 0.020971160382032394, Validation Loss: 0.023922916501760483, Learning Rate: 0.00064\n",
      "Epoch: 137, Training Loss: 0.021047277376055717, Validation Loss: 0.0240255706012249, Learning Rate: 0.00064\n",
      "Epoch: 138, Training Loss: 0.02475380338728428, Validation Loss: 0.02696843072772026, Learning Rate: 0.00064\n",
      "Epoch: 139, Training Loss: 0.02227167971432209, Validation Loss: 0.02466924674808979, Learning Rate: 0.00064\n",
      "Epoch: 140, Training Loss: 0.022098951041698456, Validation Loss: 0.02418198622763157, Learning Rate: 0.00064\n",
      "Epoch: 141, Training Loss: 0.020788945257663727, Validation Loss: 0.023143576458096504, Learning Rate: 0.00064\n",
      "Epoch: 142, Training Loss: 0.01992526464164257, Validation Loss: 0.02253047376871109, Learning Rate: 0.00064\n",
      "Epoch: 143, Training Loss: 0.02036425471305847, Validation Loss: 0.02274968847632408, Learning Rate: 0.00064\n",
      "Epoch: 144, Training Loss: 0.021618124097585678, Validation Loss: 0.023405946791172028, Learning Rate: 0.00064\n",
      "Epoch: 145, Training Loss: 0.019413020461797714, Validation Loss: 0.021575624123215675, Learning Rate: 0.00064\n",
      "Epoch: 146, Training Loss: 0.022121844813227654, Validation Loss: 0.02491838112473488, Learning Rate: 0.00064\n",
      "Epoch: 147, Training Loss: 0.02031616121530533, Validation Loss: 0.022810278460383415, Learning Rate: 0.00064\n",
      "Epoch: 148, Training Loss: 0.024304434657096863, Validation Loss: 0.027460366487503052, Learning Rate: 0.00064\n",
      "Epoch: 149, Training Loss: 0.021239081397652626, Validation Loss: 0.023958276957273483, Learning Rate: 0.00064\n",
      "Epoch: 150, Training Loss: 0.018856504932045937, Validation Loss: 0.021374627947807312, Learning Rate: 0.0005120000000000001\n",
      "Epoch: 151, Training Loss: 0.019001731649041176, Validation Loss: 0.021633794531226158, Learning Rate: 0.0005120000000000001\n",
      "Epoch: 152, Training Loss: 0.019965991377830505, Validation Loss: 0.021484563127160072, Learning Rate: 0.0005120000000000001\n",
      "Epoch: 153, Training Loss: 0.01890174113214016, Validation Loss: 0.02106259949505329, Learning Rate: 0.0005120000000000001\n",
      "Epoch: 154, Training Loss: 0.020010318607091904, Validation Loss: 0.022377856075763702, Learning Rate: 0.0005120000000000001\n",
      "Epoch: 155, Training Loss: 0.019415399059653282, Validation Loss: 0.021516259759664536, Learning Rate: 0.0005120000000000001\n",
      "Epoch: 156, Training Loss: 0.019426075741648674, Validation Loss: 0.022168245166540146, Learning Rate: 0.0005120000000000001\n",
      "Epoch: 157, Training Loss: 0.018778018653392792, Validation Loss: 0.02074538730084896, Learning Rate: 0.0005120000000000001\n",
      "Epoch: 158, Training Loss: 0.020309265702962875, Validation Loss: 0.02246963419020176, Learning Rate: 0.0005120000000000001\n",
      "Epoch: 159, Training Loss: 0.019198285415768623, Validation Loss: 0.02157086320221424, Learning Rate: 0.0005120000000000001\n",
      "Epoch: 160, Training Loss: 0.01895063742995262, Validation Loss: 0.02147074043750763, Learning Rate: 0.0005120000000000001\n",
      "Epoch: 161, Training Loss: 0.02058989740908146, Validation Loss: 0.02288147807121277, Learning Rate: 0.0005120000000000001\n",
      "Epoch: 162, Training Loss: 0.018922900781035423, Validation Loss: 0.02222103625535965, Learning Rate: 0.0005120000000000001\n",
      "Epoch: 163, Training Loss: 0.01846366748213768, Validation Loss: 0.02069118060171604, Learning Rate: 0.0005120000000000001\n",
      "Epoch: 164, Training Loss: 0.019626494497060776, Validation Loss: 0.022650858387351036, Learning Rate: 0.0005120000000000001\n",
      "Epoch: 165, Training Loss: 0.01895381696522236, Validation Loss: 0.02194533869624138, Learning Rate: 0.0005120000000000001\n",
      "Epoch: 166, Training Loss: 0.018687153235077858, Validation Loss: 0.021008282899856567, Learning Rate: 0.0005120000000000001\n",
      "Epoch: 167, Training Loss: 0.0192575566470623, Validation Loss: 0.022474216297268867, Learning Rate: 0.0005120000000000001\n",
      "Epoch: 168, Training Loss: 0.02113192342221737, Validation Loss: 0.023107891902327538, Learning Rate: 0.0005120000000000001\n",
      "Epoch: 169, Training Loss: 0.018675541505217552, Validation Loss: 0.021547745913267136, Learning Rate: 0.0005120000000000001\n",
      "Epoch: 170, Training Loss: 0.020977376028895378, Validation Loss: 0.02410360611975193, Learning Rate: 0.0005120000000000001\n",
      "Epoch: 171, Training Loss: 0.018435947597026825, Validation Loss: 0.021407082676887512, Learning Rate: 0.0005120000000000001\n",
      "Epoch: 172, Training Loss: 0.019514095038175583, Validation Loss: 0.021477898582816124, Learning Rate: 0.0005120000000000001\n",
      "Epoch: 173, Training Loss: 0.018861614167690277, Validation Loss: 0.021345529705286026, Learning Rate: 0.0005120000000000001\n",
      "Epoch: 174, Training Loss: 0.018569353967905045, Validation Loss: 0.02037608064711094, Learning Rate: 0.0005120000000000001\n",
      "Epoch: 175, Training Loss: 0.017948711290955544, Validation Loss: 0.02019922062754631, Learning Rate: 0.0005120000000000001\n",
      "Epoch: 176, Training Loss: 0.019485600292682648, Validation Loss: 0.022609898820519447, Learning Rate: 0.0005120000000000001\n",
      "Epoch: 177, Training Loss: 0.018271904438734055, Validation Loss: 0.02025541104376316, Learning Rate: 0.0005120000000000001\n",
      "Epoch: 178, Training Loss: 0.019143598154187202, Validation Loss: 0.021690957248210907, Learning Rate: 0.0005120000000000001\n",
      "Epoch: 179, Training Loss: 0.021086785942316055, Validation Loss: 0.023496167734265327, Learning Rate: 0.0005120000000000001\n",
      "Epoch: 180, Training Loss: 0.018649408593773842, Validation Loss: 0.020528405904769897, Learning Rate: 0.0005120000000000001\n",
      "Epoch: 181, Training Loss: 0.01826476864516735, Validation Loss: 0.02094130404293537, Learning Rate: 0.0005120000000000001\n",
      "Epoch: 182, Training Loss: 0.018707122653722763, Validation Loss: 0.02024814859032631, Learning Rate: 0.0005120000000000001\n",
      "Epoch: 183, Training Loss: 0.021039728075265884, Validation Loss: 0.023154938593506813, Learning Rate: 0.0005120000000000001\n",
      "Epoch: 184, Training Loss: 0.018047597259283066, Validation Loss: 0.020468365401029587, Learning Rate: 0.0005120000000000001\n",
      "Epoch: 185, Training Loss: 0.017896216362714767, Validation Loss: 0.019941531121730804, Learning Rate: 0.0005120000000000001\n",
      "Epoch: 186, Training Loss: 0.019101975485682487, Validation Loss: 0.021979279816150665, Learning Rate: 0.0005120000000000001\n",
      "Epoch: 187, Training Loss: 0.02063959650695324, Validation Loss: 0.023165233433246613, Learning Rate: 0.0005120000000000001\n",
      "Epoch: 188, Training Loss: 0.018458476290106773, Validation Loss: 0.021484164521098137, Learning Rate: 0.0005120000000000001\n",
      "Epoch: 189, Training Loss: 0.019794637337327003, Validation Loss: 0.022001970559358597, Learning Rate: 0.0005120000000000001\n",
      "Epoch: 190, Training Loss: 0.01900251768529415, Validation Loss: 0.02162630669772625, Learning Rate: 0.0005120000000000001\n",
      "Epoch: 191, Training Loss: 0.01808035559952259, Validation Loss: 0.02071429416537285, Learning Rate: 0.0005120000000000001\n",
      "Epoch: 192, Training Loss: 0.018346089869737625, Validation Loss: 0.020733501762151718, Learning Rate: 0.0005120000000000001\n",
      "Epoch: 193, Training Loss: 0.01902792416512966, Validation Loss: 0.022160906344652176, Learning Rate: 0.0005120000000000001\n",
      "Epoch: 194, Training Loss: 0.01949961483478546, Validation Loss: 0.021996669471263885, Learning Rate: 0.0005120000000000001\n",
      "Epoch: 195, Training Loss: 0.018557874485850334, Validation Loss: 0.02057780511677265, Learning Rate: 0.0005120000000000001\n",
      "Epoch: 196, Training Loss: 0.021074501797556877, Validation Loss: 0.023378288373351097, Learning Rate: 0.0005120000000000001\n",
      "Epoch: 197, Training Loss: 0.01890704222023487, Validation Loss: 0.02166929841041565, Learning Rate: 0.0005120000000000001\n",
      "Epoch: 198, Training Loss: 0.019210372120141983, Validation Loss: 0.021347936242818832, Learning Rate: 0.0005120000000000001\n",
      "Epoch: 199, Training Loss: 0.020016727969050407, Validation Loss: 0.022164320573210716, Learning Rate: 0.0005120000000000001\n",
      "Epoch: 200, Training Loss: 0.01750762201845646, Validation Loss: 0.020206088200211525, Learning Rate: 0.0004096000000000001\n",
      "Epoch: 201, Training Loss: 0.01724526286125183, Validation Loss: 0.019660046324133873, Learning Rate: 0.0004096000000000001\n",
      "Epoch: 202, Training Loss: 0.017567718401551247, Validation Loss: 0.02005854994058609, Learning Rate: 0.0004096000000000001\n",
      "Epoch: 203, Training Loss: 0.019092217087745667, Validation Loss: 0.020771337673068047, Learning Rate: 0.0004096000000000001\n",
      "Epoch: 204, Training Loss: 0.018468813970685005, Validation Loss: 0.02073165774345398, Learning Rate: 0.0004096000000000001\n",
      "Epoch: 205, Training Loss: 0.01810365915298462, Validation Loss: 0.020472748205065727, Learning Rate: 0.0004096000000000001\n",
      "Epoch: 206, Training Loss: 0.017199194058775902, Validation Loss: 0.019527461379766464, Learning Rate: 0.0004096000000000001\n",
      "Epoch: 207, Training Loss: 0.017723726108670235, Validation Loss: 0.02005782537162304, Learning Rate: 0.0004096000000000001\n",
      "Epoch: 208, Training Loss: 0.017825374379754066, Validation Loss: 0.020531024783849716, Learning Rate: 0.0004096000000000001\n",
      "Epoch: 209, Training Loss: 0.017894364893436432, Validation Loss: 0.019712548702955246, Learning Rate: 0.0004096000000000001\n",
      "Epoch: 210, Training Loss: 0.018613586202263832, Validation Loss: 0.020729083567857742, Learning Rate: 0.0004096000000000001\n",
      "Epoch: 211, Training Loss: 0.01785786636173725, Validation Loss: 0.019990239292383194, Learning Rate: 0.0004096000000000001\n",
      "Epoch: 212, Training Loss: 0.01784721575677395, Validation Loss: 0.019770625978708267, Learning Rate: 0.0004096000000000001\n",
      "Epoch: 213, Training Loss: 0.017805011942982674, Validation Loss: 0.020103437826037407, Learning Rate: 0.0004096000000000001\n",
      "Epoch: 214, Training Loss: 0.017645370215177536, Validation Loss: 0.019763411954045296, Learning Rate: 0.0004096000000000001\n",
      "Epoch: 215, Training Loss: 0.018306776881217957, Validation Loss: 0.020910393446683884, Learning Rate: 0.0004096000000000001\n",
      "Epoch: 216, Training Loss: 0.018555937334895134, Validation Loss: 0.0206281878054142, Learning Rate: 0.0004096000000000001\n",
      "Epoch: 217, Training Loss: 0.018841825425624847, Validation Loss: 0.020892785862088203, Learning Rate: 0.0004096000000000001\n",
      "Epoch: 218, Training Loss: 0.01723669096827507, Validation Loss: 0.019721098244190216, Learning Rate: 0.0004096000000000001\n",
      "Epoch: 219, Training Loss: 0.017853179946541786, Validation Loss: 0.02070423774421215, Learning Rate: 0.0004096000000000001\n",
      "Epoch: 220, Training Loss: 0.018006581813097, Validation Loss: 0.02039550617337227, Learning Rate: 0.0004096000000000001\n",
      "Epoch: 221, Training Loss: 0.01819217950105667, Validation Loss: 0.020396174862980843, Learning Rate: 0.0004096000000000001\n",
      "Epoch: 222, Training Loss: 0.018165411427617073, Validation Loss: 0.02050383761525154, Learning Rate: 0.0004096000000000001\n",
      "Epoch: 223, Training Loss: 0.017605770379304886, Validation Loss: 0.020439231768250465, Learning Rate: 0.0004096000000000001\n",
      "Epoch: 224, Training Loss: 0.01983148604631424, Validation Loss: 0.022304844111204147, Learning Rate: 0.0004096000000000001\n",
      "Epoch: 225, Training Loss: 0.017682461068034172, Validation Loss: 0.020294150337576866, Learning Rate: 0.0004096000000000001\n",
      "Epoch: 226, Training Loss: 0.022348301485180855, Validation Loss: 0.0238192155957222, Learning Rate: 0.0004096000000000001\n",
      "Epoch: 227, Training Loss: 0.017331454902887344, Validation Loss: 0.01979980804026127, Learning Rate: 0.0004096000000000001\n",
      "Epoch: 228, Training Loss: 0.01710524596273899, Validation Loss: 0.019454145804047585, Learning Rate: 0.0004096000000000001\n",
      "Epoch: 229, Training Loss: 0.017491228878498077, Validation Loss: 0.02023424580693245, Learning Rate: 0.0004096000000000001\n",
      "Epoch: 230, Training Loss: 0.018465343862771988, Validation Loss: 0.020267341285943985, Learning Rate: 0.0004096000000000001\n",
      "Epoch: 231, Training Loss: 0.01764610782265663, Validation Loss: 0.01950008235871792, Learning Rate: 0.0004096000000000001\n",
      "Epoch: 232, Training Loss: 0.017284000292420387, Validation Loss: 0.020351041108369827, Learning Rate: 0.0004096000000000001\n",
      "Epoch: 233, Training Loss: 0.01793031580746174, Validation Loss: 0.020779311656951904, Learning Rate: 0.0004096000000000001\n",
      "Epoch: 234, Training Loss: 0.018310818821191788, Validation Loss: 0.02009551413357258, Learning Rate: 0.0004096000000000001\n",
      "Epoch: 235, Training Loss: 0.01803508773446083, Validation Loss: 0.019769074395298958, Learning Rate: 0.0004096000000000001\n",
      "Epoch: 236, Training Loss: 0.019787294790148735, Validation Loss: 0.022119808942079544, Learning Rate: 0.0004096000000000001\n",
      "Epoch: 237, Training Loss: 0.0186765193939209, Validation Loss: 0.020503710955381393, Learning Rate: 0.0004096000000000001\n",
      "Epoch: 238, Training Loss: 0.017397310584783554, Validation Loss: 0.01962924748659134, Learning Rate: 0.0004096000000000001\n",
      "Epoch: 239, Training Loss: 0.017754511907696724, Validation Loss: 0.01999559812247753, Learning Rate: 0.0004096000000000001\n",
      "Epoch: 240, Training Loss: 0.017157573252916336, Validation Loss: 0.01923239789903164, Learning Rate: 0.0004096000000000001\n",
      "Epoch: 241, Training Loss: 0.018749773502349854, Validation Loss: 0.020207097753882408, Learning Rate: 0.0004096000000000001\n",
      "Epoch: 242, Training Loss: 0.01752747967839241, Validation Loss: 0.01985812932252884, Learning Rate: 0.0004096000000000001\n",
      "Epoch: 243, Training Loss: 0.01724250614643097, Validation Loss: 0.019776247441768646, Learning Rate: 0.0004096000000000001\n",
      "Epoch: 244, Training Loss: 0.017544033005833626, Validation Loss: 0.01988815702497959, Learning Rate: 0.0004096000000000001\n",
      "Epoch: 245, Training Loss: 0.02114837057888508, Validation Loss: 0.02281608246266842, Learning Rate: 0.0004096000000000001\n",
      "Epoch: 246, Training Loss: 0.01719529554247856, Validation Loss: 0.01912720873951912, Learning Rate: 0.0004096000000000001\n",
      "Epoch: 247, Training Loss: 0.017490258440375328, Validation Loss: 0.01985422894358635, Learning Rate: 0.0004096000000000001\n",
      "Epoch: 248, Training Loss: 0.0178604144603014, Validation Loss: 0.019590754061937332, Learning Rate: 0.0004096000000000001\n",
      "Epoch: 249, Training Loss: 0.018962960690259933, Validation Loss: 0.020912159234285355, Learning Rate: 0.0004096000000000001\n",
      "Epoch: 250, Training Loss: 0.016886791214346886, Validation Loss: 0.018841572105884552, Learning Rate: 0.0003276800000000001\n",
      "Epoch: 251, Training Loss: 0.016282597556710243, Validation Loss: 0.018672415986657143, Learning Rate: 0.0003276800000000001\n",
      "Epoch: 252, Training Loss: 0.01672310382127762, Validation Loss: 0.018633315339684486, Learning Rate: 0.0003276800000000001\n",
      "Epoch: 253, Training Loss: 0.016680825501680374, Validation Loss: 0.018956826999783516, Learning Rate: 0.0003276800000000001\n",
      "Epoch: 254, Training Loss: 0.017829524353146553, Validation Loss: 0.019628843292593956, Learning Rate: 0.0003276800000000001\n",
      "Epoch: 255, Training Loss: 0.016991473734378815, Validation Loss: 0.01870546117424965, Learning Rate: 0.0003276800000000001\n",
      "Epoch: 256, Training Loss: 0.016659162938594818, Validation Loss: 0.018789436668157578, Learning Rate: 0.0003276800000000001\n",
      "Epoch: 257, Training Loss: 0.017161639407277107, Validation Loss: 0.01938057690858841, Learning Rate: 0.0003276800000000001\n",
      "Epoch: 258, Training Loss: 0.016448108479380608, Validation Loss: 0.01838742569088936, Learning Rate: 0.0003276800000000001\n",
      "Epoch: 259, Training Loss: 0.017410771921277046, Validation Loss: 0.020336778834462166, Learning Rate: 0.0003276800000000001\n",
      "Epoch: 260, Training Loss: 0.01779690384864807, Validation Loss: 0.019954314455389977, Learning Rate: 0.0003276800000000001\n",
      "Epoch: 261, Training Loss: 0.016613932326436043, Validation Loss: 0.018790628761053085, Learning Rate: 0.0003276800000000001\n",
      "Epoch: 262, Training Loss: 0.017502479255199432, Validation Loss: 0.019245486706495285, Learning Rate: 0.0003276800000000001\n",
      "Epoch: 263, Training Loss: 0.01679481938481331, Validation Loss: 0.01927061192691326, Learning Rate: 0.0003276800000000001\n",
      "Epoch: 264, Training Loss: 0.01667874865233898, Validation Loss: 0.01953202299773693, Learning Rate: 0.0003276800000000001\n",
      "Epoch: 265, Training Loss: 0.0178447924554348, Validation Loss: 0.01939123496413231, Learning Rate: 0.0003276800000000001\n",
      "Epoch: 266, Training Loss: 0.017327534034848213, Validation Loss: 0.01965808868408203, Learning Rate: 0.0003276800000000001\n",
      "Epoch: 267, Training Loss: 0.01666920818388462, Validation Loss: 0.018793782219290733, Learning Rate: 0.0003276800000000001\n",
      "Epoch: 268, Training Loss: 0.017578667029738426, Validation Loss: 0.019604012370109558, Learning Rate: 0.0003276800000000001\n",
      "Epoch: 269, Training Loss: 0.0179691631346941, Validation Loss: 0.020560801029205322, Learning Rate: 0.0003276800000000001\n",
      "Epoch: 270, Training Loss: 0.017096105962991714, Validation Loss: 0.01909148134291172, Learning Rate: 0.0003276800000000001\n",
      "Epoch: 271, Training Loss: 0.016593359410762787, Validation Loss: 0.018463099375367165, Learning Rate: 0.0003276800000000001\n",
      "Epoch: 272, Training Loss: 0.016988828778266907, Validation Loss: 0.018995799124240875, Learning Rate: 0.0003276800000000001\n",
      "Epoch: 273, Training Loss: 0.018797632306814194, Validation Loss: 0.021357472985982895, Learning Rate: 0.0003276800000000001\n",
      "Epoch: 274, Training Loss: 0.017137914896011353, Validation Loss: 0.020541779696941376, Learning Rate: 0.0003276800000000001\n",
      "Epoch: 275, Training Loss: 0.016680702567100525, Validation Loss: 0.01889069564640522, Learning Rate: 0.0003276800000000001\n",
      "Epoch: 276, Training Loss: 0.01654973439872265, Validation Loss: 0.018991079181432724, Learning Rate: 0.0003276800000000001\n",
      "Epoch: 277, Training Loss: 0.016894683241844177, Validation Loss: 0.01926410384476185, Learning Rate: 0.0003276800000000001\n",
      "Epoch: 278, Training Loss: 0.017043765634298325, Validation Loss: 0.018543213605880737, Learning Rate: 0.0003276800000000001\n",
      "Epoch: 279, Training Loss: 0.016661737114191055, Validation Loss: 0.01883554458618164, Learning Rate: 0.0003276800000000001\n",
      "Epoch: 280, Training Loss: 0.016823438927531242, Validation Loss: 0.01862606778740883, Learning Rate: 0.0003276800000000001\n",
      "Epoch: 281, Training Loss: 0.01684277132153511, Validation Loss: 0.018948832526803017, Learning Rate: 0.0003276800000000001\n",
      "Epoch: 282, Training Loss: 0.017189420759677887, Validation Loss: 0.019543509930372238, Learning Rate: 0.0003276800000000001\n",
      "Epoch: 283, Training Loss: 0.016622131690382957, Validation Loss: 0.018788959830999374, Learning Rate: 0.0003276800000000001\n",
      "Epoch: 284, Training Loss: 0.016832951456308365, Validation Loss: 0.01890796236693859, Learning Rate: 0.0003276800000000001\n",
      "Epoch: 285, Training Loss: 0.017021985724568367, Validation Loss: 0.01912144012749195, Learning Rate: 0.0003276800000000001\n",
      "Epoch: 286, Training Loss: 0.016838688403367996, Validation Loss: 0.019039541482925415, Learning Rate: 0.0003276800000000001\n",
      "Epoch: 287, Training Loss: 0.018726708367466927, Validation Loss: 0.02085113525390625, Learning Rate: 0.0003276800000000001\n",
      "Epoch: 288, Training Loss: 0.01671152375638485, Validation Loss: 0.018917355686426163, Learning Rate: 0.0003276800000000001\n",
      "Epoch: 289, Training Loss: 0.016672195866703987, Validation Loss: 0.019145460799336433, Learning Rate: 0.0003276800000000001\n",
      "Epoch: 290, Training Loss: 0.016198746860027313, Validation Loss: 0.018958386033773422, Learning Rate: 0.0003276800000000001\n",
      "Epoch: 291, Training Loss: 0.01676803082227707, Validation Loss: 0.018909120932221413, Learning Rate: 0.0003276800000000001\n",
      "Epoch: 292, Training Loss: 0.017988944426178932, Validation Loss: 0.019788706675171852, Learning Rate: 0.0003276800000000001\n",
      "Epoch: 293, Training Loss: 0.016638999804854393, Validation Loss: 0.018638744950294495, Learning Rate: 0.0003276800000000001\n",
      "Epoch: 294, Training Loss: 0.0169559083878994, Validation Loss: 0.01926380768418312, Learning Rate: 0.0003276800000000001\n",
      "Epoch: 295, Training Loss: 0.016655193641781807, Validation Loss: 0.018465999513864517, Learning Rate: 0.0003276800000000001\n",
      "Epoch: 296, Training Loss: 0.01640084758400917, Validation Loss: 0.01886356994509697, Learning Rate: 0.0003276800000000001\n",
      "Epoch: 297, Training Loss: 0.016766950488090515, Validation Loss: 0.018578454852104187, Learning Rate: 0.0003276800000000001\n",
      "Epoch: 298, Training Loss: 0.016488656401634216, Validation Loss: 0.018495013937354088, Learning Rate: 0.0003276800000000001\n",
      "Epoch: 299, Training Loss: 0.016972707584500313, Validation Loss: 0.018450506031513214, Learning Rate: 0.0003276800000000001\n",
      "Epoch: 300, Training Loss: 0.01677379384636879, Validation Loss: 0.01893194578588009, Learning Rate: 0.0002621440000000001\n",
      "Epoch: 301, Training Loss: 0.016021624207496643, Validation Loss: 0.018440742045640945, Learning Rate: 0.0002621440000000001\n",
      "Epoch: 302, Training Loss: 0.016181405633687973, Validation Loss: 0.018310844898223877, Learning Rate: 0.0002621440000000001\n",
      "Epoch: 303, Training Loss: 0.015913881361484528, Validation Loss: 0.01826961524784565, Learning Rate: 0.0002621440000000001\n",
      "Epoch: 304, Training Loss: 0.016488930210471153, Validation Loss: 0.018654542043805122, Learning Rate: 0.0002621440000000001\n",
      "Epoch: 305, Training Loss: 0.016335003077983856, Validation Loss: 0.018592605367302895, Learning Rate: 0.0002621440000000001\n",
      "Epoch: 306, Training Loss: 0.01660294644534588, Validation Loss: 0.01899600774049759, Learning Rate: 0.0002621440000000001\n",
      "Epoch: 307, Training Loss: 0.01586606726050377, Validation Loss: 0.01798064447939396, Learning Rate: 0.0002621440000000001\n",
      "Epoch: 308, Training Loss: 0.016677964478731155, Validation Loss: 0.0186067633330822, Learning Rate: 0.0002621440000000001\n",
      "Epoch: 309, Training Loss: 0.0159438606351614, Validation Loss: 0.018350858241319656, Learning Rate: 0.0002621440000000001\n",
      "Epoch: 310, Training Loss: 0.016549671068787575, Validation Loss: 0.018533486872911453, Learning Rate: 0.0002621440000000001\n",
      "Epoch: 311, Training Loss: 0.01650875434279442, Validation Loss: 0.018789120018482208, Learning Rate: 0.0002621440000000001\n",
      "Epoch: 312, Training Loss: 0.016642892733216286, Validation Loss: 0.01883847452700138, Learning Rate: 0.0002621440000000001\n",
      "Epoch: 313, Training Loss: 0.01607498712837696, Validation Loss: 0.018834188580513, Learning Rate: 0.0002621440000000001\n",
      "Epoch: 314, Training Loss: 0.01618737354874611, Validation Loss: 0.01820625178515911, Learning Rate: 0.0002621440000000001\n",
      "Epoch: 315, Training Loss: 0.01590437814593315, Validation Loss: 0.018101025372743607, Learning Rate: 0.0002621440000000001\n",
      "Epoch: 316, Training Loss: 0.016167130321264267, Validation Loss: 0.018362024798989296, Learning Rate: 0.0002621440000000001\n",
      "Epoch: 317, Training Loss: 0.017137978225946426, Validation Loss: 0.019418897107243538, Learning Rate: 0.0002621440000000001\n",
      "Epoch: 318, Training Loss: 0.01664869673550129, Validation Loss: 0.019636472687125206, Learning Rate: 0.0002621440000000001\n",
      "Epoch: 319, Training Loss: 0.015995163470506668, Validation Loss: 0.018080495297908783, Learning Rate: 0.0002621440000000001\n",
      "Epoch: 320, Training Loss: 0.015797950327396393, Validation Loss: 0.018061023205518723, Learning Rate: 0.0002621440000000001\n",
      "Epoch: 321, Training Loss: 0.016108345240354538, Validation Loss: 0.01861525885760784, Learning Rate: 0.0002621440000000001\n",
      "Epoch: 322, Training Loss: 0.016433734446763992, Validation Loss: 0.018698543310165405, Learning Rate: 0.0002621440000000001\n",
      "Epoch: 323, Training Loss: 0.015998857095837593, Validation Loss: 0.018546270206570625, Learning Rate: 0.0002621440000000001\n",
      "Epoch: 324, Training Loss: 0.01594506949186325, Validation Loss: 0.018165716901421547, Learning Rate: 0.0002621440000000001\n",
      "Epoch: 325, Training Loss: 0.016725465655326843, Validation Loss: 0.018769217655062675, Learning Rate: 0.0002621440000000001\n",
      "Epoch: 326, Training Loss: 0.01596396416425705, Validation Loss: 0.01825031079351902, Learning Rate: 0.0002621440000000001\n",
      "Epoch: 327, Training Loss: 0.016449876129627228, Validation Loss: 0.01877623423933983, Learning Rate: 0.0002621440000000001\n",
      "Epoch: 328, Training Loss: 0.016237152740359306, Validation Loss: 0.018140707165002823, Learning Rate: 0.0002621440000000001\n",
      "Epoch: 329, Training Loss: 0.01613963395357132, Validation Loss: 0.01830967888236046, Learning Rate: 0.0002621440000000001\n",
      "Epoch: 330, Training Loss: 0.016210408881306648, Validation Loss: 0.01829678565263748, Learning Rate: 0.0002621440000000001\n",
      "Epoch: 331, Training Loss: 0.015942037105560303, Validation Loss: 0.0182059183716774, Learning Rate: 0.0002621440000000001\n",
      "Epoch: 332, Training Loss: 0.015550045296549797, Validation Loss: 0.0175950787961483, Learning Rate: 0.0002621440000000001\n",
      "Epoch: 333, Training Loss: 0.0166629608720541, Validation Loss: 0.01881428435444832, Learning Rate: 0.0002621440000000001\n",
      "Epoch: 334, Training Loss: 0.016590170562267303, Validation Loss: 0.01813383400440216, Learning Rate: 0.0002621440000000001\n",
      "Epoch: 335, Training Loss: 0.01639041304588318, Validation Loss: 0.01875975914299488, Learning Rate: 0.0002621440000000001\n",
      "Epoch: 336, Training Loss: 0.01692713424563408, Validation Loss: 0.019591551274061203, Learning Rate: 0.0002621440000000001\n",
      "Epoch: 337, Training Loss: 0.01644083857536316, Validation Loss: 0.01868349313735962, Learning Rate: 0.0002621440000000001\n",
      "Epoch: 338, Training Loss: 0.015949783846735954, Validation Loss: 0.01827121526002884, Learning Rate: 0.0002621440000000001\n",
      "Epoch: 339, Training Loss: 0.01820668764412403, Validation Loss: 0.020743869245052338, Learning Rate: 0.0002621440000000001\n",
      "Epoch: 340, Training Loss: 0.01666342094540596, Validation Loss: 0.019038435071706772, Learning Rate: 0.0002621440000000001\n",
      "Epoch: 341, Training Loss: 0.016028914600610733, Validation Loss: 0.018397489562630653, Learning Rate: 0.0002621440000000001\n",
      "Epoch: 342, Training Loss: 0.016783809289336205, Validation Loss: 0.018500637263059616, Learning Rate: 0.0002621440000000001\n",
      "Epoch: 343, Training Loss: 0.016251301392912865, Validation Loss: 0.019014298915863037, Learning Rate: 0.0002621440000000001\n",
      "Epoch: 344, Training Loss: 0.016003787517547607, Validation Loss: 0.0181710384786129, Learning Rate: 0.0002621440000000001\n",
      "Epoch: 345, Training Loss: 0.015571312978863716, Validation Loss: 0.01783093810081482, Learning Rate: 0.0002621440000000001\n",
      "Epoch: 346, Training Loss: 0.015799636021256447, Validation Loss: 0.01788237877190113, Learning Rate: 0.0002621440000000001\n",
      "Epoch: 347, Training Loss: 0.01600695587694645, Validation Loss: 0.018239937722682953, Learning Rate: 0.0002621440000000001\n",
      "Epoch: 348, Training Loss: 0.01667395979166031, Validation Loss: 0.019382933154702187, Learning Rate: 0.0002621440000000001\n",
      "Epoch: 349, Training Loss: 0.01651993952691555, Validation Loss: 0.01882391981780529, Learning Rate: 0.0002621440000000001\n",
      "Epoch: 350, Training Loss: 0.015811370685696602, Validation Loss: 0.018335767090320587, Learning Rate: 0.00020971520000000012\n",
      "Epoch: 351, Training Loss: 0.015855859965085983, Validation Loss: 0.01831425353884697, Learning Rate: 0.00020971520000000012\n",
      "Epoch: 352, Training Loss: 0.017076632007956505, Validation Loss: 0.01969752460718155, Learning Rate: 0.00020971520000000012\n",
      "Epoch: 353, Training Loss: 0.016160083934664726, Validation Loss: 0.01792391948401928, Learning Rate: 0.00020971520000000012\n",
      "Epoch: 354, Training Loss: 0.01571359671652317, Validation Loss: 0.017938893288373947, Learning Rate: 0.00020971520000000012\n",
      "Epoch: 355, Training Loss: 0.015423594042658806, Validation Loss: 0.017865894362330437, Learning Rate: 0.00020971520000000012\n",
      "Epoch: 356, Training Loss: 0.015546813607215881, Validation Loss: 0.0178020428866148, Learning Rate: 0.00020971520000000012\n",
      "Epoch: 357, Training Loss: 0.015560449101030827, Validation Loss: 0.017793606966733932, Learning Rate: 0.00020971520000000012\n",
      "Epoch: 358, Training Loss: 0.015736177563667297, Validation Loss: 0.01829027384519577, Learning Rate: 0.00020971520000000012\n",
      "Epoch: 359, Training Loss: 0.01542844157665968, Validation Loss: 0.01756405085325241, Learning Rate: 0.00020971520000000012\n",
      "Epoch: 360, Training Loss: 0.016399435698986053, Validation Loss: 0.018617916852235794, Learning Rate: 0.00020971520000000012\n",
      "Epoch: 361, Training Loss: 0.015543726272881031, Validation Loss: 0.0179163608700037, Learning Rate: 0.00020971520000000012\n",
      "Epoch: 362, Training Loss: 0.01626483164727688, Validation Loss: 0.01884331926703453, Learning Rate: 0.00020971520000000012\n",
      "Epoch: 363, Training Loss: 0.01581243798136711, Validation Loss: 0.018275944516062737, Learning Rate: 0.00020971520000000012\n",
      "Epoch: 364, Training Loss: 0.015501604415476322, Validation Loss: 0.01782291941344738, Learning Rate: 0.00020971520000000012\n",
      "Epoch: 365, Training Loss: 0.015703508630394936, Validation Loss: 0.017948847264051437, Learning Rate: 0.00020971520000000012\n",
      "Epoch: 366, Training Loss: 0.01550136599689722, Validation Loss: 0.018078500404953957, Learning Rate: 0.00020971520000000012\n",
      "Epoch: 367, Training Loss: 0.01551832351833582, Validation Loss: 0.018040219321846962, Learning Rate: 0.00020971520000000012\n",
      "Epoch: 368, Training Loss: 0.015774529427289963, Validation Loss: 0.01812426745891571, Learning Rate: 0.00020971520000000012\n",
      "Epoch: 369, Training Loss: 0.015765303745865822, Validation Loss: 0.018179116770625114, Learning Rate: 0.00020971520000000012\n",
      "Epoch: 370, Training Loss: 0.01591596193611622, Validation Loss: 0.018145661801099777, Learning Rate: 0.00020971520000000012\n",
      "Epoch: 371, Training Loss: 0.015720205381512642, Validation Loss: 0.01815202087163925, Learning Rate: 0.00020971520000000012\n",
      "Epoch: 372, Training Loss: 0.015773305669426918, Validation Loss: 0.018143242225050926, Learning Rate: 0.00020971520000000012\n",
      "Epoch: 373, Training Loss: 0.015455249696969986, Validation Loss: 0.017707139253616333, Learning Rate: 0.00020971520000000012\n",
      "Epoch: 374, Training Loss: 0.015736505389213562, Validation Loss: 0.017760900780558586, Learning Rate: 0.00020971520000000012\n",
      "Epoch: 375, Training Loss: 0.015704097226262093, Validation Loss: 0.018167026340961456, Learning Rate: 0.00020971520000000012\n",
      "Epoch: 376, Training Loss: 0.01595607027411461, Validation Loss: 0.018149107694625854, Learning Rate: 0.00020971520000000012\n",
      "Epoch: 377, Training Loss: 0.015520160086452961, Validation Loss: 0.01773138716816902, Learning Rate: 0.00020971520000000012\n",
      "Epoch: 378, Training Loss: 0.015738099813461304, Validation Loss: 0.01844564639031887, Learning Rate: 0.00020971520000000012\n",
      "Epoch: 379, Training Loss: 0.015375560149550438, Validation Loss: 0.01762343756854534, Learning Rate: 0.00020971520000000012\n",
      "Epoch: 380, Training Loss: 0.015520256944000721, Validation Loss: 0.017439590767025948, Learning Rate: 0.00020971520000000012\n",
      "Epoch: 381, Training Loss: 0.015331530012190342, Validation Loss: 0.017591675743460655, Learning Rate: 0.00020971520000000012\n",
      "Epoch: 382, Training Loss: 0.016159219667315483, Validation Loss: 0.01874786801636219, Learning Rate: 0.00020971520000000012\n",
      "Epoch: 383, Training Loss: 0.015550658106803894, Validation Loss: 0.01751573383808136, Learning Rate: 0.00020971520000000012\n",
      "Epoch: 384, Training Loss: 0.015900591388344765, Validation Loss: 0.018593614920973778, Learning Rate: 0.00020971520000000012\n",
      "Epoch: 385, Training Loss: 0.015298621729016304, Validation Loss: 0.017900416627526283, Learning Rate: 0.00020971520000000012\n",
      "Epoch: 386, Training Loss: 0.015869811177253723, Validation Loss: 0.017794013023376465, Learning Rate: 0.00020971520000000012\n",
      "Epoch: 387, Training Loss: 0.01567855291068554, Validation Loss: 0.018071023747324944, Learning Rate: 0.00020971520000000012\n",
      "Epoch: 388, Training Loss: 0.015462019480764866, Validation Loss: 0.01761833019554615, Learning Rate: 0.00020971520000000012\n",
      "Epoch: 389, Training Loss: 0.015901507809758186, Validation Loss: 0.018269047141075134, Learning Rate: 0.00020971520000000012\n",
      "Epoch: 390, Training Loss: 0.01538773626089096, Validation Loss: 0.01772793009877205, Learning Rate: 0.00020971520000000012\n",
      "Epoch: 391, Training Loss: 0.016576046124100685, Validation Loss: 0.01833425648510456, Learning Rate: 0.00020971520000000012\n",
      "Epoch: 392, Training Loss: 0.015363438054919243, Validation Loss: 0.017624709755182266, Learning Rate: 0.00020971520000000012\n",
      "Epoch: 393, Training Loss: 0.0155628751963377, Validation Loss: 0.017662547528743744, Learning Rate: 0.00020971520000000012\n",
      "Epoch: 394, Training Loss: 0.015421845950186253, Validation Loss: 0.017913199961185455, Learning Rate: 0.00020971520000000012\n",
      "Epoch: 395, Training Loss: 0.015276364982128143, Validation Loss: 0.017780758440494537, Learning Rate: 0.00020971520000000012\n",
      "Epoch: 396, Training Loss: 0.01573760248720646, Validation Loss: 0.01809585653245449, Learning Rate: 0.00020971520000000012\n",
      "Epoch: 397, Training Loss: 0.015509194694459438, Validation Loss: 0.01813315413892269, Learning Rate: 0.00020971520000000012\n",
      "Epoch: 398, Training Loss: 0.015996290370821953, Validation Loss: 0.018282493576407433, Learning Rate: 0.00020971520000000012\n",
      "Epoch: 399, Training Loss: 0.01576659269630909, Validation Loss: 0.01794108748435974, Learning Rate: 0.00020971520000000012\n",
      "Epoch: 400, Training Loss: 0.015214229002594948, Validation Loss: 0.01707119680941105, Learning Rate: 0.0001677721600000001\n",
      "Epoch: 401, Training Loss: 0.016520492732524872, Validation Loss: 0.01881527155637741, Learning Rate: 0.0001677721600000001\n",
      "Epoch: 402, Training Loss: 0.015726549550890923, Validation Loss: 0.017749901860952377, Learning Rate: 0.0001677721600000001\n",
      "Epoch: 403, Training Loss: 0.01577231101691723, Validation Loss: 0.01793806627392769, Learning Rate: 0.0001677721600000001\n",
      "Epoch: 404, Training Loss: 0.015344863757491112, Validation Loss: 0.017785657197237015, Learning Rate: 0.0001677721600000001\n",
      "Epoch: 405, Training Loss: 0.015134884044528008, Validation Loss: 0.017680127173662186, Learning Rate: 0.0001677721600000001\n",
      "Epoch: 406, Training Loss: 0.01519834715873003, Validation Loss: 0.01786804012954235, Learning Rate: 0.0001677721600000001\n",
      "Epoch: 407, Training Loss: 0.015357173047959805, Validation Loss: 0.01772124506533146, Learning Rate: 0.0001677721600000001\n",
      "Epoch: 408, Training Loss: 0.015160477720201015, Validation Loss: 0.01771553047001362, Learning Rate: 0.0001677721600000001\n",
      "Epoch: 409, Training Loss: 0.015022758394479752, Validation Loss: 0.017358973622322083, Learning Rate: 0.0001677721600000001\n",
      "Epoch: 410, Training Loss: 0.015297946520149708, Validation Loss: 0.017433155328035355, Learning Rate: 0.0001677721600000001\n",
      "Epoch: 411, Training Loss: 0.01557214930653572, Validation Loss: 0.01808970980346203, Learning Rate: 0.0001677721600000001\n",
      "Epoch: 412, Training Loss: 0.015293286181986332, Validation Loss: 0.017803119495511055, Learning Rate: 0.0001677721600000001\n",
      "Epoch: 413, Training Loss: 0.015210447832942009, Validation Loss: 0.017910761758685112, Learning Rate: 0.0001677721600000001\n",
      "Epoch: 414, Training Loss: 0.015727782621979713, Validation Loss: 0.018300628289580345, Learning Rate: 0.0001677721600000001\n",
      "Epoch: 415, Training Loss: 0.015343950130045414, Validation Loss: 0.017654208466410637, Learning Rate: 0.0001677721600000001\n",
      "Epoch: 416, Training Loss: 0.015378797426819801, Validation Loss: 0.018020568415522575, Learning Rate: 0.0001677721600000001\n",
      "Epoch: 417, Training Loss: 0.015232509933412075, Validation Loss: 0.01750902272760868, Learning Rate: 0.0001677721600000001\n",
      "Epoch: 418, Training Loss: 0.015082179568707943, Validation Loss: 0.017568403854966164, Learning Rate: 0.0001677721600000001\n",
      "Epoch: 419, Training Loss: 0.015168708749115467, Validation Loss: 0.01776677742600441, Learning Rate: 0.0001677721600000001\n",
      "Epoch: 420, Training Loss: 0.015423901379108429, Validation Loss: 0.017937153577804565, Learning Rate: 0.0001677721600000001\n",
      "Epoch: 421, Training Loss: 0.015804346650838852, Validation Loss: 0.018464546650648117, Learning Rate: 0.0001677721600000001\n",
      "Epoch: 422, Training Loss: 0.015066971071064472, Validation Loss: 0.01725163124501705, Learning Rate: 0.0001677721600000001\n",
      "Epoch: 423, Training Loss: 0.015039965510368347, Validation Loss: 0.017040764912962914, Learning Rate: 0.0001677721600000001\n",
      "Epoch: 424, Training Loss: 0.01513550616800785, Validation Loss: 0.0177620816975832, Learning Rate: 0.0001677721600000001\n",
      "Epoch: 425, Training Loss: 0.015586676076054573, Validation Loss: 0.01793827675282955, Learning Rate: 0.0001677721600000001\n",
      "Epoch: 426, Training Loss: 0.01577303558588028, Validation Loss: 0.01905899867415428, Learning Rate: 0.0001677721600000001\n",
      "Epoch: 427, Training Loss: 0.015122946351766586, Validation Loss: 0.017653489485383034, Learning Rate: 0.0001677721600000001\n",
      "Epoch: 428, Training Loss: 0.015141434036195278, Validation Loss: 0.017165815457701683, Learning Rate: 0.0001677721600000001\n",
      "Epoch: 429, Training Loss: 0.01507642213255167, Validation Loss: 0.01758125051856041, Learning Rate: 0.0001677721600000001\n",
      "Epoch: 430, Training Loss: 0.015340583398938179, Validation Loss: 0.018057959154248238, Learning Rate: 0.0001677721600000001\n",
      "Epoch: 431, Training Loss: 0.015056351199746132, Validation Loss: 0.01790059544146061, Learning Rate: 0.0001677721600000001\n",
      "Epoch: 432, Training Loss: 0.015223044902086258, Validation Loss: 0.01771758683025837, Learning Rate: 0.0001677721600000001\n",
      "Epoch: 433, Training Loss: 0.01507617998868227, Validation Loss: 0.017588816583156586, Learning Rate: 0.0001677721600000001\n",
      "Epoch: 434, Training Loss: 0.01514421310275793, Validation Loss: 0.017750831320881844, Learning Rate: 0.0001677721600000001\n",
      "Epoch: 435, Training Loss: 0.015075169503688812, Validation Loss: 0.017769310623407364, Learning Rate: 0.0001677721600000001\n",
      "Epoch: 436, Training Loss: 0.014979533851146698, Validation Loss: 0.01744157448410988, Learning Rate: 0.0001677721600000001\n",
      "Epoch: 437, Training Loss: 0.015323209576308727, Validation Loss: 0.017668791115283966, Learning Rate: 0.0001677721600000001\n",
      "Epoch: 438, Training Loss: 0.015305276028811932, Validation Loss: 0.01862005889415741, Learning Rate: 0.0001677721600000001\n",
      "Epoch: 439, Training Loss: 0.014934011735022068, Validation Loss: 0.017833493649959564, Learning Rate: 0.0001677721600000001\n",
      "Epoch: 440, Training Loss: 0.015949729830026627, Validation Loss: 0.018510013818740845, Learning Rate: 0.0001677721600000001\n",
      "Epoch: 441, Training Loss: 0.015076554380357265, Validation Loss: 0.017739590257406235, Learning Rate: 0.0001677721600000001\n",
      "Epoch: 442, Training Loss: 0.015430369414389133, Validation Loss: 0.01784825325012207, Learning Rate: 0.0001677721600000001\n",
      "Epoch: 443, Training Loss: 0.014971992000937462, Validation Loss: 0.017192982137203217, Learning Rate: 0.0001677721600000001\n",
      "Epoch: 444, Training Loss: 0.015197010710835457, Validation Loss: 0.01755591295659542, Learning Rate: 0.0001677721600000001\n",
      "Epoch: 445, Training Loss: 0.015063275583088398, Validation Loss: 0.017226984724402428, Learning Rate: 0.0001677721600000001\n",
      "Epoch: 446, Training Loss: 0.015080364421010017, Validation Loss: 0.017506832256913185, Learning Rate: 0.0001677721600000001\n",
      "Epoch: 447, Training Loss: 0.014916784130036831, Validation Loss: 0.017374416813254356, Learning Rate: 0.0001677721600000001\n",
      "Epoch: 448, Training Loss: 0.015428151935338974, Validation Loss: 0.018163306638598442, Learning Rate: 0.0001677721600000001\n",
      "Epoch: 449, Training Loss: 0.015026520937681198, Validation Loss: 0.017766660079360008, Learning Rate: 0.0001677721600000001\n",
      "Epoch: 450, Training Loss: 0.014856435358524323, Validation Loss: 0.017518186941742897, Learning Rate: 0.00013421772800000008\n",
      "Epoch: 451, Training Loss: 0.014812681823968887, Validation Loss: 0.017349548637866974, Learning Rate: 0.00013421772800000008\n",
      "Epoch: 452, Training Loss: 0.015011407434940338, Validation Loss: 0.017856968566775322, Learning Rate: 0.00013421772800000008\n",
      "Epoch: 453, Training Loss: 0.014878999441862106, Validation Loss: 0.01726902648806572, Learning Rate: 0.00013421772800000008\n",
      "Epoch: 454, Training Loss: 0.015002395957708359, Validation Loss: 0.018276087939739227, Learning Rate: 0.00013421772800000008\n",
      "Epoch: 455, Training Loss: 0.014871520921587944, Validation Loss: 0.017804522067308426, Learning Rate: 0.00013421772800000008\n",
      "Epoch: 456, Training Loss: 0.015050341375172138, Validation Loss: 0.01749580167233944, Learning Rate: 0.00013421772800000008\n",
      "Epoch: 457, Training Loss: 0.014656353741884232, Validation Loss: 0.017144402489066124, Learning Rate: 0.00013421772800000008\n",
      "Epoch: 458, Training Loss: 0.014686205424368382, Validation Loss: 0.01718699373304844, Learning Rate: 0.00013421772800000008\n",
      "Epoch: 459, Training Loss: 0.014680393971502781, Validation Loss: 0.01716753840446472, Learning Rate: 0.00013421772800000008\n",
      "Epoch: 460, Training Loss: 0.014758982695639133, Validation Loss: 0.017060045152902603, Learning Rate: 0.00013421772800000008\n",
      "Epoch: 461, Training Loss: 0.015252593904733658, Validation Loss: 0.017566226422786713, Learning Rate: 0.00013421772800000008\n",
      "Epoch: 462, Training Loss: 0.014902360737323761, Validation Loss: 0.017142463475465775, Learning Rate: 0.00013421772800000008\n",
      "Epoch: 463, Training Loss: 0.014834091067314148, Validation Loss: 0.01759181171655655, Learning Rate: 0.00013421772800000008\n",
      "Epoch: 464, Training Loss: 0.014685184694826603, Validation Loss: 0.0174738597124815, Learning Rate: 0.00013421772800000008\n",
      "Epoch: 465, Training Loss: 0.01530303806066513, Validation Loss: 0.01777750626206398, Learning Rate: 0.00013421772800000008\n",
      "Epoch: 466, Training Loss: 0.014702292159199715, Validation Loss: 0.01756606623530388, Learning Rate: 0.00013421772800000008\n",
      "Epoch: 467, Training Loss: 0.01518152840435505, Validation Loss: 0.01752856746315956, Learning Rate: 0.00013421772800000008\n",
      "Epoch: 468, Training Loss: 0.014708206988871098, Validation Loss: 0.01725000888109207, Learning Rate: 0.00013421772800000008\n",
      "Epoch: 469, Training Loss: 0.014713780954480171, Validation Loss: 0.017168719321489334, Learning Rate: 0.00013421772800000008\n",
      "Epoch: 470, Training Loss: 0.014801363460719585, Validation Loss: 0.01752753183245659, Learning Rate: 0.00013421772800000008\n",
      "Epoch: 471, Training Loss: 0.014756551012396812, Validation Loss: 0.017057662829756737, Learning Rate: 0.00013421772800000008\n",
      "Epoch: 472, Training Loss: 0.014988536946475506, Validation Loss: 0.017192816361784935, Learning Rate: 0.00013421772800000008\n",
      "Epoch: 473, Training Loss: 0.014710011892020702, Validation Loss: 0.01687300205230713, Learning Rate: 0.00013421772800000008\n",
      "Epoch: 474, Training Loss: 0.015442293137311935, Validation Loss: 0.01760917343199253, Learning Rate: 0.00013421772800000008\n",
      "Epoch: 475, Training Loss: 0.015145055018365383, Validation Loss: 0.01704367622733116, Learning Rate: 0.00013421772800000008\n",
      "Epoch: 476, Training Loss: 0.014748440124094486, Validation Loss: 0.01673801988363266, Learning Rate: 0.00013421772800000008\n",
      "Epoch: 477, Training Loss: 0.014646521769464016, Validation Loss: 0.016980433836579323, Learning Rate: 0.00013421772800000008\n",
      "Epoch: 478, Training Loss: 0.014631915837526321, Validation Loss: 0.017254451289772987, Learning Rate: 0.00013421772800000008\n",
      "Epoch: 479, Training Loss: 0.014586429111659527, Validation Loss: 0.01704327203333378, Learning Rate: 0.00013421772800000008\n",
      "Epoch: 480, Training Loss: 0.014760243706405163, Validation Loss: 0.017234869301319122, Learning Rate: 0.00013421772800000008\n",
      "Epoch: 481, Training Loss: 0.014646589756011963, Validation Loss: 0.01725747250020504, Learning Rate: 0.00013421772800000008\n",
      "Epoch: 482, Training Loss: 0.01449477020651102, Validation Loss: 0.017089644446969032, Learning Rate: 0.00013421772800000008\n",
      "Epoch: 483, Training Loss: 0.014720398932695389, Validation Loss: 0.017089297994971275, Learning Rate: 0.00013421772800000008\n",
      "Epoch: 484, Training Loss: 0.014561735093593597, Validation Loss: 0.01688472554087639, Learning Rate: 0.00013421772800000008\n",
      "Epoch: 485, Training Loss: 0.014846623875200748, Validation Loss: 0.017692940309643745, Learning Rate: 0.00013421772800000008\n",
      "Epoch: 486, Training Loss: 0.014586212113499641, Validation Loss: 0.01711849682033062, Learning Rate: 0.00013421772800000008\n",
      "Epoch: 487, Training Loss: 0.01472649909555912, Validation Loss: 0.017241889610886574, Learning Rate: 0.00013421772800000008\n",
      "Epoch: 488, Training Loss: 0.014686670154333115, Validation Loss: 0.01723935268819332, Learning Rate: 0.00013421772800000008\n",
      "Epoch: 489, Training Loss: 0.014658845029771328, Validation Loss: 0.01708161272108555, Learning Rate: 0.00013421772800000008\n",
      "Epoch: 490, Training Loss: 0.014832145534455776, Validation Loss: 0.017069190740585327, Learning Rate: 0.00013421772800000008\n",
      "Epoch: 491, Training Loss: 0.014731700532138348, Validation Loss: 0.01721549592912197, Learning Rate: 0.00013421772800000008\n",
      "Epoch: 492, Training Loss: 0.014657237567007542, Validation Loss: 0.01723996363580227, Learning Rate: 0.00013421772800000008\n",
      "Epoch: 493, Training Loss: 0.01504450011998415, Validation Loss: 0.01739053800702095, Learning Rate: 0.00013421772800000008\n",
      "Epoch: 494, Training Loss: 0.014477246440947056, Validation Loss: 0.016952497884631157, Learning Rate: 0.00013421772800000008\n",
      "Epoch: 495, Training Loss: 0.014610874466598034, Validation Loss: 0.017090197652578354, Learning Rate: 0.00013421772800000008\n",
      "Epoch: 496, Training Loss: 0.014665809459984303, Validation Loss: 0.01712729036808014, Learning Rate: 0.00013421772800000008\n",
      "Epoch: 497, Training Loss: 0.014893253333866596, Validation Loss: 0.017236236482858658, Learning Rate: 0.00013421772800000008\n",
      "Epoch: 498, Training Loss: 0.015096829272806644, Validation Loss: 0.017682623118162155, Learning Rate: 0.00013421772800000008\n",
      "Epoch: 499, Training Loss: 0.014658473432064056, Validation Loss: 0.017107171937823296, Learning Rate: 0.00013421772800000008\n",
      "Epoch: 500, Training Loss: 0.014409583061933517, Validation Loss: 0.01689344458281994, Learning Rate: 0.00010737418240000007\n",
      "Epoch: 501, Training Loss: 0.014567403122782707, Validation Loss: 0.01712573692202568, Learning Rate: 0.00010737418240000007\n",
      "Epoch: 502, Training Loss: 0.0143743259832263, Validation Loss: 0.016975341364741325, Learning Rate: 0.00010737418240000007\n",
      "Epoch: 503, Training Loss: 0.014470160938799381, Validation Loss: 0.016943620517849922, Learning Rate: 0.00010737418240000007\n",
      "Epoch: 504, Training Loss: 0.0144640589132905, Validation Loss: 0.01675860583782196, Learning Rate: 0.00010737418240000007\n",
      "Epoch: 505, Training Loss: 0.01431433204561472, Validation Loss: 0.01679724082350731, Learning Rate: 0.00010737418240000007\n",
      "Epoch: 506, Training Loss: 0.014448472298681736, Validation Loss: 0.01704181730747223, Learning Rate: 0.00010737418240000007\n",
      "Epoch: 507, Training Loss: 0.014362956397235394, Validation Loss: 0.016759341582655907, Learning Rate: 0.00010737418240000007\n",
      "Epoch: 508, Training Loss: 0.014398561790585518, Validation Loss: 0.01680046319961548, Learning Rate: 0.00010737418240000007\n",
      "Epoch: 509, Training Loss: 0.014373866841197014, Validation Loss: 0.01681239902973175, Learning Rate: 0.00010737418240000007\n",
      "Epoch: 510, Training Loss: 0.014819548465311527, Validation Loss: 0.016995232552289963, Learning Rate: 0.00010737418240000007\n",
      "Epoch: 511, Training Loss: 0.014413297176361084, Validation Loss: 0.01671794056892395, Learning Rate: 0.00010737418240000007\n",
      "Epoch: 512, Training Loss: 0.014399372972548008, Validation Loss: 0.017179369926452637, Learning Rate: 0.00010737418240000007\n",
      "Epoch: 513, Training Loss: 0.01435463409870863, Validation Loss: 0.01694342866539955, Learning Rate: 0.00010737418240000007\n",
      "Epoch: 514, Training Loss: 0.014506857842206955, Validation Loss: 0.01679769717156887, Learning Rate: 0.00010737418240000007\n",
      "Epoch: 515, Training Loss: 0.014380271546542645, Validation Loss: 0.016773905605077744, Learning Rate: 0.00010737418240000007\n",
      "Epoch: 516, Training Loss: 0.014404707588255405, Validation Loss: 0.016620203852653503, Learning Rate: 0.00010737418240000007\n",
      "Epoch: 517, Training Loss: 0.014405900612473488, Validation Loss: 0.016837088391184807, Learning Rate: 0.00010737418240000007\n",
      "Epoch: 518, Training Loss: 0.014356786385178566, Validation Loss: 0.01692502573132515, Learning Rate: 0.00010737418240000007\n",
      "Epoch: 519, Training Loss: 0.014323974028229713, Validation Loss: 0.016691144555807114, Learning Rate: 0.00010737418240000007\n",
      "Epoch: 520, Training Loss: 0.014787639491260052, Validation Loss: 0.017498403787612915, Learning Rate: 0.00010737418240000007\n",
      "Epoch: 521, Training Loss: 0.015268060378730297, Validation Loss: 0.017934909090399742, Learning Rate: 0.00010737418240000007\n",
      "Epoch: 522, Training Loss: 0.014489476568996906, Validation Loss: 0.017163032665848732, Learning Rate: 0.00010737418240000007\n",
      "Epoch: 523, Training Loss: 0.014279050752520561, Validation Loss: 0.016710754483938217, Learning Rate: 0.00010737418240000007\n",
      "Epoch: 524, Training Loss: 0.014817319810390472, Validation Loss: 0.017184901982545853, Learning Rate: 0.00010737418240000007\n",
      "Epoch: 525, Training Loss: 0.014437971636652946, Validation Loss: 0.01699991710484028, Learning Rate: 0.00010737418240000007\n",
      "Epoch: 526, Training Loss: 0.01475593727082014, Validation Loss: 0.017483597621321678, Learning Rate: 0.00010737418240000007\n",
      "Epoch: 527, Training Loss: 0.014263715595006943, Validation Loss: 0.016845496371388435, Learning Rate: 0.00010737418240000007\n",
      "Epoch: 528, Training Loss: 0.014384569600224495, Validation Loss: 0.01693039946258068, Learning Rate: 0.00010737418240000007\n",
      "Epoch: 529, Training Loss: 0.014403598383069038, Validation Loss: 0.017149390652775764, Learning Rate: 0.00010737418240000007\n",
      "Epoch: 530, Training Loss: 0.014324703253805637, Validation Loss: 0.01716565154492855, Learning Rate: 0.00010737418240000007\n",
      "Epoch: 531, Training Loss: 0.014624896459281445, Validation Loss: 0.01708992011845112, Learning Rate: 0.00010737418240000007\n",
      "Epoch: 532, Training Loss: 0.01430642418563366, Validation Loss: 0.016851164400577545, Learning Rate: 0.00010737418240000007\n",
      "Epoch: 533, Training Loss: 0.01436326652765274, Validation Loss: 0.016702478751540184, Learning Rate: 0.00010737418240000007\n",
      "Epoch: 534, Training Loss: 0.014370338059961796, Validation Loss: 0.01717212051153183, Learning Rate: 0.00010737418240000007\n",
      "Epoch: 535, Training Loss: 0.014260979369282722, Validation Loss: 0.0169641375541687, Learning Rate: 0.00010737418240000007\n",
      "Epoch: 536, Training Loss: 0.014841836877167225, Validation Loss: 0.017347833141684532, Learning Rate: 0.00010737418240000007\n",
      "Epoch: 537, Training Loss: 0.014389927498996258, Validation Loss: 0.016567757353186607, Learning Rate: 0.00010737418240000007\n",
      "Epoch: 538, Training Loss: 0.01467191893607378, Validation Loss: 0.017232956364750862, Learning Rate: 0.00010737418240000007\n",
      "Epoch: 539, Training Loss: 0.014401059597730637, Validation Loss: 0.016723915934562683, Learning Rate: 0.00010737418240000007\n",
      "Epoch: 540, Training Loss: 0.014364881440997124, Validation Loss: 0.017022326588630676, Learning Rate: 0.00010737418240000007\n",
      "Epoch: 541, Training Loss: 0.014210814610123634, Validation Loss: 0.01715908944606781, Learning Rate: 0.00010737418240000007\n",
      "Epoch: 542, Training Loss: 0.014615904539823532, Validation Loss: 0.017568035051226616, Learning Rate: 0.00010737418240000007\n",
      "Epoch: 543, Training Loss: 0.0144581887871027, Validation Loss: 0.017120661213994026, Learning Rate: 0.00010737418240000007\n",
      "Epoch: 544, Training Loss: 0.014112512581050396, Validation Loss: 0.016675051301717758, Learning Rate: 0.00010737418240000007\n",
      "Epoch: 545, Training Loss: 0.014180940575897694, Validation Loss: 0.017146805301308632, Learning Rate: 0.00010737418240000007\n",
      "Epoch: 546, Training Loss: 0.014173517934978008, Validation Loss: 0.016984179615974426, Learning Rate: 0.00010737418240000007\n",
      "Epoch: 547, Training Loss: 0.014226919040083885, Validation Loss: 0.016925044357776642, Learning Rate: 0.00010737418240000007\n",
      "Epoch: 548, Training Loss: 0.014183113351464272, Validation Loss: 0.01650874689221382, Learning Rate: 0.00010737418240000007\n",
      "Epoch: 549, Training Loss: 0.014128596521914005, Validation Loss: 0.01694069616496563, Learning Rate: 0.00010737418240000007\n",
      "Epoch: 550, Training Loss: 0.014045488089323044, Validation Loss: 0.016835132613778114, Learning Rate: 8.589934592000007e-05\n",
      "Epoch: 551, Training Loss: 0.014175845310091972, Validation Loss: 0.016854511573910713, Learning Rate: 8.589934592000007e-05\n",
      "Epoch: 552, Training Loss: 0.014188367873430252, Validation Loss: 0.016658606007695198, Learning Rate: 8.589934592000007e-05\n",
      "Epoch: 553, Training Loss: 0.013995656743645668, Validation Loss: 0.01678854413330555, Learning Rate: 8.589934592000007e-05\n",
      "Epoch: 554, Training Loss: 0.014068654738366604, Validation Loss: 0.016960175707936287, Learning Rate: 8.589934592000007e-05\n",
      "Epoch: 555, Training Loss: 0.01421639695763588, Validation Loss: 0.01708078570663929, Learning Rate: 8.589934592000007e-05\n",
      "Epoch: 556, Training Loss: 0.013992817141115665, Validation Loss: 0.016704820096492767, Learning Rate: 8.589934592000007e-05\n",
      "Epoch: 557, Training Loss: 0.014464021660387516, Validation Loss: 0.017199071124196053, Learning Rate: 8.589934592000007e-05\n",
      "Epoch: 558, Training Loss: 0.014198683202266693, Validation Loss: 0.016990913078188896, Learning Rate: 8.589934592000007e-05\n",
      "Epoch: 559, Training Loss: 0.013994102366268635, Validation Loss: 0.016873249784111977, Learning Rate: 8.589934592000007e-05\n",
      "Epoch: 560, Training Loss: 0.014015411958098412, Validation Loss: 0.01694048009812832, Learning Rate: 8.589934592000007e-05\n",
      "Epoch: 561, Training Loss: 0.013972783461213112, Validation Loss: 0.01692798174917698, Learning Rate: 8.589934592000007e-05\n",
      "Epoch: 562, Training Loss: 0.013950743712484837, Validation Loss: 0.016496311873197556, Learning Rate: 8.589934592000007e-05\n",
      "Epoch: 563, Training Loss: 0.014005822129547596, Validation Loss: 0.016594817861914635, Learning Rate: 8.589934592000007e-05\n",
      "Epoch: 564, Training Loss: 0.014001199044287205, Validation Loss: 0.01660510152578354, Learning Rate: 8.589934592000007e-05\n",
      "Epoch: 565, Training Loss: 0.01405044924467802, Validation Loss: 0.017015818506479263, Learning Rate: 8.589934592000007e-05\n",
      "Epoch: 566, Training Loss: 0.013871821574866772, Validation Loss: 0.016857197508215904, Learning Rate: 8.589934592000007e-05\n",
      "Epoch: 567, Training Loss: 0.013966099359095097, Validation Loss: 0.016749508678913116, Learning Rate: 8.589934592000007e-05\n",
      "Epoch: 568, Training Loss: 0.014005490578711033, Validation Loss: 0.016801906749606133, Learning Rate: 8.589934592000007e-05\n",
      "Epoch: 569, Training Loss: 0.014033479616045952, Validation Loss: 0.017021222040057182, Learning Rate: 8.589934592000007e-05\n",
      "Epoch: 570, Training Loss: 0.014035976491868496, Validation Loss: 0.01680724509060383, Learning Rate: 8.589934592000007e-05\n",
      "Epoch: 571, Training Loss: 0.014011086896061897, Validation Loss: 0.016937432810664177, Learning Rate: 8.589934592000007e-05\n",
      "Epoch: 572, Training Loss: 0.013899372890591621, Validation Loss: 0.0169378649443388, Learning Rate: 8.589934592000007e-05\n",
      "Epoch: 573, Training Loss: 0.014045570977032185, Validation Loss: 0.016718978062272072, Learning Rate: 8.589934592000007e-05\n",
      "Epoch: 574, Training Loss: 0.013849252834916115, Validation Loss: 0.016830729320645332, Learning Rate: 8.589934592000007e-05\n",
      "Epoch: 575, Training Loss: 0.01385341677814722, Validation Loss: 0.016632691025733948, Learning Rate: 8.589934592000007e-05\n",
      "Epoch: 576, Training Loss: 0.01384828519076109, Validation Loss: 0.016742685809731483, Learning Rate: 8.589934592000007e-05\n",
      "Epoch: 577, Training Loss: 0.01397497858852148, Validation Loss: 0.016913173720240593, Learning Rate: 8.589934592000007e-05\n",
      "Epoch: 578, Training Loss: 0.014131514355540276, Validation Loss: 0.016720524057745934, Learning Rate: 8.589934592000007e-05\n",
      "Epoch: 579, Training Loss: 0.013791450299322605, Validation Loss: 0.016594143584370613, Learning Rate: 8.589934592000007e-05\n",
      "Epoch: 580, Training Loss: 0.013971141539514065, Validation Loss: 0.016938060522079468, Learning Rate: 8.589934592000007e-05\n",
      "Epoch: 581, Training Loss: 0.014055252075195312, Validation Loss: 0.016957102343440056, Learning Rate: 8.589934592000007e-05\n",
      "Epoch: 582, Training Loss: 0.013811515644192696, Validation Loss: 0.016644027084112167, Learning Rate: 8.589934592000007e-05\n",
      "Epoch: 583, Training Loss: 0.013902762904763222, Validation Loss: 0.016728760674595833, Learning Rate: 8.589934592000007e-05\n",
      "Epoch: 584, Training Loss: 0.013781336136162281, Validation Loss: 0.0167813990265131, Learning Rate: 8.589934592000007e-05\n",
      "Epoch: 585, Training Loss: 0.01391529943794012, Validation Loss: 0.01670309342443943, Learning Rate: 8.589934592000007e-05\n",
      "Epoch: 586, Training Loss: 0.01421628799289465, Validation Loss: 0.017184244468808174, Learning Rate: 8.589934592000007e-05\n",
      "Epoch: 587, Training Loss: 0.01390241552144289, Validation Loss: 0.0167665034532547, Learning Rate: 8.589934592000007e-05\n",
      "Epoch: 588, Training Loss: 0.013859208673238754, Validation Loss: 0.01694074273109436, Learning Rate: 8.589934592000007e-05\n",
      "Epoch: 589, Training Loss: 0.013700890354812145, Validation Loss: 0.016445279121398926, Learning Rate: 8.589934592000007e-05\n",
      "Epoch: 590, Training Loss: 0.014141303487122059, Validation Loss: 0.01684742048382759, Learning Rate: 8.589934592000007e-05\n",
      "Epoch: 591, Training Loss: 0.013775553554296494, Validation Loss: 0.016649985685944557, Learning Rate: 8.589934592000007e-05\n",
      "Epoch: 592, Training Loss: 0.013810796663165092, Validation Loss: 0.01658085733652115, Learning Rate: 8.589934592000007e-05\n",
      "Epoch: 593, Training Loss: 0.01375754363834858, Validation Loss: 0.016977258026599884, Learning Rate: 8.589934592000007e-05\n",
      "Epoch: 594, Training Loss: 0.01373814232647419, Validation Loss: 0.01654992811381817, Learning Rate: 8.589934592000007e-05\n",
      "Epoch: 595, Training Loss: 0.013779371976852417, Validation Loss: 0.016578886657953262, Learning Rate: 8.589934592000007e-05\n",
      "Epoch: 596, Training Loss: 0.013899347744882107, Validation Loss: 0.016804244369268417, Learning Rate: 8.589934592000007e-05\n",
      "Epoch: 597, Training Loss: 0.013668774627149105, Validation Loss: 0.01670241355895996, Learning Rate: 8.589934592000007e-05\n",
      "Epoch: 598, Training Loss: 0.013653935864567757, Validation Loss: 0.016514066606760025, Learning Rate: 8.589934592000007e-05\n",
      "Epoch: 599, Training Loss: 0.013835872523486614, Validation Loss: 0.016599290072917938, Learning Rate: 8.589934592000007e-05\n",
      "Epoch: 600, Training Loss: 0.014053672552108765, Validation Loss: 0.01681511104106903, Learning Rate: 6.871947673600006e-05\n",
      "Epoch: 601, Training Loss: 0.013891896232962608, Validation Loss: 0.016663314774632454, Learning Rate: 6.871947673600006e-05\n",
      "Epoch: 602, Training Loss: 0.013933813199400902, Validation Loss: 0.016560811549425125, Learning Rate: 6.871947673600006e-05\n",
      "Epoch: 603, Training Loss: 0.013632482849061489, Validation Loss: 0.016369380056858063, Learning Rate: 6.871947673600006e-05\n",
      "Epoch: 604, Training Loss: 0.013630857691168785, Validation Loss: 0.016368631273508072, Learning Rate: 6.871947673600006e-05\n",
      "Epoch: 605, Training Loss: 0.013536551035940647, Validation Loss: 0.016460156068205833, Learning Rate: 6.871947673600006e-05\n",
      "Epoch: 606, Training Loss: 0.01351060252636671, Validation Loss: 0.016133658587932587, Learning Rate: 6.871947673600006e-05\n",
      "Epoch: 607, Training Loss: 0.013551131822168827, Validation Loss: 0.01654442958533764, Learning Rate: 6.871947673600006e-05\n",
      "Epoch: 608, Training Loss: 0.013609270565211773, Validation Loss: 0.01642576791346073, Learning Rate: 6.871947673600006e-05\n",
      "Epoch: 609, Training Loss: 0.01391612272709608, Validation Loss: 0.017055947333574295, Learning Rate: 6.871947673600006e-05\n",
      "Epoch: 610, Training Loss: 0.013594656251370907, Validation Loss: 0.016694214195013046, Learning Rate: 6.871947673600006e-05\n",
      "Epoch: 611, Training Loss: 0.013585835695266724, Validation Loss: 0.01656920276582241, Learning Rate: 6.871947673600006e-05\n",
      "Epoch: 612, Training Loss: 0.013641757890582085, Validation Loss: 0.016566326841711998, Learning Rate: 6.871947673600006e-05\n",
      "Epoch: 613, Training Loss: 0.01352402288466692, Validation Loss: 0.01629256084561348, Learning Rate: 6.871947673600006e-05\n",
      "Epoch: 614, Training Loss: 0.01356463972479105, Validation Loss: 0.01655680686235428, Learning Rate: 6.871947673600006e-05\n",
      "Epoch: 615, Training Loss: 0.013602213002741337, Validation Loss: 0.016642428934574127, Learning Rate: 6.871947673600006e-05\n",
      "Epoch: 616, Training Loss: 0.013668245635926723, Validation Loss: 0.016729068011045456, Learning Rate: 6.871947673600006e-05\n",
      "Epoch: 617, Training Loss: 0.013615568168461323, Validation Loss: 0.01675681583583355, Learning Rate: 6.871947673600006e-05\n",
      "Epoch: 618, Training Loss: 0.01366659440100193, Validation Loss: 0.016649501398205757, Learning Rate: 6.871947673600006e-05\n",
      "Epoch: 619, Training Loss: 0.01352081261575222, Validation Loss: 0.016465434804558754, Learning Rate: 6.871947673600006e-05\n",
      "Epoch: 620, Training Loss: 0.013756467029452324, Validation Loss: 0.0167031679302454, Learning Rate: 6.871947673600006e-05\n",
      "Epoch: 621, Training Loss: 0.013557614758610725, Validation Loss: 0.016299976035952568, Learning Rate: 6.871947673600006e-05\n",
      "Epoch: 622, Training Loss: 0.013472101651132107, Validation Loss: 0.016335327178239822, Learning Rate: 6.871947673600006e-05\n",
      "Epoch: 623, Training Loss: 0.013458633795380592, Validation Loss: 0.016568906605243683, Learning Rate: 6.871947673600006e-05\n",
      "Epoch: 624, Training Loss: 0.013556170277297497, Validation Loss: 0.0166365597397089, Learning Rate: 6.871947673600006e-05\n",
      "Epoch: 625, Training Loss: 0.013417292386293411, Validation Loss: 0.016391411423683167, Learning Rate: 6.871947673600006e-05\n",
      "Epoch: 626, Training Loss: 0.0134785957634449, Validation Loss: 0.016407182440161705, Learning Rate: 6.871947673600006e-05\n",
      "Epoch: 627, Training Loss: 0.01360831968486309, Validation Loss: 0.01642962358891964, Learning Rate: 6.871947673600006e-05\n",
      "Epoch: 628, Training Loss: 0.013661867938935757, Validation Loss: 0.016639254987239838, Learning Rate: 6.871947673600006e-05\n",
      "Epoch: 629, Training Loss: 0.01356609258800745, Validation Loss: 0.016545116901397705, Learning Rate: 6.871947673600006e-05\n",
      "Epoch: 630, Training Loss: 0.013616741634905338, Validation Loss: 0.016895979642868042, Learning Rate: 6.871947673600006e-05\n",
      "Epoch: 631, Training Loss: 0.013443364761769772, Validation Loss: 0.01644408330321312, Learning Rate: 6.871947673600006e-05\n",
      "Epoch: 632, Training Loss: 0.013405654579401016, Validation Loss: 0.016518965363502502, Learning Rate: 6.871947673600006e-05\n",
      "Epoch: 633, Training Loss: 0.013405834324657917, Validation Loss: 0.016444271430373192, Learning Rate: 6.871947673600006e-05\n",
      "Epoch: 634, Training Loss: 0.01341081503778696, Validation Loss: 0.01639493741095066, Learning Rate: 6.871947673600006e-05\n",
      "Epoch: 635, Training Loss: 0.013376700691878796, Validation Loss: 0.01647600531578064, Learning Rate: 6.871947673600006e-05\n",
      "Epoch: 636, Training Loss: 0.01338941603899002, Validation Loss: 0.016340404748916626, Learning Rate: 6.871947673600006e-05\n",
      "Epoch: 637, Training Loss: 0.013403580524027348, Validation Loss: 0.016772709786891937, Learning Rate: 6.871947673600006e-05\n",
      "Epoch: 638, Training Loss: 0.01351945661008358, Validation Loss: 0.016480885446071625, Learning Rate: 6.871947673600006e-05\n",
      "Epoch: 639, Training Loss: 0.013469824567437172, Validation Loss: 0.016377389430999756, Learning Rate: 6.871947673600006e-05\n",
      "Epoch: 640, Training Loss: 0.013318766839802265, Validation Loss: 0.016233112663030624, Learning Rate: 6.871947673600006e-05\n",
      "Epoch: 641, Training Loss: 0.013415569439530373, Validation Loss: 0.016566168516874313, Learning Rate: 6.871947673600006e-05\n",
      "Epoch: 642, Training Loss: 0.013328229077160358, Validation Loss: 0.016371726989746094, Learning Rate: 6.871947673600006e-05\n",
      "Epoch: 643, Training Loss: 0.013340242207050323, Validation Loss: 0.016319826245307922, Learning Rate: 6.871947673600006e-05\n",
      "Epoch: 644, Training Loss: 0.013474496081471443, Validation Loss: 0.016222424805164337, Learning Rate: 6.871947673600006e-05\n",
      "Epoch: 645, Training Loss: 0.013401365838944912, Validation Loss: 0.01635957881808281, Learning Rate: 6.871947673600006e-05\n",
      "Epoch: 646, Training Loss: 0.013306241482496262, Validation Loss: 0.016290752217173576, Learning Rate: 6.871947673600006e-05\n",
      "Epoch: 647, Training Loss: 0.013295738957822323, Validation Loss: 0.0164587814360857, Learning Rate: 6.871947673600006e-05\n",
      "Epoch: 648, Training Loss: 0.013285593129694462, Validation Loss: 0.016466664150357246, Learning Rate: 6.871947673600006e-05\n",
      "Epoch: 649, Training Loss: 0.013382651843130589, Validation Loss: 0.016486043110489845, Learning Rate: 6.871947673600006e-05\n",
      "Epoch: 650, Training Loss: 0.013194514438509941, Validation Loss: 0.0162508524954319, Learning Rate: 5.497558138880005e-05\n",
      "Epoch: 651, Training Loss: 0.01327246893197298, Validation Loss: 0.016325216740369797, Learning Rate: 5.497558138880005e-05\n",
      "Epoch: 652, Training Loss: 0.013407556340098381, Validation Loss: 0.016635164618492126, Learning Rate: 5.497558138880005e-05\n",
      "Epoch: 653, Training Loss: 0.01323307678103447, Validation Loss: 0.016387203708291054, Learning Rate: 5.497558138880005e-05\n",
      "Epoch: 654, Training Loss: 0.01319054327905178, Validation Loss: 0.016383996233344078, Learning Rate: 5.497558138880005e-05\n",
      "Epoch: 655, Training Loss: 0.01320917159318924, Validation Loss: 0.01643236167728901, Learning Rate: 5.497558138880005e-05\n",
      "Epoch: 656, Training Loss: 0.013191796839237213, Validation Loss: 0.016240689903497696, Learning Rate: 5.497558138880005e-05\n",
      "Epoch: 657, Training Loss: 0.013226098380982876, Validation Loss: 0.01647014729678631, Learning Rate: 5.497558138880005e-05\n",
      "Epoch: 658, Training Loss: 0.013238742016255856, Validation Loss: 0.016420898959040642, Learning Rate: 5.497558138880005e-05\n",
      "Epoch: 659, Training Loss: 0.013177161104977131, Validation Loss: 0.01634809374809265, Learning Rate: 5.497558138880005e-05\n",
      "Epoch: 660, Training Loss: 0.013185719959437847, Validation Loss: 0.01632879488170147, Learning Rate: 5.497558138880005e-05\n",
      "Epoch: 661, Training Loss: 0.01334760058671236, Validation Loss: 0.01645808108150959, Learning Rate: 5.497558138880005e-05\n",
      "Epoch: 662, Training Loss: 0.013296253979206085, Validation Loss: 0.0166916660964489, Learning Rate: 5.497558138880005e-05\n",
      "Epoch: 663, Training Loss: 0.013233904726803303, Validation Loss: 0.016408460214734077, Learning Rate: 5.497558138880005e-05\n",
      "Epoch: 664, Training Loss: 0.013160936534404755, Validation Loss: 0.01639772579073906, Learning Rate: 5.497558138880005e-05\n",
      "Epoch: 665, Training Loss: 0.013269221410155296, Validation Loss: 0.01627994328737259, Learning Rate: 5.497558138880005e-05\n",
      "Epoch: 666, Training Loss: 0.013130842708051205, Validation Loss: 0.016322871670126915, Learning Rate: 5.497558138880005e-05\n",
      "Epoch: 667, Training Loss: 0.013187345117330551, Validation Loss: 0.016328148543834686, Learning Rate: 5.497558138880005e-05\n",
      "Epoch: 668, Training Loss: 0.013144144788384438, Validation Loss: 0.01635790802538395, Learning Rate: 5.497558138880005e-05\n",
      "Epoch: 669, Training Loss: 0.01311738695949316, Validation Loss: 0.016194220632314682, Learning Rate: 5.497558138880005e-05\n",
      "Epoch: 670, Training Loss: 0.013113347813487053, Validation Loss: 0.01624545268714428, Learning Rate: 5.497558138880005e-05\n",
      "Epoch: 671, Training Loss: 0.01316870842128992, Validation Loss: 0.016020165756344795, Learning Rate: 5.497558138880005e-05\n",
      "Epoch: 672, Training Loss: 0.013305705040693283, Validation Loss: 0.01620446890592575, Learning Rate: 5.497558138880005e-05\n",
      "Epoch: 673, Training Loss: 0.013054951094090939, Validation Loss: 0.016384515911340714, Learning Rate: 5.497558138880005e-05\n",
      "Epoch: 674, Training Loss: 0.013184655457735062, Validation Loss: 0.016453847289085388, Learning Rate: 5.497558138880005e-05\n",
      "Epoch: 675, Training Loss: 0.013217762112617493, Validation Loss: 0.01659979857504368, Learning Rate: 5.497558138880005e-05\n",
      "Epoch: 676, Training Loss: 0.013107811100780964, Validation Loss: 0.016278143972158432, Learning Rate: 5.497558138880005e-05\n",
      "Epoch: 677, Training Loss: 0.013097324408590794, Validation Loss: 0.01621594838798046, Learning Rate: 5.497558138880005e-05\n",
      "Epoch: 678, Training Loss: 0.013052652589976788, Validation Loss: 0.01608402095735073, Learning Rate: 5.497558138880005e-05\n",
      "Epoch: 679, Training Loss: 0.013137501664459705, Validation Loss: 0.016555100679397583, Learning Rate: 5.497558138880005e-05\n",
      "Epoch: 680, Training Loss: 0.013086574152112007, Validation Loss: 0.016215631738305092, Learning Rate: 5.497558138880005e-05\n",
      "Epoch: 681, Training Loss: 0.013276093639433384, Validation Loss: 0.016473347321152687, Learning Rate: 5.497558138880005e-05\n",
      "Epoch: 682, Training Loss: 0.01300778891891241, Validation Loss: 0.016355106607079506, Learning Rate: 5.497558138880005e-05\n",
      "Epoch: 683, Training Loss: 0.013082924298942089, Validation Loss: 0.016418440267443657, Learning Rate: 5.497558138880005e-05\n",
      "Epoch: 684, Training Loss: 0.013088494539260864, Validation Loss: 0.01642303541302681, Learning Rate: 5.497558138880005e-05\n",
      "Epoch: 685, Training Loss: 0.013118883594870567, Validation Loss: 0.016842210665345192, Learning Rate: 5.497558138880005e-05\n",
      "Epoch: 686, Training Loss: 0.013011949136853218, Validation Loss: 0.016361206769943237, Learning Rate: 5.497558138880005e-05\n",
      "Epoch: 687, Training Loss: 0.013073185458779335, Validation Loss: 0.016458645462989807, Learning Rate: 5.497558138880005e-05\n",
      "Epoch: 688, Training Loss: 0.013047079555690289, Validation Loss: 0.016419075429439545, Learning Rate: 5.497558138880005e-05\n",
      "Epoch: 689, Training Loss: 0.01297882478684187, Validation Loss: 0.01650971733033657, Learning Rate: 5.497558138880005e-05\n",
      "Epoch: 690, Training Loss: 0.012994310818612576, Validation Loss: 0.016471631824970245, Learning Rate: 5.497558138880005e-05\n",
      "Epoch: 691, Training Loss: 0.013079656288027763, Validation Loss: 0.01633283868432045, Learning Rate: 5.497558138880005e-05\n",
      "Epoch: 692, Training Loss: 0.013151219114661217, Validation Loss: 0.016671976074576378, Learning Rate: 5.497558138880005e-05\n",
      "Epoch: 693, Training Loss: 0.012953557074069977, Validation Loss: 0.016235467046499252, Learning Rate: 5.497558138880005e-05\n",
      "Epoch: 694, Training Loss: 0.013376988470554352, Validation Loss: 0.016598548740148544, Learning Rate: 5.497558138880005e-05\n",
      "Epoch: 695, Training Loss: 0.013146191835403442, Validation Loss: 0.016400843858718872, Learning Rate: 5.497558138880005e-05\n",
      "Epoch: 696, Training Loss: 0.013018008321523666, Validation Loss: 0.015789085999131203, Learning Rate: 5.497558138880005e-05\n",
      "Epoch: 697, Training Loss: 0.013089640066027641, Validation Loss: 0.016165729612112045, Learning Rate: 5.497558138880005e-05\n",
      "Epoch: 698, Training Loss: 0.013060749508440495, Validation Loss: 0.016496459022164345, Learning Rate: 5.497558138880005e-05\n",
      "Epoch: 699, Training Loss: 0.013236152939498425, Validation Loss: 0.016608629375696182, Learning Rate: 5.497558138880005e-05\n",
      "Epoch: 700, Training Loss: 0.012940441258251667, Validation Loss: 0.016125356778502464, Learning Rate: 4.3980465111040044e-05\n",
      "Epoch: 701, Training Loss: 0.012909170240163803, Validation Loss: 0.016158929094672203, Learning Rate: 4.3980465111040044e-05\n",
      "Epoch: 702, Training Loss: 0.012924211099743843, Validation Loss: 0.016382936388254166, Learning Rate: 4.3980465111040044e-05\n",
      "Epoch: 703, Training Loss: 0.012970343232154846, Validation Loss: 0.016287975013256073, Learning Rate: 4.3980465111040044e-05\n",
      "Epoch: 704, Training Loss: 0.013006863184273243, Validation Loss: 0.016395313665270805, Learning Rate: 4.3980465111040044e-05\n",
      "Epoch: 705, Training Loss: 0.012929556891322136, Validation Loss: 0.016216950491070747, Learning Rate: 4.3980465111040044e-05\n",
      "Epoch: 706, Training Loss: 0.012919068336486816, Validation Loss: 0.016342993825674057, Learning Rate: 4.3980465111040044e-05\n",
      "Epoch: 707, Training Loss: 0.012947539798915386, Validation Loss: 0.016201015561819077, Learning Rate: 4.3980465111040044e-05\n",
      "Epoch: 708, Training Loss: 0.012874145060777664, Validation Loss: 0.016092868521809578, Learning Rate: 4.3980465111040044e-05\n",
      "Epoch: 709, Training Loss: 0.01287745125591755, Validation Loss: 0.016229411587119102, Learning Rate: 4.3980465111040044e-05\n",
      "Epoch: 710, Training Loss: 0.012913940474390984, Validation Loss: 0.016539430245757103, Learning Rate: 4.3980465111040044e-05\n",
      "Epoch: 711, Training Loss: 0.012871735729277134, Validation Loss: 0.016322443261742592, Learning Rate: 4.3980465111040044e-05\n",
      "Epoch: 712, Training Loss: 0.012929171323776245, Validation Loss: 0.01610153540968895, Learning Rate: 4.3980465111040044e-05\n",
      "Epoch: 713, Training Loss: 0.012852189131081104, Validation Loss: 0.016159163787961006, Learning Rate: 4.3980465111040044e-05\n",
      "Epoch: 714, Training Loss: 0.012911402620375156, Validation Loss: 0.016378501430153847, Learning Rate: 4.3980465111040044e-05\n",
      "Epoch: 715, Training Loss: 0.012864168733358383, Validation Loss: 0.016339011490345, Learning Rate: 4.3980465111040044e-05\n",
      "Epoch: 716, Training Loss: 0.012837378308176994, Validation Loss: 0.016305092722177505, Learning Rate: 4.3980465111040044e-05\n",
      "Epoch: 717, Training Loss: 0.013024951331317425, Validation Loss: 0.016502277925610542, Learning Rate: 4.3980465111040044e-05\n",
      "Epoch: 718, Training Loss: 0.012927054427564144, Validation Loss: 0.01608925871551037, Learning Rate: 4.3980465111040044e-05\n",
      "Epoch: 719, Training Loss: 0.012798280455172062, Validation Loss: 0.016271773725748062, Learning Rate: 4.3980465111040044e-05\n",
      "Epoch: 720, Training Loss: 0.012822801247239113, Validation Loss: 0.01602170430123806, Learning Rate: 4.3980465111040044e-05\n",
      "Epoch: 721, Training Loss: 0.012845263816416264, Validation Loss: 0.01626039482653141, Learning Rate: 4.3980465111040044e-05\n",
      "Epoch: 722, Training Loss: 0.012808389030396938, Validation Loss: 0.016351258382201195, Learning Rate: 4.3980465111040044e-05\n",
      "Epoch: 723, Training Loss: 0.012884508818387985, Validation Loss: 0.016343694180250168, Learning Rate: 4.3980465111040044e-05\n",
      "Epoch: 724, Training Loss: 0.01292981207370758, Validation Loss: 0.016255011782050133, Learning Rate: 4.3980465111040044e-05\n",
      "Epoch: 725, Training Loss: 0.012955410405993462, Validation Loss: 0.016410905867815018, Learning Rate: 4.3980465111040044e-05\n",
      "Epoch: 726, Training Loss: 0.012806099839508533, Validation Loss: 0.01631110906600952, Learning Rate: 4.3980465111040044e-05\n",
      "Epoch: 727, Training Loss: 0.012822043150663376, Validation Loss: 0.016234030947089195, Learning Rate: 4.3980465111040044e-05\n",
      "Epoch: 728, Training Loss: 0.012803684920072556, Validation Loss: 0.016213547438383102, Learning Rate: 4.3980465111040044e-05\n",
      "Epoch: 729, Training Loss: 0.012823746539652348, Validation Loss: 0.016120275482535362, Learning Rate: 4.3980465111040044e-05\n",
      "Epoch: 730, Training Loss: 0.012791593559086323, Validation Loss: 0.016220573335886, Learning Rate: 4.3980465111040044e-05\n",
      "Epoch: 731, Training Loss: 0.012894765473902225, Validation Loss: 0.016531897708773613, Learning Rate: 4.3980465111040044e-05\n",
      "Epoch: 732, Training Loss: 0.01279026735574007, Validation Loss: 0.016307534649968147, Learning Rate: 4.3980465111040044e-05\n",
      "Epoch: 733, Training Loss: 0.012739522382616997, Validation Loss: 0.015913065522909164, Learning Rate: 4.3980465111040044e-05\n",
      "Epoch: 734, Training Loss: 0.012853104621171951, Validation Loss: 0.015985680744051933, Learning Rate: 4.3980465111040044e-05\n",
      "Epoch: 735, Training Loss: 0.012769822962582111, Validation Loss: 0.01610170491039753, Learning Rate: 4.3980465111040044e-05\n",
      "Epoch: 736, Training Loss: 0.012799154967069626, Validation Loss: 0.016296932473778725, Learning Rate: 4.3980465111040044e-05\n",
      "Epoch: 737, Training Loss: 0.012999472208321095, Validation Loss: 0.01640287972986698, Learning Rate: 4.3980465111040044e-05\n",
      "Epoch: 738, Training Loss: 0.012803220190107822, Validation Loss: 0.016096901148557663, Learning Rate: 4.3980465111040044e-05\n",
      "Epoch: 739, Training Loss: 0.012869661673903465, Validation Loss: 0.01612813025712967, Learning Rate: 4.3980465111040044e-05\n",
      "Epoch: 740, Training Loss: 0.012750601395964622, Validation Loss: 0.016130106523633003, Learning Rate: 4.3980465111040044e-05\n",
      "Epoch: 741, Training Loss: 0.012791731394827366, Validation Loss: 0.016306918114423752, Learning Rate: 4.3980465111040044e-05\n",
      "Epoch: 742, Training Loss: 0.012747540138661861, Validation Loss: 0.016094284132122993, Learning Rate: 4.3980465111040044e-05\n",
      "Epoch: 743, Training Loss: 0.012730955146253109, Validation Loss: 0.01593630760908127, Learning Rate: 4.3980465111040044e-05\n",
      "Epoch: 744, Training Loss: 0.012801306322216988, Validation Loss: 0.016143497079610825, Learning Rate: 4.3980465111040044e-05\n",
      "Epoch: 745, Training Loss: 0.01266802940517664, Validation Loss: 0.01607513055205345, Learning Rate: 4.3980465111040044e-05\n",
      "Epoch: 746, Training Loss: 0.012721255421638489, Validation Loss: 0.016085172072052956, Learning Rate: 4.3980465111040044e-05\n",
      "Epoch: 747, Training Loss: 0.012727244757115841, Validation Loss: 0.01609514281153679, Learning Rate: 4.3980465111040044e-05\n",
      "Epoch: 748, Training Loss: 0.012741566635668278, Validation Loss: 0.016371773555874825, Learning Rate: 4.3980465111040044e-05\n",
      "Epoch: 749, Training Loss: 0.012765897437930107, Validation Loss: 0.016308147460222244, Learning Rate: 4.3980465111040044e-05\n",
      "Epoch: 750, Training Loss: 0.012734873220324516, Validation Loss: 0.015970265492796898, Learning Rate: 3.5184372088832036e-05\n",
      "Epoch: 751, Training Loss: 0.012635382823646069, Validation Loss: 0.015954751521348953, Learning Rate: 3.5184372088832036e-05\n",
      "Epoch: 752, Training Loss: 0.012671031057834625, Validation Loss: 0.016126219183206558, Learning Rate: 3.5184372088832036e-05\n",
      "Epoch: 753, Training Loss: 0.012671035714447498, Validation Loss: 0.01620996929705143, Learning Rate: 3.5184372088832036e-05\n",
      "Epoch: 754, Training Loss: 0.012656494975090027, Validation Loss: 0.01593538373708725, Learning Rate: 3.5184372088832036e-05\n",
      "Epoch: 755, Training Loss: 0.012681256048381329, Validation Loss: 0.01607968844473362, Learning Rate: 3.5184372088832036e-05\n",
      "Epoch: 756, Training Loss: 0.012635312974452972, Validation Loss: 0.016152407974004745, Learning Rate: 3.5184372088832036e-05\n",
      "Epoch: 757, Training Loss: 0.01265106350183487, Validation Loss: 0.016030583530664444, Learning Rate: 3.5184372088832036e-05\n",
      "Epoch: 758, Training Loss: 0.012688410468399525, Validation Loss: 0.016172152012586594, Learning Rate: 3.5184372088832036e-05\n",
      "Epoch: 759, Training Loss: 0.01265945564955473, Validation Loss: 0.01627313159406185, Learning Rate: 3.5184372088832036e-05\n",
      "Epoch: 760, Training Loss: 0.012700437568128109, Validation Loss: 0.01618153601884842, Learning Rate: 3.5184372088832036e-05\n",
      "Epoch: 761, Training Loss: 0.012679407373070717, Validation Loss: 0.016248686239123344, Learning Rate: 3.5184372088832036e-05\n",
      "Epoch: 762, Training Loss: 0.012707916088402271, Validation Loss: 0.016174623742699623, Learning Rate: 3.5184372088832036e-05\n",
      "Epoch: 763, Training Loss: 0.012616820633411407, Validation Loss: 0.016172310337424278, Learning Rate: 3.5184372088832036e-05\n",
      "Epoch: 764, Training Loss: 0.01275168638676405, Validation Loss: 0.016118522733449936, Learning Rate: 3.5184372088832036e-05\n",
      "Epoch: 765, Training Loss: 0.012703629210591316, Validation Loss: 0.016278469935059547, Learning Rate: 3.5184372088832036e-05\n",
      "Epoch: 766, Training Loss: 0.012597463093698025, Validation Loss: 0.01628117449581623, Learning Rate: 3.5184372088832036e-05\n",
      "Epoch: 767, Training Loss: 0.012651427648961544, Validation Loss: 0.016049129888415337, Learning Rate: 3.5184372088832036e-05\n",
      "Epoch: 768, Training Loss: 0.012623477727174759, Validation Loss: 0.01618477888405323, Learning Rate: 3.5184372088832036e-05\n",
      "Epoch: 769, Training Loss: 0.012626858428120613, Validation Loss: 0.016135429963469505, Learning Rate: 3.5184372088832036e-05\n",
      "Epoch: 770, Training Loss: 0.012608645483851433, Validation Loss: 0.016126153990626335, Learning Rate: 3.5184372088832036e-05\n",
      "Epoch: 771, Training Loss: 0.012586783617734909, Validation Loss: 0.016162406653165817, Learning Rate: 3.5184372088832036e-05\n",
      "Epoch: 772, Training Loss: 0.012609451077878475, Validation Loss: 0.016193483024835587, Learning Rate: 3.5184372088832036e-05\n",
      "Epoch: 773, Training Loss: 0.012587104924023151, Validation Loss: 0.016138169914484024, Learning Rate: 3.5184372088832036e-05\n",
      "Epoch: 774, Training Loss: 0.012549901381134987, Validation Loss: 0.01623678021132946, Learning Rate: 3.5184372088832036e-05\n",
      "Epoch: 775, Training Loss: 0.012563240714371204, Validation Loss: 0.01617087982594967, Learning Rate: 3.5184372088832036e-05\n",
      "Epoch: 776, Training Loss: 0.012575382366776466, Validation Loss: 0.016078921034932137, Learning Rate: 3.5184372088832036e-05\n",
      "Epoch: 777, Training Loss: 0.012611006386578083, Validation Loss: 0.016099322587251663, Learning Rate: 3.5184372088832036e-05\n",
      "Epoch: 778, Training Loss: 0.012573459185659885, Validation Loss: 0.015985636040568352, Learning Rate: 3.5184372088832036e-05\n",
      "Epoch: 779, Training Loss: 0.012573990039527416, Validation Loss: 0.01604437828063965, Learning Rate: 3.5184372088832036e-05\n",
      "Epoch: 780, Training Loss: 0.01251227967441082, Validation Loss: 0.016097264364361763, Learning Rate: 3.5184372088832036e-05\n",
      "Epoch: 781, Training Loss: 0.012564505450427532, Validation Loss: 0.016096601262688637, Learning Rate: 3.5184372088832036e-05\n",
      "Epoch: 782, Training Loss: 0.012523651123046875, Validation Loss: 0.016086004674434662, Learning Rate: 3.5184372088832036e-05\n",
      "Epoch: 783, Training Loss: 0.01252606138586998, Validation Loss: 0.016098622232675552, Learning Rate: 3.5184372088832036e-05\n",
      "Epoch: 784, Training Loss: 0.01253619696944952, Validation Loss: 0.016158606857061386, Learning Rate: 3.5184372088832036e-05\n",
      "Epoch: 785, Training Loss: 0.012697277590632439, Validation Loss: 0.016246696934103966, Learning Rate: 3.5184372088832036e-05\n",
      "Epoch: 786, Training Loss: 0.01259989757090807, Validation Loss: 0.016007650643587112, Learning Rate: 3.5184372088832036e-05\n",
      "Epoch: 787, Training Loss: 0.012531819753348827, Validation Loss: 0.016134191304445267, Learning Rate: 3.5184372088832036e-05\n",
      "Epoch: 788, Training Loss: 0.012551740743219852, Validation Loss: 0.015969756990671158, Learning Rate: 3.5184372088832036e-05\n",
      "Epoch: 789, Training Loss: 0.012632861733436584, Validation Loss: 0.016156794503331184, Learning Rate: 3.5184372088832036e-05\n",
      "Epoch: 790, Training Loss: 0.012595542706549168, Validation Loss: 0.01605580374598503, Learning Rate: 3.5184372088832036e-05\n",
      "Epoch: 791, Training Loss: 0.012550612911581993, Validation Loss: 0.016144581139087677, Learning Rate: 3.5184372088832036e-05\n",
      "Epoch: 792, Training Loss: 0.012533013708889484, Validation Loss: 0.016210326924920082, Learning Rate: 3.5184372088832036e-05\n",
      "Epoch: 793, Training Loss: 0.012530987150967121, Validation Loss: 0.016237441450357437, Learning Rate: 3.5184372088832036e-05\n",
      "Epoch: 794, Training Loss: 0.01254600565880537, Validation Loss: 0.016267353668808937, Learning Rate: 3.5184372088832036e-05\n",
      "Epoch: 795, Training Loss: 0.012545941397547722, Validation Loss: 0.016306161880493164, Learning Rate: 3.5184372088832036e-05\n",
      "Epoch: 796, Training Loss: 0.012606569565832615, Validation Loss: 0.016279954463243484, Learning Rate: 3.5184372088832036e-05\n",
      "Epoch: 797, Training Loss: 0.012540155090391636, Validation Loss: 0.016057603061199188, Learning Rate: 3.5184372088832036e-05\n",
      "Epoch: 798, Training Loss: 0.012637918815016747, Validation Loss: 0.016321120783686638, Learning Rate: 3.5184372088832036e-05\n",
      "Epoch: 799, Training Loss: 0.012527749873697758, Validation Loss: 0.016354721039533615, Learning Rate: 3.5184372088832036e-05\n",
      "Epoch: 800, Training Loss: 0.012507805600762367, Validation Loss: 0.016126371920108795, Learning Rate: 2.814749767106563e-05\n",
      "Epoch: 801, Training Loss: 0.012472724542021751, Validation Loss: 0.01615261659026146, Learning Rate: 2.814749767106563e-05\n",
      "Epoch: 802, Training Loss: 0.012474062852561474, Validation Loss: 0.01590820960700512, Learning Rate: 2.814749767106563e-05\n",
      "Epoch: 803, Training Loss: 0.012455666437745094, Validation Loss: 0.016109883785247803, Learning Rate: 2.814749767106563e-05\n",
      "Epoch: 804, Training Loss: 0.012456417083740234, Validation Loss: 0.015981772914528847, Learning Rate: 2.814749767106563e-05\n",
      "Epoch: 805, Training Loss: 0.012464906089007854, Validation Loss: 0.016105247661471367, Learning Rate: 2.814749767106563e-05\n",
      "Epoch: 806, Training Loss: 0.012642854824662209, Validation Loss: 0.016315290704369545, Learning Rate: 2.814749767106563e-05\n",
      "Epoch: 807, Training Loss: 0.012446172535419464, Validation Loss: 0.016031987965106964, Learning Rate: 2.814749767106563e-05\n",
      "Epoch: 808, Training Loss: 0.01245024986565113, Validation Loss: 0.016053805127739906, Learning Rate: 2.814749767106563e-05\n",
      "Epoch: 809, Training Loss: 0.012427871115505695, Validation Loss: 0.01610798016190529, Learning Rate: 2.814749767106563e-05\n",
      "Epoch: 810, Training Loss: 0.012422865256667137, Validation Loss: 0.01623259112238884, Learning Rate: 2.814749767106563e-05\n",
      "Epoch: 811, Training Loss: 0.012403642758727074, Validation Loss: 0.016122955828905106, Learning Rate: 2.814749767106563e-05\n",
      "Epoch: 812, Training Loss: 0.012434824369847775, Validation Loss: 0.016141433268785477, Learning Rate: 2.814749767106563e-05\n",
      "Epoch: 813, Training Loss: 0.012411323375999928, Validation Loss: 0.016109207645058632, Learning Rate: 2.814749767106563e-05\n",
      "Epoch: 814, Training Loss: 0.01241061557084322, Validation Loss: 0.01607799530029297, Learning Rate: 2.814749767106563e-05\n",
      "Epoch: 815, Training Loss: 0.012445826083421707, Validation Loss: 0.016047809273004532, Learning Rate: 2.814749767106563e-05\n",
      "Epoch: 816, Training Loss: 0.012413598597049713, Validation Loss: 0.016073156148195267, Learning Rate: 2.814749767106563e-05\n",
      "Epoch: 817, Training Loss: 0.012416278012096882, Validation Loss: 0.016075998544692993, Learning Rate: 2.814749767106563e-05\n",
      "Epoch: 818, Training Loss: 0.01242945808917284, Validation Loss: 0.01611945405602455, Learning Rate: 2.814749767106563e-05\n",
      "Epoch: 819, Training Loss: 0.012439826503396034, Validation Loss: 0.016111597418785095, Learning Rate: 2.814749767106563e-05\n",
      "Epoch: 820, Training Loss: 0.012449524365365505, Validation Loss: 0.016057858243584633, Learning Rate: 2.814749767106563e-05\n",
      "Epoch: 821, Training Loss: 0.012522432953119278, Validation Loss: 0.01606481894850731, Learning Rate: 2.814749767106563e-05\n",
      "Epoch: 822, Training Loss: 0.01238704752177, Validation Loss: 0.016085250303149223, Learning Rate: 2.814749767106563e-05\n",
      "Epoch: 823, Training Loss: 0.012389320880174637, Validation Loss: 0.016079463064670563, Learning Rate: 2.814749767106563e-05\n",
      "Epoch: 824, Training Loss: 0.012465695850551128, Validation Loss: 0.015768583863973618, Learning Rate: 2.814749767106563e-05\n",
      "Epoch: 825, Training Loss: 0.012378623709082603, Validation Loss: 0.01571245864033699, Learning Rate: 2.814749767106563e-05\n",
      "Epoch: 826, Training Loss: 0.0123754246160388, Validation Loss: 0.015881557017564774, Learning Rate: 2.814749767106563e-05\n",
      "Epoch: 827, Training Loss: 0.012384875677525997, Validation Loss: 0.01602957397699356, Learning Rate: 2.814749767106563e-05\n",
      "Epoch: 828, Training Loss: 0.012416611425578594, Validation Loss: 0.01619805209338665, Learning Rate: 2.814749767106563e-05\n",
      "Epoch: 829, Training Loss: 0.012377000413835049, Validation Loss: 0.015994790941476822, Learning Rate: 2.814749767106563e-05\n",
      "Epoch: 830, Training Loss: 0.012366948649287224, Validation Loss: 0.015979696065187454, Learning Rate: 2.814749767106563e-05\n",
      "Epoch: 831, Training Loss: 0.012372484430670738, Validation Loss: 0.016134243458509445, Learning Rate: 2.814749767106563e-05\n",
      "Epoch: 832, Training Loss: 0.012362729758024216, Validation Loss: 0.016037574037909508, Learning Rate: 2.814749767106563e-05\n",
      "Epoch: 833, Training Loss: 0.012469613924622536, Validation Loss: 0.01622936688363552, Learning Rate: 2.814749767106563e-05\n",
      "Epoch: 834, Training Loss: 0.01239644642919302, Validation Loss: 0.016013043001294136, Learning Rate: 2.814749767106563e-05\n",
      "Epoch: 835, Training Loss: 0.012375527992844582, Validation Loss: 0.016031300649046898, Learning Rate: 2.814749767106563e-05\n",
      "Epoch: 836, Training Loss: 0.01238305028527975, Validation Loss: 0.016010871157050133, Learning Rate: 2.814749767106563e-05\n",
      "Epoch: 837, Training Loss: 0.012364828959107399, Validation Loss: 0.016111211851239204, Learning Rate: 2.814749767106563e-05\n",
      "Epoch: 838, Training Loss: 0.012340175919234753, Validation Loss: 0.016064178198575974, Learning Rate: 2.814749767106563e-05\n",
      "Epoch: 839, Training Loss: 0.012391343712806702, Validation Loss: 0.016186850145459175, Learning Rate: 2.814749767106563e-05\n",
      "Epoch: 840, Training Loss: 0.01241845078766346, Validation Loss: 0.016328908503055573, Learning Rate: 2.814749767106563e-05\n",
      "Epoch: 841, Training Loss: 0.012331466190516949, Validation Loss: 0.01593066193163395, Learning Rate: 2.814749767106563e-05\n",
      "Epoch: 842, Training Loss: 0.012360310181975365, Validation Loss: 0.0161444079130888, Learning Rate: 2.814749767106563e-05\n",
      "Epoch: 843, Training Loss: 0.012347765266895294, Validation Loss: 0.016071125864982605, Learning Rate: 2.814749767106563e-05\n",
      "Epoch: 844, Training Loss: 0.012360888533294201, Validation Loss: 0.01620042324066162, Learning Rate: 2.814749767106563e-05\n",
      "Epoch: 845, Training Loss: 0.012384718284010887, Validation Loss: 0.015945658087730408, Learning Rate: 2.814749767106563e-05\n",
      "Epoch: 846, Training Loss: 0.01235062163323164, Validation Loss: 0.016141176223754883, Learning Rate: 2.814749767106563e-05\n",
      "Epoch: 847, Training Loss: 0.012346062809228897, Validation Loss: 0.016263345256447792, Learning Rate: 2.814749767106563e-05\n",
      "Epoch: 848, Training Loss: 0.012354112230241299, Validation Loss: 0.016049884259700775, Learning Rate: 2.814749767106563e-05\n",
      "Epoch: 849, Training Loss: 0.012347596697509289, Validation Loss: 0.015935182571411133, Learning Rate: 2.814749767106563e-05\n",
      "Epoch: 850, Training Loss: 0.012287244200706482, Validation Loss: 0.01605561003088951, Learning Rate: 2.2517998136852506e-05\n",
      "Epoch: 851, Training Loss: 0.012355940416455269, Validation Loss: 0.01612594909965992, Learning Rate: 2.2517998136852506e-05\n",
      "Epoch: 852, Training Loss: 0.01230023242533207, Validation Loss: 0.016002928838133812, Learning Rate: 2.2517998136852506e-05\n",
      "Epoch: 853, Training Loss: 0.012307270430028439, Validation Loss: 0.01598297245800495, Learning Rate: 2.2517998136852506e-05\n",
      "Epoch: 854, Training Loss: 0.01230867113918066, Validation Loss: 0.015929020941257477, Learning Rate: 2.2517998136852506e-05\n",
      "Epoch: 855, Training Loss: 0.012300563044846058, Validation Loss: 0.01598513312637806, Learning Rate: 2.2517998136852506e-05\n",
      "Epoch: 856, Training Loss: 0.012277340516448021, Validation Loss: 0.01595645397901535, Learning Rate: 2.2517998136852506e-05\n",
      "Epoch: 857, Training Loss: 0.012312766164541245, Validation Loss: 0.016023922711610794, Learning Rate: 2.2517998136852506e-05\n",
      "Epoch: 858, Training Loss: 0.012267383746802807, Validation Loss: 0.015945523977279663, Learning Rate: 2.2517998136852506e-05\n",
      "Epoch: 859, Training Loss: 0.012293399311602116, Validation Loss: 0.015980105847120285, Learning Rate: 2.2517998136852506e-05\n",
      "Epoch: 860, Training Loss: 0.012270343489944935, Validation Loss: 0.0160585418343544, Learning Rate: 2.2517998136852506e-05\n",
      "Epoch: 861, Training Loss: 0.01229172758758068, Validation Loss: 0.01596289686858654, Learning Rate: 2.2517998136852506e-05\n",
      "Epoch: 862, Training Loss: 0.012356136925518513, Validation Loss: 0.016019875183701515, Learning Rate: 2.2517998136852506e-05\n",
      "Epoch: 863, Training Loss: 0.012272263877093792, Validation Loss: 0.01593773253262043, Learning Rate: 2.2517998136852506e-05\n",
      "Epoch: 864, Training Loss: 0.012250992469489574, Validation Loss: 0.01585843227803707, Learning Rate: 2.2517998136852506e-05\n",
      "Epoch: 865, Training Loss: 0.012277781032025814, Validation Loss: 0.01587461121380329, Learning Rate: 2.2517998136852506e-05\n",
      "Epoch: 866, Training Loss: 0.01226134318858385, Validation Loss: 0.01581701450049877, Learning Rate: 2.2517998136852506e-05\n",
      "Epoch: 867, Training Loss: 0.012272034771740437, Validation Loss: 0.016051102429628372, Learning Rate: 2.2517998136852506e-05\n",
      "Epoch: 868, Training Loss: 0.012261643074452877, Validation Loss: 0.01588663086295128, Learning Rate: 2.2517998136852506e-05\n",
      "Epoch: 869, Training Loss: 0.012294449843466282, Validation Loss: 0.015895307064056396, Learning Rate: 2.2517998136852506e-05\n",
      "Epoch: 870, Training Loss: 0.012275133281946182, Validation Loss: 0.01604367420077324, Learning Rate: 2.2517998136852506e-05\n",
      "Epoch: 871, Training Loss: 0.012307081371545792, Validation Loss: 0.016183504834771156, Learning Rate: 2.2517998136852506e-05\n",
      "Epoch: 872, Training Loss: 0.012288318015635014, Validation Loss: 0.016039352864027023, Learning Rate: 2.2517998136852506e-05\n",
      "Epoch: 873, Training Loss: 0.012259691022336483, Validation Loss: 0.01603742502629757, Learning Rate: 2.2517998136852506e-05\n",
      "Epoch: 874, Training Loss: 0.012301777489483356, Validation Loss: 0.016477562487125397, Learning Rate: 2.2517998136852506e-05\n",
      "Epoch: 875, Training Loss: 0.012241635471582413, Validation Loss: 0.016024021431803703, Learning Rate: 2.2517998136852506e-05\n",
      "Epoch: 876, Training Loss: 0.012296793051064014, Validation Loss: 0.01605294644832611, Learning Rate: 2.2517998136852506e-05\n",
      "Epoch: 877, Training Loss: 0.012273037806153297, Validation Loss: 0.01608123444020748, Learning Rate: 2.2517998136852506e-05\n",
      "Epoch: 878, Training Loss: 0.012295643799006939, Validation Loss: 0.01624276489019394, Learning Rate: 2.2517998136852506e-05\n",
      "Epoch: 879, Training Loss: 0.012243262492120266, Validation Loss: 0.016147319227457047, Learning Rate: 2.2517998136852506e-05\n",
      "Epoch: 880, Training Loss: 0.012302168644964695, Validation Loss: 0.01632051356136799, Learning Rate: 2.2517998136852506e-05\n",
      "Epoch: 881, Training Loss: 0.012264959514141083, Validation Loss: 0.016033610329031944, Learning Rate: 2.2517998136852506e-05\n",
      "Epoch: 882, Training Loss: 0.012370760552585125, Validation Loss: 0.01603485643863678, Learning Rate: 2.2517998136852506e-05\n",
      "Epoch: 883, Training Loss: 0.012286915443837643, Validation Loss: 0.016059402376413345, Learning Rate: 2.2517998136852506e-05\n",
      "Epoch: 884, Training Loss: 0.01226008590310812, Validation Loss: 0.01600465551018715, Learning Rate: 2.2517998136852506e-05\n",
      "Epoch: 885, Training Loss: 0.012237370945513248, Validation Loss: 0.015975361689925194, Learning Rate: 2.2517998136852506e-05\n",
      "Epoch: 886, Training Loss: 0.01225233729928732, Validation Loss: 0.016059696674346924, Learning Rate: 2.2517998136852506e-05\n",
      "Epoch: 887, Training Loss: 0.012217367067933083, Validation Loss: 0.01587596721947193, Learning Rate: 2.2517998136852506e-05\n",
      "Epoch: 888, Training Loss: 0.012225015088915825, Validation Loss: 0.01586526446044445, Learning Rate: 2.2517998136852506e-05\n",
      "Epoch: 889, Training Loss: 0.012232113629579544, Validation Loss: 0.01597159169614315, Learning Rate: 2.2517998136852506e-05\n",
      "Epoch: 890, Training Loss: 0.012226931750774384, Validation Loss: 0.01597204990684986, Learning Rate: 2.2517998136852506e-05\n",
      "Epoch: 891, Training Loss: 0.012216862291097641, Validation Loss: 0.015904942527413368, Learning Rate: 2.2517998136852506e-05\n",
      "Epoch: 892, Training Loss: 0.012227551080286503, Validation Loss: 0.015877623111009598, Learning Rate: 2.2517998136852506e-05\n",
      "Epoch: 893, Training Loss: 0.012200131081044674, Validation Loss: 0.01597578264772892, Learning Rate: 2.2517998136852506e-05\n",
      "Epoch: 894, Training Loss: 0.012230471707880497, Validation Loss: 0.015936965122818947, Learning Rate: 2.2517998136852506e-05\n",
      "Epoch: 895, Training Loss: 0.01223781332373619, Validation Loss: 0.01631631888449192, Learning Rate: 2.2517998136852506e-05\n",
      "Epoch: 896, Training Loss: 0.01222344022244215, Validation Loss: 0.016180094331502914, Learning Rate: 2.2517998136852506e-05\n",
      "Epoch: 897, Training Loss: 0.012280277907848358, Validation Loss: 0.016225259751081467, Learning Rate: 2.2517998136852506e-05\n",
      "Epoch: 898, Training Loss: 0.0122129051014781, Validation Loss: 0.016060784459114075, Learning Rate: 2.2517998136852506e-05\n",
      "Epoch: 899, Training Loss: 0.012194852344691753, Validation Loss: 0.01609148643910885, Learning Rate: 2.2517998136852506e-05\n",
      "Epoch: 900, Training Loss: 0.012184089049696922, Validation Loss: 0.016056763008236885, Learning Rate: 1.8014398509482006e-05\n",
      "Epoch: 901, Training Loss: 0.012190653942525387, Validation Loss: 0.016129177063703537, Learning Rate: 1.8014398509482006e-05\n",
      "Epoch: 902, Training Loss: 0.012188948690891266, Validation Loss: 0.016055725514888763, Learning Rate: 1.8014398509482006e-05\n",
      "Epoch: 903, Training Loss: 0.012242570519447327, Validation Loss: 0.01614297367632389, Learning Rate: 1.8014398509482006e-05\n",
      "Epoch: 904, Training Loss: 0.012178987264633179, Validation Loss: 0.01607234589755535, Learning Rate: 1.8014398509482006e-05\n",
      "Epoch: 905, Training Loss: 0.012178405188024044, Validation Loss: 0.015974028035998344, Learning Rate: 1.8014398509482006e-05\n",
      "Epoch: 906, Training Loss: 0.012211985886096954, Validation Loss: 0.016062913462519646, Learning Rate: 1.8014398509482006e-05\n",
      "Epoch: 907, Training Loss: 0.012191416695713997, Validation Loss: 0.016106152907013893, Learning Rate: 1.8014398509482006e-05\n",
      "Epoch: 908, Training Loss: 0.01223475206643343, Validation Loss: 0.015995927155017853, Learning Rate: 1.8014398509482006e-05\n",
      "Epoch: 909, Training Loss: 0.01217519398778677, Validation Loss: 0.01589101366698742, Learning Rate: 1.8014398509482006e-05\n",
      "Epoch: 910, Training Loss: 0.012163297273218632, Validation Loss: 0.015862569212913513, Learning Rate: 1.8014398509482006e-05\n",
      "Epoch: 911, Training Loss: 0.012168110348284245, Validation Loss: 0.016008052974939346, Learning Rate: 1.8014398509482006e-05\n",
      "Epoch: 912, Training Loss: 0.012182441540062428, Validation Loss: 0.016082607209682465, Learning Rate: 1.8014398509482006e-05\n",
      "Epoch: 913, Training Loss: 0.012164216488599777, Validation Loss: 0.01597726158797741, Learning Rate: 1.8014398509482006e-05\n",
      "Epoch: 914, Training Loss: 0.012175408191978931, Validation Loss: 0.015900123864412308, Learning Rate: 1.8014398509482006e-05\n",
      "Epoch: 915, Training Loss: 0.012192568741738796, Validation Loss: 0.01598861813545227, Learning Rate: 1.8014398509482006e-05\n",
      "Epoch: 916, Training Loss: 0.012143053114414215, Validation Loss: 0.015870485454797745, Learning Rate: 1.8014398509482006e-05\n",
      "Epoch: 917, Training Loss: 0.012158380821347237, Validation Loss: 0.015976697206497192, Learning Rate: 1.8014398509482006e-05\n",
      "Epoch: 918, Training Loss: 0.012146230787038803, Validation Loss: 0.01582670584321022, Learning Rate: 1.8014398509482006e-05\n",
      "Epoch: 919, Training Loss: 0.012160220183432102, Validation Loss: 0.015866169705986977, Learning Rate: 1.8014398509482006e-05\n",
      "Epoch: 920, Training Loss: 0.012200289405882359, Validation Loss: 0.016006991267204285, Learning Rate: 1.8014398509482006e-05\n",
      "Epoch: 921, Training Loss: 0.01215298380702734, Validation Loss: 0.015950357541441917, Learning Rate: 1.8014398509482006e-05\n",
      "Epoch: 922, Training Loss: 0.01214542705565691, Validation Loss: 0.01594107784330845, Learning Rate: 1.8014398509482006e-05\n",
      "Epoch: 923, Training Loss: 0.012144388630986214, Validation Loss: 0.01593315787613392, Learning Rate: 1.8014398509482006e-05\n",
      "Epoch: 924, Training Loss: 0.012166816741228104, Validation Loss: 0.0160320233553648, Learning Rate: 1.8014398509482006e-05\n",
      "Epoch: 925, Training Loss: 0.012162824161350727, Validation Loss: 0.015827493742108345, Learning Rate: 1.8014398509482006e-05\n",
      "Epoch: 926, Training Loss: 0.012141523882746696, Validation Loss: 0.0158099215477705, Learning Rate: 1.8014398509482006e-05\n",
      "Epoch: 927, Training Loss: 0.012153455056250095, Validation Loss: 0.015920447185635567, Learning Rate: 1.8014398509482006e-05\n",
      "Epoch: 928, Training Loss: 0.012148014269769192, Validation Loss: 0.015824979171156883, Learning Rate: 1.8014398509482006e-05\n",
      "Epoch: 929, Training Loss: 0.012145046144723892, Validation Loss: 0.015949754044413567, Learning Rate: 1.8014398509482006e-05\n",
      "Epoch: 930, Training Loss: 0.012141786515712738, Validation Loss: 0.015792598947882652, Learning Rate: 1.8014398509482006e-05\n",
      "Epoch: 931, Training Loss: 0.012132824398577213, Validation Loss: 0.016083834692835808, Learning Rate: 1.8014398509482006e-05\n",
      "Epoch: 932, Training Loss: 0.012136592529714108, Validation Loss: 0.015794984996318817, Learning Rate: 1.8014398509482006e-05\n",
      "Epoch: 933, Training Loss: 0.012177038006484509, Validation Loss: 0.015920011326670647, Learning Rate: 1.8014398509482006e-05\n",
      "Epoch: 934, Training Loss: 0.012146811932325363, Validation Loss: 0.015886053442955017, Learning Rate: 1.8014398509482006e-05\n",
      "Epoch: 935, Training Loss: 0.012123222462832928, Validation Loss: 0.015890570357441902, Learning Rate: 1.8014398509482006e-05\n",
      "Epoch: 936, Training Loss: 0.012128258123993874, Validation Loss: 0.01580064184963703, Learning Rate: 1.8014398509482006e-05\n",
      "Epoch: 937, Training Loss: 0.012122014537453651, Validation Loss: 0.01591944321990013, Learning Rate: 1.8014398509482006e-05\n",
      "Epoch: 938, Training Loss: 0.012115257792174816, Validation Loss: 0.015720747411251068, Learning Rate: 1.8014398509482006e-05\n",
      "Epoch: 939, Training Loss: 0.01211505476385355, Validation Loss: 0.015725407749414444, Learning Rate: 1.8014398509482006e-05\n",
      "Epoch: 940, Training Loss: 0.012115050107240677, Validation Loss: 0.01579270325601101, Learning Rate: 1.8014398509482006e-05\n",
      "Epoch: 941, Training Loss: 0.012165498919785023, Validation Loss: 0.0159195214509964, Learning Rate: 1.8014398509482006e-05\n",
      "Epoch: 942, Training Loss: 0.012126575224101543, Validation Loss: 0.01581680215895176, Learning Rate: 1.8014398509482006e-05\n",
      "Epoch: 943, Training Loss: 0.01209904532879591, Validation Loss: 0.015805428847670555, Learning Rate: 1.8014398509482006e-05\n",
      "Epoch: 944, Training Loss: 0.012106681242585182, Validation Loss: 0.015897363424301147, Learning Rate: 1.8014398509482006e-05\n",
      "Epoch: 945, Training Loss: 0.012100853957235813, Validation Loss: 0.01584307663142681, Learning Rate: 1.8014398509482006e-05\n",
      "Epoch: 946, Training Loss: 0.012159550562500954, Validation Loss: 0.015882493928074837, Learning Rate: 1.8014398509482006e-05\n",
      "Epoch: 947, Training Loss: 0.012109308503568172, Validation Loss: 0.015872759744524956, Learning Rate: 1.8014398509482006e-05\n",
      "Epoch: 948, Training Loss: 0.012095656245946884, Validation Loss: 0.01579272747039795, Learning Rate: 1.8014398509482006e-05\n",
      "Epoch: 949, Training Loss: 0.012175826355814934, Validation Loss: 0.01592116989195347, Learning Rate: 1.8014398509482006e-05\n",
      "Epoch: 950, Training Loss: 0.012088757008314133, Validation Loss: 0.015739191323518753, Learning Rate: 1.4411518807585605e-05\n",
      "Epoch: 951, Training Loss: 0.012095135636627674, Validation Loss: 0.015864508226513863, Learning Rate: 1.4411518807585605e-05\n",
      "Epoch: 952, Training Loss: 0.012092169374227524, Validation Loss: 0.01585371606051922, Learning Rate: 1.4411518807585605e-05\n",
      "Epoch: 953, Training Loss: 0.012078983709216118, Validation Loss: 0.01581563986837864, Learning Rate: 1.4411518807585605e-05\n",
      "Epoch: 954, Training Loss: 0.012073538266122341, Validation Loss: 0.015797875821590424, Learning Rate: 1.4411518807585605e-05\n",
      "Epoch: 955, Training Loss: 0.012080000713467598, Validation Loss: 0.015832778066396713, Learning Rate: 1.4411518807585605e-05\n",
      "Epoch: 956, Training Loss: 0.012086883187294006, Validation Loss: 0.015954207628965378, Learning Rate: 1.4411518807585605e-05\n",
      "Epoch: 957, Training Loss: 0.012089641764760017, Validation Loss: 0.015977373346686363, Learning Rate: 1.4411518807585605e-05\n",
      "Epoch: 958, Training Loss: 0.012070906348526478, Validation Loss: 0.01593730039894581, Learning Rate: 1.4411518807585605e-05\n",
      "Epoch: 959, Training Loss: 0.012104046531021595, Validation Loss: 0.01596806012094021, Learning Rate: 1.4411518807585605e-05\n",
      "Epoch: 960, Training Loss: 0.012066175229847431, Validation Loss: 0.015926189720630646, Learning Rate: 1.4411518807585605e-05\n",
      "Epoch: 961, Training Loss: 0.012069715186953545, Validation Loss: 0.015753773972392082, Learning Rate: 1.4411518807585605e-05\n",
      "Epoch: 962, Training Loss: 0.012071044184267521, Validation Loss: 0.015837596729397774, Learning Rate: 1.4411518807585605e-05\n",
      "Epoch: 963, Training Loss: 0.012112804688513279, Validation Loss: 0.015931397676467896, Learning Rate: 1.4411518807585605e-05\n",
      "Epoch: 964, Training Loss: 0.012073023244738579, Validation Loss: 0.015889888629317284, Learning Rate: 1.4411518807585605e-05\n",
      "Epoch: 965, Training Loss: 0.012071528472006321, Validation Loss: 0.01593124121427536, Learning Rate: 1.4411518807585605e-05\n",
      "Epoch: 966, Training Loss: 0.012099901214241982, Validation Loss: 0.01593118906021118, Learning Rate: 1.4411518807585605e-05\n",
      "Epoch: 967, Training Loss: 0.012060613371431828, Validation Loss: 0.015884658321738243, Learning Rate: 1.4411518807585605e-05\n",
      "Epoch: 968, Training Loss: 0.012097788974642754, Validation Loss: 0.015947001054883003, Learning Rate: 1.4411518807585605e-05\n",
      "Epoch: 969, Training Loss: 0.01207047887146473, Validation Loss: 0.015923380851745605, Learning Rate: 1.4411518807585605e-05\n",
      "Epoch: 970, Training Loss: 0.012065845541656017, Validation Loss: 0.015905871987342834, Learning Rate: 1.4411518807585605e-05\n",
      "Epoch: 971, Training Loss: 0.012058383785188198, Validation Loss: 0.01593666709959507, Learning Rate: 1.4411518807585605e-05\n",
      "Epoch: 972, Training Loss: 0.012065974995493889, Validation Loss: 0.015876706689596176, Learning Rate: 1.4411518807585605e-05\n",
      "Epoch: 973, Training Loss: 0.012079705484211445, Validation Loss: 0.01596222259104252, Learning Rate: 1.4411518807585605e-05\n",
      "Epoch: 974, Training Loss: 0.012041985057294369, Validation Loss: 0.015931693837046623, Learning Rate: 1.4411518807585605e-05\n",
      "Epoch: 975, Training Loss: 0.012080859392881393, Validation Loss: 0.015949808061122894, Learning Rate: 1.4411518807585605e-05\n",
      "Epoch: 976, Training Loss: 0.012070509605109692, Validation Loss: 0.015846282243728638, Learning Rate: 1.4411518807585605e-05\n",
      "Epoch: 977, Training Loss: 0.012052892707288265, Validation Loss: 0.01589028164744377, Learning Rate: 1.4411518807585605e-05\n",
      "Epoch: 978, Training Loss: 0.012045011855661869, Validation Loss: 0.015990743413567543, Learning Rate: 1.4411518807585605e-05\n",
      "Epoch: 979, Training Loss: 0.012070296332240105, Validation Loss: 0.015859300270676613, Learning Rate: 1.4411518807585605e-05\n",
      "Epoch: 980, Training Loss: 0.012048664502799511, Validation Loss: 0.015804285183548927, Learning Rate: 1.4411518807585605e-05\n",
      "Epoch: 981, Training Loss: 0.012049301527440548, Validation Loss: 0.01581960916519165, Learning Rate: 1.4411518807585605e-05\n",
      "Epoch: 982, Training Loss: 0.012043637223541737, Validation Loss: 0.015921829268336296, Learning Rate: 1.4411518807585605e-05\n",
      "Epoch: 983, Training Loss: 0.012039671652019024, Validation Loss: 0.015932723879814148, Learning Rate: 1.4411518807585605e-05\n",
      "Epoch: 984, Training Loss: 0.012063632719218731, Validation Loss: 0.01590464450418949, Learning Rate: 1.4411518807585605e-05\n",
      "Epoch: 985, Training Loss: 0.012044272385537624, Validation Loss: 0.01589047722518444, Learning Rate: 1.4411518807585605e-05\n",
      "Epoch: 986, Training Loss: 0.012034964747726917, Validation Loss: 0.015848932787775993, Learning Rate: 1.4411518807585605e-05\n",
      "Epoch: 987, Training Loss: 0.012047293595969677, Validation Loss: 0.015807047486305237, Learning Rate: 1.4411518807585605e-05\n",
      "Epoch: 988, Training Loss: 0.012040352448821068, Validation Loss: 0.015847595408558846, Learning Rate: 1.4411518807585605e-05\n",
      "Epoch: 989, Training Loss: 0.012044613249599934, Validation Loss: 0.015915492549538612, Learning Rate: 1.4411518807585605e-05\n",
      "Epoch: 990, Training Loss: 0.01206193771213293, Validation Loss: 0.01593897119164467, Learning Rate: 1.4411518807585605e-05\n",
      "Epoch: 991, Training Loss: 0.012038893066346645, Validation Loss: 0.01594685949385166, Learning Rate: 1.4411518807585605e-05\n",
      "Epoch: 992, Training Loss: 0.012059329077601433, Validation Loss: 0.015950726345181465, Learning Rate: 1.4411518807585605e-05\n",
      "Epoch: 993, Training Loss: 0.012027954682707787, Validation Loss: 0.01584629714488983, Learning Rate: 1.4411518807585605e-05\n",
      "Epoch: 994, Training Loss: 0.012033886276185513, Validation Loss: 0.015910474583506584, Learning Rate: 1.4411518807585605e-05\n",
      "Epoch: 995, Training Loss: 0.012032916769385338, Validation Loss: 0.015673812478780746, Learning Rate: 1.4411518807585605e-05\n",
      "Epoch: 996, Training Loss: 0.012045610696077347, Validation Loss: 0.01583034358918667, Learning Rate: 1.4411518807585605e-05\n",
      "Epoch: 997, Training Loss: 0.012029485777020454, Validation Loss: 0.015802960842847824, Learning Rate: 1.4411518807585605e-05\n",
      "Epoch: 998, Training Loss: 0.012020932510495186, Validation Loss: 0.015828674659132957, Learning Rate: 1.4411518807585605e-05\n",
      "Epoch: 999, Training Loss: 0.012019925750792027, Validation Loss: 0.015870751813054085, Learning Rate: 1.4411518807585605e-05\n",
      "Epoch: 1000, Training Loss: 0.012004714459180832, Validation Loss: 0.015808498486876488, Learning Rate: 1.1529215046068485e-05\n",
      "Epoch: 1001, Training Loss: 0.0120064876973629, Validation Loss: 0.015849536284804344, Learning Rate: 1.1529215046068485e-05\n",
      "Epoch: 1002, Training Loss: 0.012005914002656937, Validation Loss: 0.015885673463344574, Learning Rate: 1.1529215046068485e-05\n",
      "Epoch: 1003, Training Loss: 0.012008496560156345, Validation Loss: 0.01584840752184391, Learning Rate: 1.1529215046068485e-05\n",
      "Epoch: 1004, Training Loss: 0.012012481689453125, Validation Loss: 0.015855446457862854, Learning Rate: 1.1529215046068485e-05\n",
      "Epoch: 1005, Training Loss: 0.012034296058118343, Validation Loss: 0.015823619440197945, Learning Rate: 1.1529215046068485e-05\n",
      "Epoch: 1006, Training Loss: 0.012010524049401283, Validation Loss: 0.01587972603738308, Learning Rate: 1.1529215046068485e-05\n",
      "Epoch: 1007, Training Loss: 0.012012604624032974, Validation Loss: 0.015815280377864838, Learning Rate: 1.1529215046068485e-05\n",
      "Epoch: 1008, Training Loss: 0.012009389698505402, Validation Loss: 0.015785768628120422, Learning Rate: 1.1529215046068485e-05\n",
      "Epoch: 1009, Training Loss: 0.012014958076179028, Validation Loss: 0.015898140147328377, Learning Rate: 1.1529215046068485e-05\n",
      "Epoch: 1010, Training Loss: 0.012037882581353188, Validation Loss: 0.015821423381567, Learning Rate: 1.1529215046068485e-05\n",
      "Epoch: 1011, Training Loss: 0.012007439509034157, Validation Loss: 0.015799373388290405, Learning Rate: 1.1529215046068485e-05\n",
      "Epoch: 1012, Training Loss: 0.012036008760333061, Validation Loss: 0.01581847481429577, Learning Rate: 1.1529215046068485e-05\n",
      "Epoch: 1013, Training Loss: 0.012010937556624413, Validation Loss: 0.015778841450810432, Learning Rate: 1.1529215046068485e-05\n",
      "Epoch: 1014, Training Loss: 0.011996014043688774, Validation Loss: 0.01582835428416729, Learning Rate: 1.1529215046068485e-05\n",
      "Epoch: 1015, Training Loss: 0.011999574489891529, Validation Loss: 0.015874706208705902, Learning Rate: 1.1529215046068485e-05\n",
      "Epoch: 1016, Training Loss: 0.012007969431579113, Validation Loss: 0.01581650972366333, Learning Rate: 1.1529215046068485e-05\n",
      "Epoch: 1017, Training Loss: 0.012003724463284016, Validation Loss: 0.01584484986960888, Learning Rate: 1.1529215046068485e-05\n",
      "Epoch: 1018, Training Loss: 0.011995485052466393, Validation Loss: 0.015844454988837242, Learning Rate: 1.1529215046068485e-05\n",
      "Epoch: 1019, Training Loss: 0.0119947399944067, Validation Loss: 0.015743495896458626, Learning Rate: 1.1529215046068485e-05\n",
      "Epoch: 1020, Training Loss: 0.012002609670162201, Validation Loss: 0.01572190597653389, Learning Rate: 1.1529215046068485e-05\n",
      "Epoch: 1021, Training Loss: 0.011979084461927414, Validation Loss: 0.015771307051181793, Learning Rate: 1.1529215046068485e-05\n",
      "Epoch: 1022, Training Loss: 0.012003431096673012, Validation Loss: 0.01609913446009159, Learning Rate: 1.1529215046068485e-05\n",
      "Epoch: 1023, Training Loss: 0.01198857743293047, Validation Loss: 0.016106072813272476, Learning Rate: 1.1529215046068485e-05\n",
      "Epoch: 1024, Training Loss: 0.011989782564342022, Validation Loss: 0.015988821163773537, Learning Rate: 1.1529215046068485e-05\n",
      "Epoch: 1025, Training Loss: 0.011997455731034279, Validation Loss: 0.01595006324350834, Learning Rate: 1.1529215046068485e-05\n",
      "Epoch: 1026, Training Loss: 0.0119864447042346, Validation Loss: 0.015991931781172752, Learning Rate: 1.1529215046068485e-05\n",
      "Epoch: 1027, Training Loss: 0.01201238390058279, Validation Loss: 0.015948817133903503, Learning Rate: 1.1529215046068485e-05\n",
      "Epoch: 1028, Training Loss: 0.011991978622972965, Validation Loss: 0.015961339697241783, Learning Rate: 1.1529215046068485e-05\n",
      "Epoch: 1029, Training Loss: 0.011977527290582657, Validation Loss: 0.015895983204245567, Learning Rate: 1.1529215046068485e-05\n",
      "Epoch: 1030, Training Loss: 0.011977293528616428, Validation Loss: 0.015918148681521416, Learning Rate: 1.1529215046068485e-05\n",
      "Epoch: 1031, Training Loss: 0.011978643015027046, Validation Loss: 0.015921736136078835, Learning Rate: 1.1529215046068485e-05\n",
      "Epoch: 1032, Training Loss: 0.011981231160461903, Validation Loss: 0.015913767740130424, Learning Rate: 1.1529215046068485e-05\n",
      "Epoch: 1033, Training Loss: 0.011982238851487637, Validation Loss: 0.01593797095119953, Learning Rate: 1.1529215046068485e-05\n",
      "Epoch: 1034, Training Loss: 0.01198524609208107, Validation Loss: 0.01586836390197277, Learning Rate: 1.1529215046068485e-05\n",
      "Epoch: 1035, Training Loss: 0.01198478415608406, Validation Loss: 0.015811393037438393, Learning Rate: 1.1529215046068485e-05\n",
      "Epoch: 1036, Training Loss: 0.011986972764134407, Validation Loss: 0.01592588797211647, Learning Rate: 1.1529215046068485e-05\n",
      "Epoch: 1037, Training Loss: 0.012016454711556435, Validation Loss: 0.01596049591898918, Learning Rate: 1.1529215046068485e-05\n",
      "Epoch: 1038, Training Loss: 0.01199499610811472, Validation Loss: 0.01593432016670704, Learning Rate: 1.1529215046068485e-05\n",
      "Epoch: 1039, Training Loss: 0.011964709497988224, Validation Loss: 0.01589026115834713, Learning Rate: 1.1529215046068485e-05\n",
      "Epoch: 1040, Training Loss: 0.011980121023952961, Validation Loss: 0.016211578622460365, Learning Rate: 1.1529215046068485e-05\n",
      "Epoch: 1041, Training Loss: 0.011993532069027424, Validation Loss: 0.01606835424900055, Learning Rate: 1.1529215046068485e-05\n",
      "Epoch: 1042, Training Loss: 0.011982304975390434, Validation Loss: 0.01609000377357006, Learning Rate: 1.1529215046068485e-05\n",
      "Epoch: 1043, Training Loss: 0.011975604109466076, Validation Loss: 0.01608920842409134, Learning Rate: 1.1529215046068485e-05\n",
      "Epoch: 1044, Training Loss: 0.011982239782810211, Validation Loss: 0.01604204997420311, Learning Rate: 1.1529215046068485e-05\n",
      "Epoch: 1045, Training Loss: 0.012007167562842369, Validation Loss: 0.016070980578660965, Learning Rate: 1.1529215046068485e-05\n",
      "Epoch: 1046, Training Loss: 0.011973130516707897, Validation Loss: 0.016019020229578018, Learning Rate: 1.1529215046068485e-05\n",
      "Epoch: 1047, Training Loss: 0.011968201957643032, Validation Loss: 0.015890395268797874, Learning Rate: 1.1529215046068485e-05\n",
      "Epoch: 1048, Training Loss: 0.011961112730205059, Validation Loss: 0.015939148142933846, Learning Rate: 1.1529215046068485e-05\n",
      "Epoch: 1049, Training Loss: 0.01195213571190834, Validation Loss: 0.015918176621198654, Learning Rate: 1.1529215046068485e-05\n",
      "Epoch: 1050, Training Loss: 0.01195036992430687, Validation Loss: 0.01591603457927704, Learning Rate: 9.223372036854789e-06\n",
      "Epoch: 1051, Training Loss: 0.011946545913815498, Validation Loss: 0.015865083783864975, Learning Rate: 9.223372036854789e-06\n",
      "Epoch: 1052, Training Loss: 0.011950921267271042, Validation Loss: 0.01592843607068062, Learning Rate: 9.223372036854789e-06\n",
      "Epoch: 1053, Training Loss: 0.011953650042414665, Validation Loss: 0.01592513732612133, Learning Rate: 9.223372036854789e-06\n",
      "Epoch: 1054, Training Loss: 0.011947416700422764, Validation Loss: 0.01586933806538582, Learning Rate: 9.223372036854789e-06\n",
      "Epoch: 1055, Training Loss: 0.011949043720960617, Validation Loss: 0.015867117792367935, Learning Rate: 9.223372036854789e-06\n",
      "Epoch: 1056, Training Loss: 0.0119470851495862, Validation Loss: 0.01594102941453457, Learning Rate: 9.223372036854789e-06\n",
      "Epoch: 1057, Training Loss: 0.01194877177476883, Validation Loss: 0.015872400254011154, Learning Rate: 9.223372036854789e-06\n",
      "Epoch: 1058, Training Loss: 0.011941407807171345, Validation Loss: 0.015889737755060196, Learning Rate: 9.223372036854789e-06\n",
      "Epoch: 1059, Training Loss: 0.011943920515477657, Validation Loss: 0.015871968120336533, Learning Rate: 9.223372036854789e-06\n",
      "Epoch: 1060, Training Loss: 0.011944138444960117, Validation Loss: 0.015916159376502037, Learning Rate: 9.223372036854789e-06\n",
      "Epoch: 1061, Training Loss: 0.011943415738642216, Validation Loss: 0.015868689864873886, Learning Rate: 9.223372036854789e-06\n",
      "Epoch: 1062, Training Loss: 0.011939370073378086, Validation Loss: 0.015861615538597107, Learning Rate: 9.223372036854789e-06\n",
      "Epoch: 1063, Training Loss: 0.011953911744058132, Validation Loss: 0.0159002672880888, Learning Rate: 9.223372036854789e-06\n",
      "Epoch: 1064, Training Loss: 0.01194089837372303, Validation Loss: 0.01584986411035061, Learning Rate: 9.223372036854789e-06\n",
      "Epoch: 1065, Training Loss: 0.01194552518427372, Validation Loss: 0.01583491638302803, Learning Rate: 9.223372036854789e-06\n",
      "Epoch: 1066, Training Loss: 0.011940110474824905, Validation Loss: 0.01583259366452694, Learning Rate: 9.223372036854789e-06\n",
      "Epoch: 1067, Training Loss: 0.011936607770621777, Validation Loss: 0.01589229330420494, Learning Rate: 9.223372036854789e-06\n",
      "Epoch: 1068, Training Loss: 0.011934931389987469, Validation Loss: 0.015806805342435837, Learning Rate: 9.223372036854789e-06\n",
      "Epoch: 1069, Training Loss: 0.011944672092795372, Validation Loss: 0.015841012820601463, Learning Rate: 9.223372036854789e-06\n",
      "Epoch: 1070, Training Loss: 0.011935391463339329, Validation Loss: 0.01581292413175106, Learning Rate: 9.223372036854789e-06\n",
      "Epoch: 1071, Training Loss: 0.011936377733945847, Validation Loss: 0.01580481417477131, Learning Rate: 9.223372036854789e-06\n",
      "Epoch: 1072, Training Loss: 0.011933431029319763, Validation Loss: 0.015814216807484627, Learning Rate: 9.223372036854789e-06\n",
      "Epoch: 1073, Training Loss: 0.011930176056921482, Validation Loss: 0.015838945284485817, Learning Rate: 9.223372036854789e-06\n",
      "Epoch: 1074, Training Loss: 0.011929553002119064, Validation Loss: 0.01584572345018387, Learning Rate: 9.223372036854789e-06\n",
      "Epoch: 1075, Training Loss: 0.011932010762393475, Validation Loss: 0.015843451023101807, Learning Rate: 9.223372036854789e-06\n",
      "Epoch: 1076, Training Loss: 0.01194167323410511, Validation Loss: 0.01585567742586136, Learning Rate: 9.223372036854789e-06\n",
      "Epoch: 1077, Training Loss: 0.01194021012634039, Validation Loss: 0.01588209718465805, Learning Rate: 9.223372036854789e-06\n",
      "Epoch: 1078, Training Loss: 0.01193329505622387, Validation Loss: 0.015756333246827126, Learning Rate: 9.223372036854789e-06\n",
      "Epoch: 1079, Training Loss: 0.011924616992473602, Validation Loss: 0.015768399462103844, Learning Rate: 9.223372036854789e-06\n",
      "Epoch: 1080, Training Loss: 0.011925473809242249, Validation Loss: 0.015759490430355072, Learning Rate: 9.223372036854789e-06\n",
      "Epoch: 1081, Training Loss: 0.01192161999642849, Validation Loss: 0.015804564580321312, Learning Rate: 9.223372036854789e-06\n",
      "Epoch: 1082, Training Loss: 0.011927939020097256, Validation Loss: 0.0157439224421978, Learning Rate: 9.223372036854789e-06\n",
      "Epoch: 1083, Training Loss: 0.011923804879188538, Validation Loss: 0.01582643762230873, Learning Rate: 9.223372036854789e-06\n",
      "Epoch: 1084, Training Loss: 0.011929306201636791, Validation Loss: 0.015812242403626442, Learning Rate: 9.223372036854789e-06\n",
      "Epoch: 1085, Training Loss: 0.011923759244382381, Validation Loss: 0.01582365669310093, Learning Rate: 9.223372036854789e-06\n",
      "Epoch: 1086, Training Loss: 0.011914624832570553, Validation Loss: 0.01581641286611557, Learning Rate: 9.223372036854789e-06\n",
      "Epoch: 1087, Training Loss: 0.011929846368730068, Validation Loss: 0.01579231396317482, Learning Rate: 9.223372036854789e-06\n",
      "Epoch: 1088, Training Loss: 0.011935237795114517, Validation Loss: 0.016041282564401627, Learning Rate: 9.223372036854789e-06\n",
      "Epoch: 1089, Training Loss: 0.011918545700609684, Validation Loss: 0.0159468874335289, Learning Rate: 9.223372036854789e-06\n",
      "Epoch: 1090, Training Loss: 0.011917976662516594, Validation Loss: 0.015919756144285202, Learning Rate: 9.223372036854789e-06\n",
      "Epoch: 1091, Training Loss: 0.011942335404455662, Validation Loss: 0.01590602844953537, Learning Rate: 9.223372036854789e-06\n",
      "Epoch: 1092, Training Loss: 0.011910656467080116, Validation Loss: 0.015890957787632942, Learning Rate: 9.223372036854789e-06\n",
      "Epoch: 1093, Training Loss: 0.011915133334696293, Validation Loss: 0.01587561145424843, Learning Rate: 9.223372036854789e-06\n",
      "Epoch: 1094, Training Loss: 0.011921655386686325, Validation Loss: 0.01592240296304226, Learning Rate: 9.223372036854789e-06\n",
      "Epoch: 1095, Training Loss: 0.011909733526408672, Validation Loss: 0.015889249742031097, Learning Rate: 9.223372036854789e-06\n",
      "Epoch: 1096, Training Loss: 0.011921469122171402, Validation Loss: 0.01595558412373066, Learning Rate: 9.223372036854789e-06\n",
      "Epoch: 1097, Training Loss: 0.01191844791173935, Validation Loss: 0.01594950444996357, Learning Rate: 9.223372036854789e-06\n",
      "Epoch: 1098, Training Loss: 0.011921750381588936, Validation Loss: 0.015942078083753586, Learning Rate: 9.223372036854789e-06\n",
      "Epoch: 1099, Training Loss: 0.011916963383555412, Validation Loss: 0.015912311151623726, Learning Rate: 9.223372036854789e-06\n",
      "Epoch: 1100, Training Loss: 0.011915476061403751, Validation Loss: 0.015983982011675835, Learning Rate: 7.378697629483831e-06\n",
      "Epoch: 1101, Training Loss: 0.011899999342858791, Validation Loss: 0.015931371599435806, Learning Rate: 7.378697629483831e-06\n",
      "Epoch: 1102, Training Loss: 0.011905823834240437, Validation Loss: 0.01597914844751358, Learning Rate: 7.378697629483831e-06\n",
      "Epoch: 1103, Training Loss: 0.011902735568583012, Validation Loss: 0.015906529501080513, Learning Rate: 7.378697629483831e-06\n",
      "Epoch: 1104, Training Loss: 0.011896922253072262, Validation Loss: 0.01590709201991558, Learning Rate: 7.378697629483831e-06\n",
      "Epoch: 1105, Training Loss: 0.011899800039827824, Validation Loss: 0.015835288912057877, Learning Rate: 7.378697629483831e-06\n",
      "Epoch: 1106, Training Loss: 0.011894093826413155, Validation Loss: 0.015848249197006226, Learning Rate: 7.378697629483831e-06\n",
      "Epoch: 1107, Training Loss: 0.011898265220224857, Validation Loss: 0.015857063233852386, Learning Rate: 7.378697629483831e-06\n",
      "Epoch: 1108, Training Loss: 0.011898915283381939, Validation Loss: 0.015876851975917816, Learning Rate: 7.378697629483831e-06\n",
      "Epoch: 1109, Training Loss: 0.011901134625077248, Validation Loss: 0.01585320010781288, Learning Rate: 7.378697629483831e-06\n",
      "Epoch: 1110, Training Loss: 0.011895351111888885, Validation Loss: 0.015873966738581657, Learning Rate: 7.378697629483831e-06\n",
      "Epoch: 1111, Training Loss: 0.011897172778844833, Validation Loss: 0.015846386551856995, Learning Rate: 7.378697629483831e-06\n",
      "Epoch: 1112, Training Loss: 0.011893956921994686, Validation Loss: 0.015838395804166794, Learning Rate: 7.378697629483831e-06\n",
      "Epoch: 1113, Training Loss: 0.011893737129867077, Validation Loss: 0.01582135446369648, Learning Rate: 7.378697629483831e-06\n",
      "Epoch: 1114, Training Loss: 0.011898982338607311, Validation Loss: 0.015836209058761597, Learning Rate: 7.378697629483831e-06\n",
      "Epoch: 1115, Training Loss: 0.011895276606082916, Validation Loss: 0.01580461487174034, Learning Rate: 7.378697629483831e-06\n",
      "Epoch: 1116, Training Loss: 0.01189445611089468, Validation Loss: 0.015854239463806152, Learning Rate: 7.378697629483831e-06\n",
      "Epoch: 1117, Training Loss: 0.011894267983734608, Validation Loss: 0.015763984993100166, Learning Rate: 7.378697629483831e-06\n",
      "Epoch: 1118, Training Loss: 0.011892507784068584, Validation Loss: 0.015853945165872574, Learning Rate: 7.378697629483831e-06\n",
      "Epoch: 1119, Training Loss: 0.011901039630174637, Validation Loss: 0.01582390069961548, Learning Rate: 7.378697629483831e-06\n",
      "Epoch: 1120, Training Loss: 0.011891757138073444, Validation Loss: 0.015899360179901123, Learning Rate: 7.378697629483831e-06\n",
      "Epoch: 1121, Training Loss: 0.011891582049429417, Validation Loss: 0.015815235674381256, Learning Rate: 7.378697629483831e-06\n",
      "Epoch: 1122, Training Loss: 0.011897692456841469, Validation Loss: 0.015831472352147102, Learning Rate: 7.378697629483831e-06\n",
      "Epoch: 1123, Training Loss: 0.011882676742970943, Validation Loss: 0.015814069658517838, Learning Rate: 7.378697629483831e-06\n",
      "Epoch: 1124, Training Loss: 0.011891118250787258, Validation Loss: 0.015862062573432922, Learning Rate: 7.378697629483831e-06\n",
      "Epoch: 1125, Training Loss: 0.011889742687344551, Validation Loss: 0.01577077805995941, Learning Rate: 7.378697629483831e-06\n",
      "Epoch: 1126, Training Loss: 0.011896293610334396, Validation Loss: 0.015690429136157036, Learning Rate: 7.378697629483831e-06\n",
      "Epoch: 1127, Training Loss: 0.011893859133124352, Validation Loss: 0.015745151787996292, Learning Rate: 7.378697629483831e-06\n",
      "Epoch: 1128, Training Loss: 0.011886734515428543, Validation Loss: 0.015703639015555382, Learning Rate: 7.378697629483831e-06\n",
      "Epoch: 1129, Training Loss: 0.011886101216077805, Validation Loss: 0.015749193727970123, Learning Rate: 7.378697629483831e-06\n",
      "Epoch: 1130, Training Loss: 0.011881601996719837, Validation Loss: 0.015811188146471977, Learning Rate: 7.378697629483831e-06\n",
      "Epoch: 1131, Training Loss: 0.011881385929882526, Validation Loss: 0.015811696648597717, Learning Rate: 7.378697629483831e-06\n",
      "Epoch: 1132, Training Loss: 0.011880464851856232, Validation Loss: 0.0157356858253479, Learning Rate: 7.378697629483831e-06\n",
      "Epoch: 1133, Training Loss: 0.011879091151058674, Validation Loss: 0.015785129740834236, Learning Rate: 7.378697629483831e-06\n",
      "Epoch: 1134, Training Loss: 0.011879169382154942, Validation Loss: 0.01572006568312645, Learning Rate: 7.378697629483831e-06\n",
      "Epoch: 1135, Training Loss: 0.011885198764503002, Validation Loss: 0.01571725681424141, Learning Rate: 7.378697629483831e-06\n",
      "Epoch: 1136, Training Loss: 0.011877517215907574, Validation Loss: 0.015737758949398994, Learning Rate: 7.378697629483831e-06\n",
      "Epoch: 1137, Training Loss: 0.011876361444592476, Validation Loss: 0.01571049354970455, Learning Rate: 7.378697629483831e-06\n",
      "Epoch: 1138, Training Loss: 0.01187645923346281, Validation Loss: 0.01580638252198696, Learning Rate: 7.378697629483831e-06\n",
      "Epoch: 1139, Training Loss: 0.011881127022206783, Validation Loss: 0.015774106606841087, Learning Rate: 7.378697629483831e-06\n",
      "Epoch: 1140, Training Loss: 0.011880210600793362, Validation Loss: 0.01579282246530056, Learning Rate: 7.378697629483831e-06\n",
      "Epoch: 1141, Training Loss: 0.011887920089066029, Validation Loss: 0.015761854127049446, Learning Rate: 7.378697629483831e-06\n",
      "Epoch: 1142, Training Loss: 0.011886723339557648, Validation Loss: 0.015653926879167557, Learning Rate: 7.378697629483831e-06\n",
      "Epoch: 1143, Training Loss: 0.011892946437001228, Validation Loss: 0.01583772711455822, Learning Rate: 7.378697629483831e-06\n",
      "Epoch: 1144, Training Loss: 0.01187127735465765, Validation Loss: 0.015751782804727554, Learning Rate: 7.378697629483831e-06\n",
      "Epoch: 1145, Training Loss: 0.011874079704284668, Validation Loss: 0.015872590243816376, Learning Rate: 7.378697629483831e-06\n",
      "Epoch: 1146, Training Loss: 0.011882844381034374, Validation Loss: 0.015793398022651672, Learning Rate: 7.378697629483831e-06\n",
      "Epoch: 1147, Training Loss: 0.011877751909196377, Validation Loss: 0.015754666179418564, Learning Rate: 7.378697629483831e-06\n",
      "Epoch: 1148, Training Loss: 0.01187420915812254, Validation Loss: 0.015796897932887077, Learning Rate: 7.378697629483831e-06\n",
      "Epoch: 1149, Training Loss: 0.01187841221690178, Validation Loss: 0.015735093504190445, Learning Rate: 7.378697629483831e-06\n",
      "Epoch: 1150, Training Loss: 0.011864595115184784, Validation Loss: 0.01581442356109619, Learning Rate: 5.902958103587065e-06\n",
      "Epoch: 1151, Training Loss: 0.011864585801959038, Validation Loss: 0.01571084000170231, Learning Rate: 5.902958103587065e-06\n",
      "Epoch: 1152, Training Loss: 0.01186650525778532, Validation Loss: 0.015721211209893227, Learning Rate: 5.902958103587065e-06\n",
      "Epoch: 1153, Training Loss: 0.011867029592394829, Validation Loss: 0.015671391040086746, Learning Rate: 5.902958103587065e-06\n",
      "Epoch: 1154, Training Loss: 0.0118632260710001, Validation Loss: 0.01574067957699299, Learning Rate: 5.902958103587065e-06\n",
      "Epoch: 1155, Training Loss: 0.011858800426125526, Validation Loss: 0.015764502808451653, Learning Rate: 5.902958103587065e-06\n",
      "Epoch: 1156, Training Loss: 0.011869600974023342, Validation Loss: 0.015755150467157364, Learning Rate: 5.902958103587065e-06\n",
      "Epoch: 1157, Training Loss: 0.011861405335366726, Validation Loss: 0.015756094828248024, Learning Rate: 5.902958103587065e-06\n",
      "Epoch: 1158, Training Loss: 0.011860262602567673, Validation Loss: 0.015709949657320976, Learning Rate: 5.902958103587065e-06\n",
      "Epoch: 1159, Training Loss: 0.011864873580634594, Validation Loss: 0.015741143375635147, Learning Rate: 5.902958103587065e-06\n",
      "Epoch: 1160, Training Loss: 0.011857426725327969, Validation Loss: 0.01573377288877964, Learning Rate: 5.902958103587065e-06\n",
      "Epoch: 1161, Training Loss: 0.011860777623951435, Validation Loss: 0.01577962003648281, Learning Rate: 5.902958103587065e-06\n",
      "Epoch: 1162, Training Loss: 0.011859177611768246, Validation Loss: 0.015782594680786133, Learning Rate: 5.902958103587065e-06\n",
      "Epoch: 1163, Training Loss: 0.011861633509397507, Validation Loss: 0.015744883567094803, Learning Rate: 5.902958103587065e-06\n",
      "Epoch: 1164, Training Loss: 0.011858533136546612, Validation Loss: 0.01579931564629078, Learning Rate: 5.902958103587065e-06\n",
      "Epoch: 1165, Training Loss: 0.0118571687489748, Validation Loss: 0.015737110748887062, Learning Rate: 5.902958103587065e-06\n",
      "Epoch: 1166, Training Loss: 0.011861865408718586, Validation Loss: 0.01579323783516884, Learning Rate: 5.902958103587065e-06\n",
      "Epoch: 1167, Training Loss: 0.011856877245008945, Validation Loss: 0.015792585909366608, Learning Rate: 5.902958103587065e-06\n",
      "Epoch: 1168, Training Loss: 0.011855085380375385, Validation Loss: 0.01583218015730381, Learning Rate: 5.902958103587065e-06\n",
      "Epoch: 1169, Training Loss: 0.011854254640638828, Validation Loss: 0.015728455036878586, Learning Rate: 5.902958103587065e-06\n",
      "Epoch: 1170, Training Loss: 0.011857503093779087, Validation Loss: 0.01578894816339016, Learning Rate: 5.902958103587065e-06\n",
      "Epoch: 1171, Training Loss: 0.011861901730298996, Validation Loss: 0.01568262092769146, Learning Rate: 5.902958103587065e-06\n",
      "Epoch: 1172, Training Loss: 0.011857014149427414, Validation Loss: 0.015778901055455208, Learning Rate: 5.902958103587065e-06\n",
      "Epoch: 1173, Training Loss: 0.011858882382512093, Validation Loss: 0.015787729993462563, Learning Rate: 5.902958103587065e-06\n",
      "Epoch: 1174, Training Loss: 0.011859237216413021, Validation Loss: 0.01577400229871273, Learning Rate: 5.902958103587065e-06\n",
      "Epoch: 1175, Training Loss: 0.011849958449602127, Validation Loss: 0.01573369838297367, Learning Rate: 5.902958103587065e-06\n",
      "Epoch: 1176, Training Loss: 0.011851808987557888, Validation Loss: 0.015750102698802948, Learning Rate: 5.902958103587065e-06\n",
      "Epoch: 1177, Training Loss: 0.0118483891710639, Validation Loss: 0.01575549878180027, Learning Rate: 5.902958103587065e-06\n",
      "Epoch: 1178, Training Loss: 0.011849178932607174, Validation Loss: 0.015750430524349213, Learning Rate: 5.902958103587065e-06\n",
      "Epoch: 1179, Training Loss: 0.011857648380100727, Validation Loss: 0.01583235152065754, Learning Rate: 5.902958103587065e-06\n",
      "Epoch: 1180, Training Loss: 0.011848300695419312, Validation Loss: 0.01569085381925106, Learning Rate: 5.902958103587065e-06\n",
      "Epoch: 1181, Training Loss: 0.011847605928778648, Validation Loss: 0.015687303617596626, Learning Rate: 5.902958103587065e-06\n",
      "Epoch: 1182, Training Loss: 0.011847326532006264, Validation Loss: 0.01568063534796238, Learning Rate: 5.902958103587065e-06\n",
      "Epoch: 1183, Training Loss: 0.011850782670080662, Validation Loss: 0.01566699892282486, Learning Rate: 5.902958103587065e-06\n",
      "Epoch: 1184, Training Loss: 0.01185568142682314, Validation Loss: 0.015719514340162277, Learning Rate: 5.902958103587065e-06\n",
      "Epoch: 1185, Training Loss: 0.011847891844809055, Validation Loss: 0.01571767032146454, Learning Rate: 5.902958103587065e-06\n",
      "Epoch: 1186, Training Loss: 0.011842513456940651, Validation Loss: 0.015689564868807793, Learning Rate: 5.902958103587065e-06\n",
      "Epoch: 1187, Training Loss: 0.01184967253357172, Validation Loss: 0.015733810141682625, Learning Rate: 5.902958103587065e-06\n",
      "Epoch: 1188, Training Loss: 0.011850276030600071, Validation Loss: 0.015736475586891174, Learning Rate: 5.902958103587065e-06\n",
      "Epoch: 1189, Training Loss: 0.01184307225048542, Validation Loss: 0.01568790338933468, Learning Rate: 5.902958103587065e-06\n",
      "Epoch: 1190, Training Loss: 0.011844394728541374, Validation Loss: 0.015715571120381355, Learning Rate: 5.902958103587065e-06\n",
      "Epoch: 1191, Training Loss: 0.01184000913053751, Validation Loss: 0.015722857788205147, Learning Rate: 5.902958103587065e-06\n",
      "Epoch: 1192, Training Loss: 0.011847185902297497, Validation Loss: 0.01570923440158367, Learning Rate: 5.902958103587065e-06\n",
      "Epoch: 1193, Training Loss: 0.011839007027447224, Validation Loss: 0.015667136758565903, Learning Rate: 5.902958103587065e-06\n",
      "Epoch: 1194, Training Loss: 0.011845065280795097, Validation Loss: 0.01573275588452816, Learning Rate: 5.902958103587065e-06\n",
      "Epoch: 1195, Training Loss: 0.011840852908790112, Validation Loss: 0.01566782407462597, Learning Rate: 5.902958103587065e-06\n",
      "Epoch: 1196, Training Loss: 0.01184055395424366, Validation Loss: 0.01570642925798893, Learning Rate: 5.902958103587065e-06\n",
      "Epoch: 1197, Training Loss: 0.011839115060865879, Validation Loss: 0.01567850448191166, Learning Rate: 5.902958103587065e-06\n",
      "Epoch: 1198, Training Loss: 0.011841841042041779, Validation Loss: 0.015706924721598625, Learning Rate: 5.902958103587065e-06\n",
      "Epoch: 1199, Training Loss: 0.011839712038636208, Validation Loss: 0.015691213309764862, Learning Rate: 5.902958103587065e-06\n"
     ]
    }
   ],
   "source": [
    "epoches = 1200\n",
    "\n",
    "# Training process\n",
    "\n",
    "for epoch in range(epoches):\n",
    "    for x_batch, y_batch, u_batch in data_loader_latent:\n",
    "        optimizer_koopman.zero_grad()\n",
    "        loss = mse_loss(model_koopman(x_batch, u_batch), y_batch)\n",
    "        loss.backward()\n",
    "        optimizer_koopman.step()\n",
    "        \n",
    "    \n",
    "    train_loss = mse_loss(model_koopman(x_train_latent, u_train), y_train_latent)\n",
    "    train_loss_value = train_loss.item()\n",
    "    train_losses_koopman.append(train_loss_value)\n",
    "        \n",
    "    # Test the model\n",
    "    with torch.no_grad():\n",
    "        val_loss = mse_loss(model_koopman(x_test_latent, u_test), y_test_latent)\n",
    "        val_losses_koopman.append(val_loss.item())\n",
    "\n",
    "    current_lr = optimizer_koopman.param_groups[0]['lr']\n",
    "    scheduler_koopman.step()\n",
    "    print(f'Epoch: {epoch}, Training Loss: {train_loss.item()}, Validation Loss: {val_loss.item()}, Learning Rate: {current_lr}')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(0.0619, grad_fn=<DivBackward0>)\n"
     ]
    }
   ],
   "source": [
    "x_data_scaled_tensor = torch.tensor(x_data_scaled, dtype=torch.float32)\n",
    "y_data_scaled_tensor = torch.tensor(y_data_scaled, dtype=torch.float32)\n",
    "u_data_scaled_tensor = torch.tensor(u_data_scaled, dtype=torch.float32)\n",
    "x_data_latent = model_psi(x_data_scaled_tensor)\n",
    "y_data_latent = model_psi(y_data_scaled_tensor)\n",
    "err = y_data_latent - model_koopman(x_data_latent, u_data_scaled_tensor)\n",
    "err_norm = torch.norm(err)\n",
    "y_data_latent_norm = torch.norm(y_data_latent)\n",
    "ratio = err_norm / y_data_latent_norm\n",
    "print(ratio)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "torch",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.17"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
